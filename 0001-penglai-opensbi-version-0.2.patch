From 5e48eadf63bf7fabb61271b7e65129646cfc01b6 Mon Sep 17 00:00:00 2001
From: anonymous <2748250768@qq.com>
Date: Thu, 18 Mar 2021 13:41:07 +0800
Subject: [PATCH] penglai opensbi version 0.2

---
 include/sbi/riscv_encoding.h                  |   20 +
 include/sbi/riscv_locks.h                     |    2 +
 include/sbi/sbi_console.h                     |    2 +
 include/sbi/sbi_ecall.h                       |    3 +
 include/sbi/sbi_ecall_interface.h             |    3 +
 include/sbi/sbi_ipi.h                         |   10 +
 include/sbi/sbi_ipi_destroy_enclave.h         |   18 +
 include/sbi/sbi_math.h                        |   74 +
 include/sbi/sbi_platform.h                    |    1 +
 include/sbi/sbi_pmp.h                         |   10 +
 include/sbi/sbi_tvm.h                         |   16 +
 include/sbi/sbi_types.h                       |    1 +
 include/sm/attest.h                           |   16 +
 include/sm/enclave.h                          |  250 ++
 include/sm/enclave_args.h                     |  155 +
 include/sm/enclave_mm.h                       |   25 +
 include/sm/enclave_vm.h                       |   88 +
 include/sm/encoding.h                         | 1472 +++++++++
 include/sm/gm/big.h                           |   88 +
 include/sm/gm/ecc.h                           |   38 +
 include/sm/gm/random.h                        |    8 +
 include/sm/gm/sm2.h                           |   34 +
 include/sm/gm/sm3.h                           |  103 +
 include/sm/gm/typedef.h                       |  164 +
 include/sm/ipi.h                              |   26 +
 include/sm/platform/pt_area/platform.h        |    9 +
 include/sm/platform/pt_area/platform_thread.h |   18 +
 include/sm/pmp.h                              |   66 +
 include/sm/relay_page.h                       |   12 +
 include/sm/server_enclave.h                   |   21 +
 include/sm/sm.h                               |  144 +
 include/sm/thread.h                           |   66 +
 include/sm/vm.h                               |   36 +
 lib/sbi/objects.mk                            |   20 +
 lib/sbi/sbi_ecall.c                           |  115 +-
 lib/sbi/sbi_ecall_penglai.c                   |   98 +
 lib/sbi/sbi_hart.c                            |    9 +-
 lib/sbi/sbi_illegal_insn.c                    |   59 +
 lib/sbi/sbi_init.c                            |   27 +
 lib/sbi/sbi_ipi.c                             |   30 +
 lib/sbi/sbi_ipi_destroy_enclave.c             |  142 +
 lib/sbi/sbi_pmp.c                             |  123 +
 lib/sbi/sbi_trap.c                            |   66 +-
 lib/sbi/sbi_tvm.c                             |  141 +
 lib/sbi/sm/.gitignore                         |    1 +
 lib/sbi/sm/attest.c                           |  124 +
 lib/sbi/sm/enclave.c                          | 2725 +++++++++++++++++
 lib/sbi/sm/enclave_mm.c                       |  195 ++
 lib/sbi/sm/enclave_vm.c                       |  596 ++++
 lib/sbi/sm/gm/big.c                           |  853 ++++++
 lib/sbi/sm/gm/ecc.c                           |  356 +++
 lib/sbi/sm/gm/random.c                        |   18 +
 lib/sbi/sm/gm/sm2.c                           |  603 ++++
 lib/sbi/sm/gm/sm3.c                           |  325 ++
 lib/sbi/sm/platform/pt_area/platform.c        |   31 +
 lib/sbi/sm/platform/pt_area/platform_thread.c |   56 +
 lib/sbi/sm/pmp.c                              |  240 ++
 lib/sbi/sm/relay_page.c                       |  221 ++
 lib/sbi/sm/server_enclave.c                   |  469 +++
 lib/sbi/sm/sm.ac                              |    9 +
 lib/sbi/sm/sm.c                               | 1374 +++++++++
 lib/sbi/sm/thread.c                           |  101 +
 62 files changed, 12104 insertions(+), 22 deletions(-)
 create mode 100644 include/sbi/sbi_ipi_destroy_enclave.h
 create mode 100644 include/sbi/sbi_pmp.h
 create mode 100644 include/sbi/sbi_tvm.h
 create mode 100644 include/sm/attest.h
 create mode 100644 include/sm/enclave.h
 create mode 100644 include/sm/enclave_args.h
 create mode 100644 include/sm/enclave_mm.h
 create mode 100644 include/sm/enclave_vm.h
 create mode 100644 include/sm/encoding.h
 create mode 100644 include/sm/gm/big.h
 create mode 100644 include/sm/gm/ecc.h
 create mode 100644 include/sm/gm/random.h
 create mode 100644 include/sm/gm/sm2.h
 create mode 100644 include/sm/gm/sm3.h
 create mode 100644 include/sm/gm/typedef.h
 create mode 100644 include/sm/ipi.h
 create mode 100644 include/sm/platform/pt_area/platform.h
 create mode 100644 include/sm/platform/pt_area/platform_thread.h
 create mode 100644 include/sm/pmp.h
 create mode 100644 include/sm/relay_page.h
 create mode 100644 include/sm/server_enclave.h
 create mode 100644 include/sm/sm.h
 create mode 100644 include/sm/thread.h
 create mode 100644 include/sm/vm.h
 create mode 100644 lib/sbi/sbi_ecall_penglai.c
 create mode 100644 lib/sbi/sbi_ipi_destroy_enclave.c
 create mode 100644 lib/sbi/sbi_pmp.c
 create mode 100644 lib/sbi/sbi_tvm.c
 create mode 100644 lib/sbi/sm/.gitignore
 create mode 100644 lib/sbi/sm/attest.c
 create mode 100644 lib/sbi/sm/enclave.c
 create mode 100644 lib/sbi/sm/enclave_mm.c
 create mode 100644 lib/sbi/sm/enclave_vm.c
 create mode 100644 lib/sbi/sm/gm/big.c
 create mode 100644 lib/sbi/sm/gm/ecc.c
 create mode 100644 lib/sbi/sm/gm/random.c
 create mode 100644 lib/sbi/sm/gm/sm2.c
 create mode 100644 lib/sbi/sm/gm/sm3.c
 create mode 100644 lib/sbi/sm/platform/pt_area/platform.c
 create mode 100644 lib/sbi/sm/platform/pt_area/platform_thread.c
 create mode 100644 lib/sbi/sm/pmp.c
 create mode 100644 lib/sbi/sm/relay_page.c
 create mode 100644 lib/sbi/sm/server_enclave.c
 create mode 100644 lib/sbi/sm/sm.ac
 create mode 100644 lib/sbi/sm/sm.c
 create mode 100644 lib/sbi/sm/thread.c

diff --git a/include/sbi/riscv_encoding.h b/include/sbi/riscv_encoding.h
index 827c86c..7a4908c 100644
--- a/include/sbi/riscv_encoding.h
+++ b/include/sbi/riscv_encoding.h
@@ -577,6 +577,26 @@
 					 (s32)(((insn) >> 7) & 0x1f))
 #define MASK_FUNCT3			0x7000
 
+/*penglai defination*/
+#define PTE_PPN_SHIFT 10
+#define PTE_V     0x001 // Valid
+#define PTE_R     0x002 // Read
+#define PTE_W     0x004 // Write
+#define PTE_X     0x008 // Execute
+#define PTE_U     0x010 // User
+#define PTE_G     0x020 // Global
+#define PTE_A     0x040 // Accessed
+#define PTE_D     0x080 // Dirty
+#define PTE_SOFT  0x300 // Reserved for Software
+
+//TODO:
+//riscv64 page config
+#define RISCV_PGSHIFT 12
+#define RISCV_PGSIZE (1 << RISCV_PGSHIFT)
+#define RISCV_PTENUM 512
+#define RISCV_PGLEVEL_BITS 9
+
 /* clang-format on */
 
+#define DRAM_BASE          0x80000000
 #endif
diff --git a/include/sbi/riscv_locks.h b/include/sbi/riscv_locks.h
index 55da7c0..60076e7 100644
--- a/include/sbi/riscv_locks.h
+++ b/include/sbi/riscv_locks.h
@@ -14,6 +14,8 @@ typedef struct {
 	volatile long lock;
 } spinlock_t;
 
+#define SPINLOCK_INIT {0}
+
 #define __RISCV_SPIN_UNLOCKED 0
 
 #define SPIN_LOCK_INIT(_lptr) (_lptr)->lock = __RISCV_SPIN_UNLOCKED
diff --git a/include/sbi/sbi_console.h b/include/sbi/sbi_console.h
index 7d648f0..e64a862 100644
--- a/include/sbi/sbi_console.h
+++ b/include/sbi/sbi_console.h
@@ -36,4 +36,6 @@ struct sbi_scratch;
 
 int sbi_console_init(struct sbi_scratch *scratch);
 
+#define sbi_bug(fmt, ...) sbi_printf("[ERROR] "fmt, ##__VA_ARGS__)
+#define sbi_debug(fmt, ...) sbi_printf("[DEBUG] "fmt, ##__VA_ARGS__)
 #endif
diff --git a/include/sbi/sbi_ecall.h b/include/sbi/sbi_ecall.h
index 3273ba6..6111ec9 100644
--- a/include/sbi/sbi_ecall.h
+++ b/include/sbi/sbi_ecall.h
@@ -37,6 +37,7 @@ extern struct sbi_ecall_extension ecall_rfence;
 extern struct sbi_ecall_extension ecall_ipi;
 extern struct sbi_ecall_extension ecall_vendor;
 extern struct sbi_ecall_extension ecall_hsm;
+extern struct sbi_ecall_extension ecall_pengali;
 
 u16 sbi_ecall_version_major(void);
 
@@ -52,6 +53,8 @@ int sbi_ecall_register_extension(struct sbi_ecall_extension *ext);
 
 void sbi_ecall_unregister_extension(struct sbi_ecall_extension *ext);
 
+int enclave_call_trap(struct sbi_trap_regs* regs);
+
 int sbi_ecall_handler(struct sbi_trap_regs *regs);
 
 int sbi_ecall_init(void);
diff --git a/include/sbi/sbi_ecall_interface.h b/include/sbi/sbi_ecall_interface.h
index af30500..123c1e5 100644
--- a/include/sbi/sbi_ecall_interface.h
+++ b/include/sbi/sbi_ecall_interface.h
@@ -27,6 +27,9 @@
 #define SBI_EXT_IPI				0x735049
 #define SBI_EXT_RFENCE				0x52464E43
 #define SBI_EXT_HSM				0x48534D
+//TODO:
+//Why this magic number
+#define SBI_EXT_PENGLAI				0x100100 
 
 /* SBI function IDs for BASE extension*/
 #define SBI_EXT_BASE_GET_SPEC_VERSION		0x0
diff --git a/include/sbi/sbi_ipi.h b/include/sbi/sbi_ipi.h
index 617872c..b051bd3 100644
--- a/include/sbi/sbi_ipi.h
+++ b/include/sbi/sbi_ipi.h
@@ -11,6 +11,7 @@
 #define __SBI_IPI_H__
 
 #include <sbi/sbi_types.h>
+#include <sbi/sbi_trap.h>
 
 /* clang-format off */
 
@@ -47,6 +48,13 @@ struct sbi_ipi_event_ops {
 	 * remote HART after IPI is triggered.
 	 */
 	void (* process)(struct sbi_scratch *scratch);
+
+	/**
+	 * Process callback to handle IPI event in the enclave
+	 * Note: This is a mandatory callback and it is called on the
+	 * remote HART after IPI is triggered.
+	 */
+	void (* e_process)(struct sbi_scratch *scratch, struct sbi_trap_regs* regs);
 };
 
 int sbi_ipi_send_many(ulong hmask, ulong hbase, u32 event, void *data);
@@ -63,6 +71,8 @@ int sbi_ipi_send_halt(ulong hmask, ulong hbase);
 
 void sbi_ipi_process(void);
 
+void sbi_ipi_process_in_enclave(struct sbi_trap_regs* regs);
+
 int sbi_ipi_init(struct sbi_scratch *scratch, bool cold_boot);
 
 void sbi_ipi_exit(struct sbi_scratch *scratch);
diff --git a/include/sbi/sbi_ipi_destroy_enclave.h b/include/sbi/sbi_ipi_destroy_enclave.h
new file mode 100644
index 0000000..da8c9a3
--- /dev/null
+++ b/include/sbi/sbi_ipi_destroy_enclave.h
@@ -0,0 +1,18 @@
+#ifndef __SBI_IPI_DESTROY_ENCLAVE_H__
+#define __SBI_IPI_DESTROY_ENCLAVE_H__
+
+#include <sbi/sbi_types.h>
+#include <sbi/sbi_hartmask.h>
+
+struct ipi_destroy_enclave_data_t
+{
+  ulong host_ptbr;
+  int enclave_id;
+  struct sbi_hartmask smask;
+};
+
+struct sbi_scratch;
+int sbi_ipi_destroy_enclave_init(struct sbi_scratch *scratch, bool cold_boot);
+int sbi_send_ipi_destroy_enclave(ulong hmask, ulong hbase, struct ipi_destroy_enclave_data_t* ipi_destroy_enclave_data);
+void set_ipi_destroy_enclave_and_sync(u32 remote_hart,ulong host_ptbr, int enclave_id);
+#endif
\ No newline at end of file
diff --git a/include/sbi/sbi_math.h b/include/sbi/sbi_math.h
index 564fd58..664ab0a 100644
--- a/include/sbi/sbi_math.h
+++ b/include/sbi/sbi_math.h
@@ -12,4 +12,78 @@
 
 unsigned long log2roundup(unsigned long x);
 
+#define ilog2(n)              \
+(                             \
+  (n) < 2 ? 0 :               \
+  (n) & (1ULL << 63) ? 63 :   \
+  (n) & (1ULL << 62) ? 62 :   \
+  (n) & (1ULL << 61) ? 61 :   \
+  (n) & (1ULL << 60) ? 60 :	  \
+  (n) & (1ULL << 59) ? 59 :	  \
+  (n) & (1ULL << 58) ? 58 :	  \
+  (n) & (1ULL << 57) ? 57 :	  \
+  (n) & (1ULL << 56) ? 56 :	  \
+  (n) & (1ULL << 55) ? 55 :	  \
+  (n) & (1ULL << 54) ? 54 :	  \
+  (n) & (1ULL << 53) ? 53 :	  \
+  (n) & (1ULL << 52) ? 52 :	  \
+  (n) & (1ULL << 51) ? 51 :	  \
+  (n) & (1ULL << 50) ? 50 :	  \
+  (n) & (1ULL << 49) ? 49 :	  \
+  (n) & (1ULL << 48) ? 48 :	  \
+  (n) & (1ULL << 47) ? 47 :	  \
+  (n) & (1ULL << 46) ? 46 :	  \
+  (n) & (1ULL << 45) ? 45 :	  \
+  (n) & (1ULL << 44) ? 44 :	  \
+  (n) & (1ULL << 43) ? 43 :	  \
+  (n) & (1ULL << 42) ? 42 :	  \
+  (n) & (1ULL << 41) ? 41 :	  \
+  (n) & (1ULL << 40) ? 40 :	  \
+  (n) & (1ULL << 39) ? 39 :	  \
+  (n) & (1ULL << 38) ? 38 :	  \
+  (n) & (1ULL << 37) ? 37 :	  \
+  (n) & (1ULL << 36) ? 36 :	  \
+  (n) & (1ULL << 35) ? 35 :	  \
+  (n) & (1ULL << 34) ? 34 :	  \
+  (n) & (1ULL << 33) ? 33 :	  \
+  (n) & (1ULL << 32) ? 32 :	  \
+  (n) & (1ULL << 31) ? 31 :	  \
+  (n) & (1ULL << 30) ? 30 :	  \
+  (n) & (1ULL << 29) ? 29 :	  \
+  (n) & (1ULL << 28) ? 28 :	  \
+  (n) & (1ULL << 27) ? 27 :	  \
+  (n) & (1ULL << 26) ? 26 :	  \
+  (n) & (1ULL << 25) ? 25 :	  \
+  (n) & (1ULL << 24) ? 24 :	  \
+  (n) & (1ULL << 23) ? 23 :	  \
+  (n) & (1ULL << 22) ? 22 :	  \
+  (n) & (1ULL << 21) ? 21 :	  \
+  (n) & (1ULL << 20) ? 20 :	  \
+  (n) & (1ULL << 19) ? 19 :	  \
+  (n) & (1ULL << 18) ? 18 :	  \
+  (n) & (1ULL << 17) ? 17 :	  \
+  (n) & (1ULL << 16) ? 16 :	  \
+  (n) & (1ULL << 15) ? 15 :	  \
+  (n) & (1ULL << 14) ? 14 :	  \
+  (n) & (1ULL << 13) ? 13 :	  \
+  (n) & (1ULL << 12) ? 12 :	  \
+  (n) & (1ULL << 11) ? 11 :	  \
+  (n) & (1ULL << 10) ? 10 :	  \
+  (n) & (1ULL <<  9) ?  9 :	  \
+  (n) & (1ULL <<  8) ?  8 :	  \
+  (n) & (1ULL <<  7) ?  7 :	  \
+  (n) & (1ULL <<  6) ?  6 :	  \
+  (n) & (1ULL <<  5) ?  5 :	  \
+  (n) & (1ULL <<  4) ?  4 :	  \
+  (n) & (1ULL <<  3) ?  3 :	  \
+  (n) & (1ULL <<  2) ?  2 :	  \
+  1                           \
+)
+
+#define power_2_align(n) (1 << (ilog2(n-1)+1))
+
+#define size_down_align(n, size) (n - ((n) % (size)))
+
+#define size_up_align(n, size) (size_down_align(n, size) + ((n) % (size) ? (size) : 0))
+
 #endif
diff --git a/include/sbi/sbi_platform.h b/include/sbi/sbi_platform.h
index 8087148..82de38e 100644
--- a/include/sbi/sbi_platform.h
+++ b/include/sbi/sbi_platform.h
@@ -45,6 +45,7 @@
 #include <sbi/sbi_scratch.h>
 #include <sbi/sbi_trap.h>
 #include <sbi/sbi_version.h>
+#include <sbi/sbi_pmp.h>
 
 /** Possible feature flags of a platform */
 enum sbi_platform_features {
diff --git a/include/sbi/sbi_pmp.h b/include/sbi/sbi_pmp.h
new file mode 100644
index 0000000..4b4523b
--- /dev/null
+++ b/include/sbi/sbi_pmp.h
@@ -0,0 +1,10 @@
+#ifndef __SBI_PMP_H__
+#define __SBI_PMP_H__
+
+#include "sm/pmp.h"
+#include <sbi/sbi_types.h>
+#include <sbi/sbi_hartmask.h>
+struct sbi_scratch;
+int sbi_pmp_init(struct sbi_scratch *scratch, bool cold_boot);
+int sbi_send_pmp(ulong hmask, ulong hbase, struct pmp_data_t* pmp_data);
+#endif
\ No newline at end of file
diff --git a/include/sbi/sbi_tvm.h b/include/sbi/sbi_tvm.h
new file mode 100644
index 0000000..04839c6
--- /dev/null
+++ b/include/sbi/sbi_tvm.h
@@ -0,0 +1,16 @@
+#ifndef __SBI_TVM_H__
+#define __SBI_TVM_H__
+
+#include <sbi/sbi_types.h>
+#include <sbi/sbi_hartmask.h>
+
+struct tvm_data_t
+{
+  struct sbi_hartmask smask;
+};
+
+struct sbi_scratch;
+int sbi_tvm_init(struct sbi_scratch *scratch, bool cold_boot);
+int sbi_send_tvm(ulong hmask, ulong hbase, struct tvm_data_t* tvm_data);
+void set_tvm_and_sync();
+#endif
\ No newline at end of file
diff --git a/include/sbi/sbi_types.h b/include/sbi/sbi_types.h
index 0952d5c..b004419 100644
--- a/include/sbi/sbi_types.h
+++ b/include/sbi/sbi_types.h
@@ -47,6 +47,7 @@ typedef unsigned long long	uint64_t;
 typedef int			bool;
 typedef unsigned long		ulong;
 typedef unsigned long		uintptr_t;
+typedef long		intptr_t;
 typedef unsigned long		size_t;
 typedef long			ssize_t;
 typedef unsigned long		virtual_addr_t;
diff --git a/include/sm/attest.h b/include/sm/attest.h
new file mode 100644
index 0000000..df18eac
--- /dev/null
+++ b/include/sm/attest.h
@@ -0,0 +1,16 @@
+#ifndef _ATTEST_H
+#define _ATTEST_H
+
+#include "sm/enclave.h"
+
+void hash_enclave(struct enclave_t* enclave, void* hash, uintptr_t nonce);
+
+void hash_shadow_enclave(struct shadow_enclave_t* enclave, void* hash, uintptr_t nonce);
+
+void update_hash_shadow_enclave(struct shadow_enclave_t *enclave, void* hash, uintptr_t nonce_arg);
+
+void sign_enclave(void* signature, void* hash);
+
+int verify_enclave(void* signature, void* hash);
+
+#endif /* _ATTEST_H */
diff --git a/include/sm/enclave.h b/include/sm/enclave.h
new file mode 100644
index 0000000..39275e5
--- /dev/null
+++ b/include/sm/enclave.h
@@ -0,0 +1,250 @@
+#ifndef _ENCLAVE_H
+#define _ENCLAVE_H
+
+#include "sbi/riscv_encoding.h"
+#include "sm/enclave_args.h"
+#include "sbi/riscv_atomic.h" 
+#include "sbi/riscv_locks.h"
+#include "sbi/sbi_string.h"
+#include "sbi/riscv_asm.h"
+#include "sbi/sbi_types.h"
+#include "sm/thread.h"
+#include "sm/vm.h"
+
+
+
+#define ENCLAVES_PER_METADATA_REGION 100
+#define ENCLAVE_METADATA_REGION_SIZE ((sizeof(struct enclave_t)) * ENCLAVES_PER_METADATA_REGION)
+#define SHADOW_ENCLAVE_METADATA_REGION_SIZE ((sizeof(struct shadow_enclave_t)) * ENCLAVES_PER_METADATA_REGION)
+#define RELAY_PAGE_NUM 5
+#define MAX_HARTS 8
+#define ENCLAVE_MODE 1
+#define NORMAL_MODE 0
+
+#define SET_ENCLAVE_METADATA(point, enclave, create_args, struct_type, base) do { \
+  enclave->entry_point = point; \
+  enclave->ocall_func_id = ((struct_type)create_args)->ecall_arg0; \
+  enclave->ocall_arg0 = ((struct_type)create_args)->ecall_arg1; \
+  enclave->ocall_arg1 = ((struct_type)create_args)->ecall_arg2; \
+  enclave->ocall_syscall_num = ((struct_type)create_args)->ecall_arg3; \
+  enclave->kbuffer = ((struct_type)create_args)->kbuffer; \
+  enclave->kbuffer_size = ((struct_type)create_args)->kbuffer_size; \
+  enclave->shm_paddr = ((struct_type)create_args)->shm_paddr; \
+  enclave->shm_size = ((struct_type)create_args)->shm_size; \
+  enclave->host_ptbr = csr_read(CSR_SATP); \
+  enclave->root_page_table = ((struct_type)create_args)->base + RISCV_PGSIZE; \
+  enclave->thread_context.encl_ptbr = ((((struct_type)create_args)->base+RISCV_PGSIZE) >> RISCV_PGSHIFT) | SATP_MODE_CHOICE; \
+  enclave->type = NORMAL_ENCLAVE; \
+  enclave->state = FRESH; \
+  enclave->caller_eid = -1; \
+  enclave->top_caller_eid = -1; \
+  enclave->cur_callee_eid = -1; \
+  sbi_memcpy(enclave->enclave_name, ((struct_type)create_args)->name, NAME_LEN); \
+} while(0)
+
+struct link_mem_t
+{
+  unsigned long mem_size;
+  unsigned long slab_size;
+  unsigned long slab_num;
+  char* addr;
+  struct link_mem_t* next_link_mem;    
+};
+
+typedef enum 
+{
+  DESTROYED = -1,
+  INVALID = 0,
+  FRESH = 1,
+  RUNNABLE,
+  RUNNING,
+  STOPPED, 
+  ATTESTING,
+  OCALLING
+} enclave_state_t;
+
+struct vm_area_struct
+{
+  unsigned long va_start;
+  unsigned long va_end;
+
+  struct vm_area_struct *vm_next;
+  struct pm_area_struct *pma;
+};
+
+struct pm_area_struct
+{
+  unsigned long paddr;
+  unsigned long size;
+  unsigned long free_mem;
+
+  struct pm_area_struct *pm_next;
+};
+
+struct page_t
+{
+  uintptr_t paddr;
+  struct page_t *next;
+};
+
+struct enclave_t
+{
+  unsigned int eid;
+  enclave_type_t type;
+  enclave_state_t state;
+
+  ///vm_area_struct lists
+  struct vm_area_struct* text_vma;
+  struct vm_area_struct* stack_vma;
+  uintptr_t _stack_top; ///lowest address of stack area
+  struct vm_area_struct* heap_vma;
+  uintptr_t _heap_top;  ///highest address of heap area
+  struct vm_area_struct* mmap_vma;
+
+  ///pm_area_struct list
+  struct pm_area_struct* pma_list;
+  struct page_t* free_pages;
+  uintptr_t free_pages_num;
+
+  ///root page table of enclave
+  unsigned long root_page_table;
+
+  ///root page table register for host
+  unsigned long host_ptbr;
+
+  ///entry point of enclave
+  unsigned long entry_point;
+
+  ///shared mem with kernel
+  unsigned long kbuffer;//paddr
+  unsigned long kbuffer_size;
+
+  ///shared mem with host
+  unsigned long shm_paddr;
+  unsigned long shm_size;
+
+  // host memory arg
+  unsigned long mm_arg_paddr[RELAY_PAGE_NUM];
+  unsigned long mm_arg_size[RELAY_PAGE_NUM];
+
+  unsigned long* ocall_func_id;
+  unsigned long* ocall_arg0;
+  unsigned long* ocall_arg1;
+  unsigned long* ocall_syscall_num;
+
+  // enclave thread context
+  // TODO: support multiple threads
+  struct thread_state_t thread_context;
+  unsigned int top_caller_eid;
+  unsigned int caller_eid;
+  unsigned int cur_callee_eid;
+  unsigned char hash[HASH_SIZE];
+  char enclave_name[NAME_LEN];
+};
+
+struct shadow_enclave_t
+{
+  unsigned int eid;
+
+  enclave_state_t state;
+  struct page_t* free_pages;
+  uintptr_t free_pages_num;
+
+  ///root page table of enclave
+  unsigned long root_page_table;
+
+  ///root page table register for host
+  unsigned long host_ptbr;
+
+  ///entry point of enclave
+  unsigned long entry_point;
+  struct thread_state_t thread_context;
+  unsigned char hash[HASH_SIZE];
+};
+
+/**
+ * cpu state
+ */
+struct cpu_state_t
+{
+  int in_enclave; /// whether current hart is in enclave-mode
+  int eid; /// the eid of current enclave if the hart in enclave-mode
+};
+
+void acquire_enclave_metadata_lock();
+void release_enclave_metadata_lock();
+
+int cpu_in_enclave(int i);
+int cpu_eid(int i);
+int check_in_enclave_world();
+int get_curr_enclave_id();
+struct enclave_t* __get_enclave(int eid);
+
+uintptr_t copy_from_host(void* dest, void* src, size_t size);
+uintptr_t copy_to_host(void* dest, void* src, size_t size);
+int copy_word_to_host(unsigned int* ptr, uintptr_t value);
+int copy_dword_to_host(uintptr_t* ptr, uintptr_t value);
+
+struct link_mem_t* init_mem_link(unsigned long mem_size, unsigned long slab_size);
+struct link_mem_t* add_link_mem(struct link_mem_t** tail);
+
+struct enclave_t* __alloc_enclave();
+int __free_enclave(int eid);
+void free_enclave_memory(struct pm_area_struct *pma);
+
+uintptr_t create_enclave(enclave_create_param_t create_args);
+uintptr_t attest_enclave(uintptr_t eid, uintptr_t report, uintptr_t nonce);
+uintptr_t attest_shadow_enclave(uintptr_t eid, uintptr_t report, uintptr_t nonce);
+uintptr_t run_enclave(uintptr_t* regs, unsigned int eid, uintptr_t addr, uintptr_t size);
+uintptr_t stop_enclave(uintptr_t* regs, unsigned int eid);
+uintptr_t wake_enclave(uintptr_t* regs, unsigned int eid);
+uintptr_t destroy_enclave(uintptr_t* regs, unsigned int eid);
+uintptr_t resume_enclave(uintptr_t* regs, unsigned int eid);
+uintptr_t resume_from_ocall(uintptr_t* regs, unsigned int eid);
+
+uintptr_t exit_enclave(uintptr_t* regs, unsigned long retval);
+uintptr_t enclave_mmap(uintptr_t* regs, uintptr_t vaddr, uintptr_t size);
+uintptr_t enclave_unmap(uintptr_t* regs, uintptr_t vaddr, uintptr_t size);
+
+uintptr_t create_shadow_enclave(enclave_create_param_t create_args);
+uintptr_t run_shadow_enclave(uintptr_t* regs, unsigned int eid, shadow_enclave_run_param_t enclave_run_param, uintptr_t addr, uintptr_t size);
+
+struct call_enclave_arg_t
+{
+  uintptr_t req_arg;
+  uintptr_t resp_val;
+  uintptr_t req_vaddr;
+  uintptr_t req_size;
+  uintptr_t resp_vaddr;
+  uintptr_t resp_size;
+};
+uintptr_t call_enclave(uintptr_t *regs, unsigned int enclave_id, uintptr_t arg);
+uintptr_t enclave_return(uintptr_t *regs, uintptr_t arg);
+uintptr_t enclave_sys_write(uintptr_t *regs);
+uintptr_t enclave_sbrk(uintptr_t* regs, intptr_t size);
+uintptr_t enclave_read_sec(uintptr_t *regs, uintptr_t sec);
+uintptr_t enclave_write_sec(uintptr_t *regs, uintptr_t sec);
+uintptr_t enclave_return_relay_page(uintptr_t *regs);
+uintptr_t do_timer_irq(uintptr_t* regs, uintptr_t mcause, uintptr_t mepc);
+uintptr_t do_yield(uintptr_t* regs, uintptr_t mcause, uintptr_t mepc);
+uintptr_t ipi_stop_enclave(uintptr_t *regs, uintptr_t host_ptbr, int eid);
+uintptr_t ipi_destroy_enclave(uintptr_t *regs, uintptr_t host_ptbr, int eid);
+
+
+//relay page
+#define ENTRY_PER_METADATA_REGION 100
+#define ENTRY_PER_RELAY_PAGE_REGION 20
+
+struct relay_page_entry_t
+{
+  char enclave_name[NAME_LEN];
+  unsigned long  addr;
+  unsigned long size;
+};
+uintptr_t change_relay_page_ownership(unsigned long relay_page_addr, unsigned long relay_page_size, char *enclave_name);
+struct relay_page_entry_t* __get_relay_page_by_name(char* enclave_name, int *slab_index, int *link_mem_index);
+int __free_relay_page_entry(unsigned long relay_page_addr, unsigned long relay_page_size);
+struct relay_page_entry_t* __alloc_relay_page_entry(char *enclave_name, unsigned long relay_page_addr, unsigned long relay_page_size);
+
+
+#endif /* _ENCLAVE_H */
diff --git a/include/sm/enclave_args.h b/include/sm/enclave_args.h
new file mode 100644
index 0000000..14e2c72
--- /dev/null
+++ b/include/sm/enclave_args.h
@@ -0,0 +1,155 @@
+#ifndef _ENCLAVE_ARGS_H
+#define _ENCLAVE_ARGS_H
+#include "sm/thread.h"
+#define HASH_SIZE              32
+#define PRIVATE_KEY_SIZE       32
+#define PUBLIC_KEY_SIZE        64
+#define SIGNATURE_SIZE         64
+
+#define MANU_PUB_KEY           (void*)((unsigned long)0x801ff000)
+#define DEV_PUB_KEY            (MANU_PUB_KEY + PUBLIC_KEY_SIZE)
+#define DEV_PRI_KEY            (DEV_PUB_KEY + PUBLIC_KEY_SIZE)
+#define SM_PUB_KEY             (DEV_PRI_KEY + PRIVATE_KEY_SIZE)
+#define SM_PRI_KEY             (SM_PUB_KEY + PUBLIC_KEY_SIZE)
+#define SM_HASH                (SM_PRI_KEY + PRIVATE_KEY_SIZE)
+#define SM_SIGNATURE           (SM_HASH + HASH_SIZE)
+
+struct mm_alloc_arg_t
+{
+  unsigned long req_size;
+  uintptr_t resp_addr;
+  unsigned long resp_size;
+};
+
+struct sm_report_t
+{
+  unsigned char hash[HASH_SIZE];
+  unsigned char signature[SIGNATURE_SIZE];
+  unsigned char sm_pub_key[PUBLIC_KEY_SIZE];
+};
+
+struct enclave_report_t
+{
+  unsigned char hash[HASH_SIZE];
+  unsigned char signature[SIGNATURE_SIZE];
+  uintptr_t nonce;
+};
+
+struct report_t
+{
+  struct sm_report_t sm;
+  struct enclave_report_t enclave;
+  unsigned char dev_pub_key[PUBLIC_KEY_SIZE];
+};
+
+struct signature_t
+{
+  unsigned char r[PUBLIC_KEY_SIZE/2];
+  unsigned char s[PUBLIC_KEY_SIZE/2];
+};
+
+struct pt_entry_t
+{
+  unsigned long pte_addr;
+  unsigned long pte;
+};
+
+#if __riscv_xlen == 64
+
+#define NAME_LEN           16
+
+typedef enum
+{
+  NORMAL_ENCLAVE = 0,
+  SERVER_ENCLAVE = 1
+} enclave_type_t;
+
+typedef struct enclave_create_param
+{
+  unsigned int *eid_ptr;
+  char name[NAME_LEN];
+  enclave_type_t type;
+
+  unsigned long paddr;
+  unsigned long size;
+
+  unsigned long entry_point;
+
+  unsigned long free_mem;
+
+  //enclave shared mem with kernel
+  unsigned long kbuffer;//paddr
+  unsigned long kbuffer_size;
+
+  //enclave shared mem with host
+  unsigned long shm_paddr;
+  unsigned long shm_size;
+
+  unsigned long *ecall_arg0;
+  unsigned long *ecall_arg1;
+  unsigned long *ecall_arg2;
+  unsigned long *ecall_arg3;
+} enclave_create_param_t;
+
+typedef struct shadow_enclave_run_param
+{
+  unsigned long sptbr;
+  unsigned long free_page;
+  unsigned long size;
+  unsigned int *eid_ptr;
+
+  unsigned long kbuffer;//paddr
+  unsigned long kbuffer_size;
+
+  unsigned long shm_paddr;
+  unsigned long shm_size;
+
+  unsigned long schrodinger_paddr;
+  unsigned long schrodinger_size;
+
+  unsigned long *ecall_arg0;
+  unsigned long *ecall_arg1;
+  unsigned long *ecall_arg2;
+  unsigned long *ecall_arg3;
+  char name[NAME_LEN];
+} shadow_enclave_run_param_t;
+
+#else
+
+#define ATTRIBUTE_R               0x1
+#define ATTRIBUTE_W               0x2
+#define ATTRIBUTE_X               0x4
+#define DEFAULT_EAPP_REGIONS_NUM             5
+
+struct region_t {
+  unsigned long base;
+  unsigned long size;
+  unsigned long attributes;
+};
+
+struct eapp_t {
+  unsigned long offset;
+  unsigned long size;
+  unsigned long uuid;
+  struct region_t regions[DEFAULT_EAPP_REGIONS_NUM];
+};
+
+typedef struct enclave_create_param
+{
+  unsigned long uuid;
+  unsigned long *eid_ptr;
+
+  unsigned long untrusted_ptr;
+  unsigned long untrusted_size;
+}enclave_create_param_t;
+
+struct init_enclave_create_param_t
+{
+  unsigned long uuid;
+  unsigned long entry_point;
+  struct region_t regions[DEFAULT_EAPP_REGIONS_NUM];
+};
+
+#endif /* __riscv_xlen == 64 */
+
+#endif /* _ENCLAVE_ARGS_H */
diff --git a/include/sm/enclave_mm.h b/include/sm/enclave_mm.h
new file mode 100644
index 0000000..5d7a1d3
--- /dev/null
+++ b/include/sm/enclave_mm.h
@@ -0,0 +1,25 @@
+#ifndef _ENCLAVE_MM_H
+#define _ENCLAVE_MM_H
+
+#include "sbi/sbi_types.h"
+#include "sm/enclave.h"
+
+struct mm_region_list_t
+{
+  uintptr_t paddr;
+  unsigned long size;
+  struct mm_region_list_t *next;
+};
+
+int check_and_set_secure_memory(unsigned long paddr, unsigned long size);
+int __free_secure_memory(unsigned long paddr, unsigned long size);
+int free_secure_memory(unsigned long paddr, unsigned long size);
+
+uintptr_t mm_init(uintptr_t paddr, unsigned long size);
+void* mm_alloc(unsigned long req_size, unsigned long* resp_size);
+int mm_free(void* paddr, unsigned long size);
+
+int grant_enclave_access(struct enclave_t* enclave);
+int retrieve_enclave_access(struct enclave_t *enclave);
+
+#endif /* _ENCLAVE_MM_H */
diff --git a/include/sm/enclave_vm.h b/include/sm/enclave_vm.h
new file mode 100644
index 0000000..0c74064
--- /dev/null
+++ b/include/sm/enclave_vm.h
@@ -0,0 +1,88 @@
+#ifndef _ENCLAVE_VM_H
+#define _ENCLAVE_VM_H
+
+#include "enclave.h"
+// #include "encoding.h"
+#include "vm.h"
+
+/* default layout of enclave */
+//#####################
+//#   reserved for    #
+//#       s mode      #
+//##################### 0xffffffe000000000 //actually this is the start address of kernel's image
+//#       hole        #
+//##################### 0x0000004000000000
+//#    shared mem     #
+//#     with host     #
+//##################### 0x0000003900000000
+//#                   #
+//#    host mm arg    #
+//#                   #
+//##################### 0x0000003800000000
+//#                   #
+//#       stack       #
+//#                   #
+//##################### 0x0000003000000000
+//#       mmap        #
+//#                   #
+//##################### brk
+//#                   #
+//#       heap        #
+//#                   #
+//##################### 0x0000001000000000
+//#                   #
+//#   text/code/bss   #
+//#                   #
+//##################### 0x0000000000001000 //not fixed, depends on enclave's lds
+//#       hole        #
+//##################### 0x0
+
+#define ENCLAVE_DEFAULT_KBUFFER_SIZE    0x1000UL
+#define ENCLAVE_DEFAULT_KBUFFER         0xffffffe000000000UL
+#define ENCLAVE_DEFAULT_SHM_BASE        0x0000003900000000UL
+#define ENCLAVE_DEFAULT_MM_ARG_BASE     0x0000003800000000UL
+#define ENCLAVE_DEFAULT_STACK_BASE      0x0000003800000000UL
+#define ENCLAVE_DEFAULT_STACK_SIZE      128*1024
+#define ENCLAVE_DEFAULT_MMAP_BASE       0x0000003000000000UL
+#define ENCLAVE_DEFAULT_HEAP_BASE       0x0000001000000000UL
+#define ENCLAVE_DEFAULT_TEXT_BASE       0x0000000000001000UL
+
+#define PAGE_UP(paddr) (((paddr) + RISCV_PGSIZE - 1) & (~(RISCV_PGSIZE - 1)))
+#define PAGE_DOWN(paddr) ((addr) & (~(RISCV_PGSIZE - 1)))
+#define PADDR_TO_PFN(paddr) ((paddr) >> RISCV_PGSHIFT)
+#define PAGE_PFN_SHIFT 10
+#define PAGE_ATTRIBUTION(pte) (pte & ((1<<PAGE_PFN_SHIFT) - 1))
+#define PTE_VALID(pte) (pte & PTE_V)
+#define PTE_ILLEGAL(pte) ((pte & PTE_V) && (pte & PTE_W) && !(pte & PTE_R))
+#define VALIDATE_PTE(pte) (pte | PTE_V)
+#define INVALIDATE_PTE(pte) (pte & (~PTE_V))
+#define PTE_TO_PFN(pte) (pte >> PTE_PPN_SHIFT)
+#define PFN_TO_PTE(pfn, attribution) ((pfn<<PAGE_PFN_SHIFT) | attribution)
+#define PTE_TO_PA(pte) ((pte >> PTE_PPN_SHIFT)<<RISCV_PGSHIFT)
+#define PA_TO_PTE(pa, attribution) (((pa>>RISCV_PGSHIFT)<<PAGE_PFN_SHIFT) | attribution)
+#define SATP_PPN  0x00000FFFFFFFFFFFUL
+#define IS_PGD(pte) (pte & SATP_MODE_CHOICE)
+#define IS_LEAF_PTE(pte) ((pte & PTE_V) && (pte & PTE_R || pte & PTE_X))
+#define PGD_TO_PFN(pgd) (pgd & SATP_PPN)
+#define RISCV_PGLEVELS ((VA_BITS - RISCV_PGSHIFT) / RISCV_PGLEVEL_BITS)
+
+void traverse_vmas(uintptr_t root_page_table, struct vm_area_struct *vma);
+int insert_vma(struct vm_area_struct **vma_list, struct vm_area_struct *vma, uintptr_t up_bound);
+int delete_vma(struct vm_area_struct **vma_list, struct vm_area_struct *vma);
+struct vm_area_struct* find_vma(struct vm_area_struct *vma_list, uintptr_t vaddr, uintptr_t size);
+int insert_pma(struct pm_area_struct **pma_list, struct pm_area_struct *pma);
+int delete_pma(struct pm_area_struct **pma_list, struct pm_area_struct *pma);
+
+int check_enclave_layout(uintptr_t root_page_table, uintptr_t va_start, uintptr_t va_end, uintptr_t pa_start, uintptr_t pa_end);
+
+void* va_to_pa(uintptr_t* root_page_table, void* va);
+
+int mmap(uintptr_t* root_page_table, struct page_t **free_pages, uintptr_t vaddr, uintptr_t paddr, uintptr_t size);
+int unmap(uintptr_t* root_page_table, uintptr_t vaddr, uintptr_t size);
+// Debug function:
+uintptr_t *pte_walk(uintptr_t *root_page_table, uintptr_t va);
+
+int __copy_page_table(pte_t* page_table, struct page_t ** free_page, int level, pte_t* copy_page);
+int map_empty_page(uintptr_t* root_page_table, struct page_t **free_pages, uintptr_t vaddr, uintptr_t size);
+
+#endif /* _ENCLAVE_VM_H */
diff --git a/include/sm/encoding.h b/include/sm/encoding.h
new file mode 100644
index 0000000..5e0ca29
--- /dev/null
+++ b/include/sm/encoding.h
@@ -0,0 +1,1472 @@
+// See LICENSE for license details.
+
+#ifndef RISCV_CSR_ENCODING_H
+#define RISCV_CSR_ENCODING_H
+
+#define MSTATUS_UIE         0x00000001
+#define MSTATUS_SIE         0x00000002
+#define MSTATUS_HIE         0x00000004
+#define MSTATUS_MIE         0x00000008
+#define MSTATUS_UPIE        0x00000010
+#define MSTATUS_SPIE        0x00000020
+#define MSTATUS_HPIE        0x00000040
+#define MSTATUS_MPIE        0x00000080
+#define MSTATUS_SPP         0x00000100
+#define MSTATUS_HPP         0x00000600
+#define MSTATUS_MPP         0x00001800
+#define MSTATUS_FS          0x00006000
+#define MSTATUS_XS          0x00018000
+#define MSTATUS_MPRV        0x00020000
+#define MSTATUS_SUM         0x00040000
+#define MSTATUS_MXR         0x00080000
+#define MSTATUS_TVM         0x00100000
+#define MSTATUS_TW          0x00200000
+#define MSTATUS_TSR         0x00400000
+#define MSTATUS32_SD        0x80000000
+#define MSTATUS_UXL         0x0000000300000000
+#define MSTATUS_SXL         0x0000000C00000000
+#define MSTATUS64_SD        0x8000000000000000
+
+#define SSTATUS_UIE         0x00000001
+#define SSTATUS_SIE         0x00000002
+#define SSTATUS_UPIE        0x00000010
+#define SSTATUS_SPIE        0x00000020
+#define SSTATUS_SPP         0x00000100
+#define SSTATUS_FS          0x00006000
+#define SSTATUS_XS          0x00018000
+#define SSTATUS_SUM         0x00040000
+#define SSTATUS_MXR         0x00080000
+#define SSTATUS32_SD        0x80000000
+#define SSTATUS_UXL         0x0000000300000000
+#define SSTATUS64_SD        0x8000000000000000
+
+#define DCSR_XDEBUGVER      (3U<<30)
+#define DCSR_NDRESET        (1<<29)
+#define DCSR_FULLRESET      (1<<28)
+#define DCSR_EBREAKM        (1<<15)
+#define DCSR_EBREAKH        (1<<14)
+#define DCSR_EBREAKS        (1<<13)
+#define DCSR_EBREAKU        (1<<12)
+#define DCSR_STOPCYCLE      (1<<10)
+#define DCSR_STOPTIME       (1<<9)
+#define DCSR_CAUSE          (7<<6)
+#define DCSR_DEBUGINT       (1<<5)
+#define DCSR_HALT           (1<<3)
+#define DCSR_STEP           (1<<2)
+#define DCSR_PRV            (3<<0)
+
+#define DCSR_CAUSE_NONE     0
+#define DCSR_CAUSE_SWBP     1
+#define DCSR_CAUSE_HWBP     2
+#define DCSR_CAUSE_DEBUGINT 3
+#define DCSR_CAUSE_STEP     4
+#define DCSR_CAUSE_HALT     5
+
+#define MCONTROL_TYPE(xlen)    (0xfULL<<((xlen)-4))
+#define MCONTROL_DMODE(xlen)   (1ULL<<((xlen)-5))
+#define MCONTROL_MASKMAX(xlen) (0x3fULL<<((xlen)-11))
+
+#define MCONTROL_SELECT     (1<<19)
+#define MCONTROL_TIMING     (1<<18)
+#define MCONTROL_ACTION     (0x3f<<12)
+#define MCONTROL_CHAIN      (1<<11)
+#define MCONTROL_MATCH      (0xf<<7)
+#define MCONTROL_M          (1<<6)
+#define MCONTROL_H          (1<<5)
+#define MCONTROL_S          (1<<4)
+#define MCONTROL_U          (1<<3)
+#define MCONTROL_EXECUTE    (1<<2)
+#define MCONTROL_STORE      (1<<1)
+#define MCONTROL_LOAD       (1<<0)
+
+#define MCONTROL_TYPE_NONE      0
+#define MCONTROL_TYPE_MATCH     2
+
+#define MCONTROL_ACTION_DEBUG_EXCEPTION   0
+#define MCONTROL_ACTION_DEBUG_MODE        1
+#define MCONTROL_ACTION_TRACE_START       2
+#define MCONTROL_ACTION_TRACE_STOP        3
+#define MCONTROL_ACTION_TRACE_EMIT        4
+
+#define MCONTROL_MATCH_EQUAL     0
+#define MCONTROL_MATCH_NAPOT     1
+#define MCONTROL_MATCH_GE        2
+#define MCONTROL_MATCH_LT        3
+#define MCONTROL_MATCH_MASK_LOW  4
+#define MCONTROL_MATCH_MASK_HIGH 5
+
+#define MIP_SSIP            (1 << IRQ_S_SOFT)
+#define MIP_HSIP            (1 << IRQ_H_SOFT)
+#define MIP_MSIP            (1 << IRQ_M_SOFT)
+#define MIP_STIP            (1 << IRQ_S_TIMER)
+#define MIP_HTIP            (1 << IRQ_H_TIMER)
+#define MIP_MTIP            (1 << IRQ_M_TIMER)
+#define MIP_SEIP            (1 << IRQ_S_EXT)
+#define MIP_HEIP            (1 << IRQ_H_EXT)
+#define MIP_MEIP            (1 << IRQ_M_EXT)
+
+#define SIP_SSIP MIP_SSIP
+#define SIP_STIP MIP_STIP
+
+#define PRV_U 0
+#define PRV_S 1
+#define PRV_H 2
+#define PRV_M 3
+
+#define SATP32_MODE 0x80000000
+#define SATP32_ASID 0x7FC00000
+#define SATP32_PPN  0x003FFFFF
+#define SATP64_MODE 0xF000000000000000
+#define SATP64_ASID 0x0FFFF00000000000
+#define SATP64_PPN  0x00000FFFFFFFFFFF
+
+#define SATP_MODE_OFF  0
+#define SATP_MODE_SV32 1
+#define SATP_MODE_SV39 8
+#define SATP_MODE_SV48 9
+#define SATP_MODE_SV57 10
+#define SATP_MODE_SV64 11
+
+#define PMP_R     0x01
+#define PMP_W     0x02
+#define PMP_X     0x04
+#define PMP_A     0x18
+#define PMP_L     0x80
+#define PMP_SHIFT 2
+
+#define PMP_TOR   0x08
+#define PMP_NA4   0x10
+#define PMP_NAPOT 0x18
+
+#define IRQ_S_SOFT   1
+#define IRQ_H_SOFT   2
+#define IRQ_M_SOFT   3
+#define IRQ_S_TIMER  5
+#define IRQ_H_TIMER  6
+#define IRQ_M_TIMER  7
+#define IRQ_S_EXT    9
+#define IRQ_H_EXT    10
+#define IRQ_M_EXT    11
+#define IRQ_COP      12
+#define IRQ_HOST     13
+
+#define DEFAULT_RSTVEC     0x00001000
+#define CLINT_BASE         0x02000000
+#define CLINT_SIZE         0x000c0000
+#define EXT_IO_BASE        0x40000000
+#define DRAM_BASE          0x80000000
+
+// page table entry (PTE) fields
+#define PTE_V     0x001 // Valid
+#define PTE_R     0x002 // Read
+#define PTE_W     0x004 // Write
+#define PTE_X     0x008 // Execute
+#define PTE_U     0x010 // User
+#define PTE_G     0x020 // Global
+#define PTE_A     0x040 // Accessed
+#define PTE_D     0x080 // Dirty
+#define PTE_SOFT  0x300 // Reserved for Software
+
+#define PTE_PPN_SHIFT 10
+
+#define PTE_TABLE(PTE) (((PTE) & (PTE_V | PTE_R | PTE_W | PTE_X)) == PTE_V)
+
+#ifdef __riscv
+
+#if __riscv_xlen == 64
+# define MSTATUS_SD MSTATUS64_SD
+# define SSTATUS_SD SSTATUS64_SD
+# define RISCV_PGLEVEL_BITS 9
+# define SATP_MODE SATP64_MODE
+#else
+# define MSTATUS_SD MSTATUS32_SD
+# define SSTATUS_SD SSTATUS32_SD
+# define RISCV_PGLEVEL_BITS 10
+# define SATP_MODE SATP32_MODE
+#endif
+#define RISCV_PGSHIFT 12
+#define RISCV_PGSIZE (1 << RISCV_PGSHIFT)
+#define RISCV_PTENUM 512
+
+#ifndef __ASSEMBLER__
+
+#ifdef __GNUC__
+
+#define read_csr(reg) ({ unsigned long __tmp; \
+  asm volatile ("csrr %0, " #reg : "=r"(__tmp)); \
+  __tmp; })
+
+#define write_csr(reg, val) ({ \
+  asm volatile ("csrw " #reg ", %0" :: "rK"(val)); })
+
+#define swap_csr(reg, val) ({ unsigned long __tmp; \
+  asm volatile ("csrrw %0, " #reg ", %1" : "=r"(__tmp) : "rK"(val)); \
+  __tmp; })
+
+#define set_csr(reg, bit) ({ unsigned long __tmp; \
+  asm volatile ("csrrs %0, " #reg ", %1" : "=r"(__tmp) : "rK"(bit)); \
+  __tmp; })
+
+#define clear_csr(reg, bit) ({ unsigned long __tmp; \
+  asm volatile ("csrrc %0, " #reg ", %1" : "=r"(__tmp) : "rK"(bit)); \
+  __tmp; })
+
+#define rdtime() read_csr(time)
+#define rdcycle() read_csr(cycle)
+#define rdinstret() read_csr(instret)
+
+#endif
+
+#endif
+
+#endif
+
+#endif
+/* Automatically generated by parse-opcodes.  */
+#ifndef RISCV_ENCODING_H
+#define RISCV_ENCODING_H
+#define MATCH_BEQ 0x63
+#define MASK_BEQ  0x707f
+#define MATCH_BNE 0x1063
+#define MASK_BNE  0x707f
+#define MATCH_BLT 0x4063
+#define MASK_BLT  0x707f
+#define MATCH_BGE 0x5063
+#define MASK_BGE  0x707f
+#define MATCH_BLTU 0x6063
+#define MASK_BLTU  0x707f
+#define MATCH_BGEU 0x7063
+#define MASK_BGEU  0x707f
+#define MATCH_JALR 0x67
+#define MASK_JALR  0x707f
+#define MATCH_JAL 0x6f
+#define MASK_JAL  0x7f
+#define MATCH_LUI 0x37
+#define MASK_LUI  0x7f
+#define MATCH_AUIPC 0x17
+#define MASK_AUIPC  0x7f
+#define MATCH_ADDI 0x13
+#define MASK_ADDI  0x707f
+#define MATCH_SLLI 0x1013
+#define MASK_SLLI  0xfc00707f
+#define MATCH_SLTI 0x2013
+#define MASK_SLTI  0x707f
+#define MATCH_SLTIU 0x3013
+#define MASK_SLTIU  0x707f
+#define MATCH_XORI 0x4013
+#define MASK_XORI  0x707f
+#define MATCH_SRLI 0x5013
+#define MASK_SRLI  0xfc00707f
+#define MATCH_SRAI 0x40005013
+#define MASK_SRAI  0xfc00707f
+#define MATCH_ORI 0x6013
+#define MASK_ORI  0x707f
+#define MATCH_ANDI 0x7013
+#define MASK_ANDI  0x707f
+#define MATCH_ADD 0x33
+#define MASK_ADD  0xfe00707f
+#define MATCH_SUB 0x40000033
+#define MASK_SUB  0xfe00707f
+#define MATCH_SLL 0x1033
+#define MASK_SLL  0xfe00707f
+#define MATCH_SLT 0x2033
+#define MASK_SLT  0xfe00707f
+#define MATCH_SLTU 0x3033
+#define MASK_SLTU  0xfe00707f
+#define MATCH_XOR 0x4033
+#define MASK_XOR  0xfe00707f
+#define MATCH_SRL 0x5033
+#define MASK_SRL  0xfe00707f
+#define MATCH_SRA 0x40005033
+#define MASK_SRA  0xfe00707f
+#define MATCH_OR 0x6033
+#define MASK_OR  0xfe00707f
+#define MATCH_AND 0x7033
+#define MASK_AND  0xfe00707f
+#define MATCH_ADDIW 0x1b
+#define MASK_ADDIW  0x707f
+#define MATCH_SLLIW 0x101b
+#define MASK_SLLIW  0xfe00707f
+#define MATCH_SRLIW 0x501b
+#define MASK_SRLIW  0xfe00707f
+#define MATCH_SRAIW 0x4000501b
+#define MASK_SRAIW  0xfe00707f
+#define MATCH_ADDW 0x3b
+#define MASK_ADDW  0xfe00707f
+#define MATCH_SUBW 0x4000003b
+#define MASK_SUBW  0xfe00707f
+#define MATCH_SLLW 0x103b
+#define MASK_SLLW  0xfe00707f
+#define MATCH_SRLW 0x503b
+#define MASK_SRLW  0xfe00707f
+#define MATCH_SRAW 0x4000503b
+#define MASK_SRAW  0xfe00707f
+#define MATCH_LB 0x3
+#define MASK_LB  0x707f
+#define MATCH_LH 0x1003
+#define MASK_LH  0x707f
+#define MATCH_LW 0x2003
+#define MASK_LW  0x707f
+#define MATCH_LD 0x3003
+#define MASK_LD  0x707f
+#define MATCH_LBU 0x4003
+#define MASK_LBU  0x707f
+#define MATCH_LHU 0x5003
+#define MASK_LHU  0x707f
+#define MATCH_LWU 0x6003
+#define MASK_LWU  0x707f
+#define MATCH_SB 0x23
+#define MASK_SB  0x707f
+#define MATCH_SH 0x1023
+#define MASK_SH  0x707f
+#define MATCH_SW 0x2023
+#define MASK_SW  0x707f
+#define MATCH_SD 0x3023
+#define MASK_SD  0x707f
+#define MATCH_FENCE 0xf
+#define MASK_FENCE  0x707f
+#define MATCH_FENCE_I 0x100f
+#define MASK_FENCE_I  0x707f
+#define MATCH_MUL 0x2000033
+#define MASK_MUL  0xfe00707f
+#define MATCH_MULH 0x2001033
+#define MASK_MULH  0xfe00707f
+#define MATCH_MULHSU 0x2002033
+#define MASK_MULHSU  0xfe00707f
+#define MATCH_MULHU 0x2003033
+#define MASK_MULHU  0xfe00707f
+#define MATCH_DIV 0x2004033
+#define MASK_DIV  0xfe00707f
+#define MATCH_DIVU 0x2005033
+#define MASK_DIVU  0xfe00707f
+#define MATCH_REM 0x2006033
+#define MASK_REM  0xfe00707f
+#define MATCH_REMU 0x2007033
+#define MASK_REMU  0xfe00707f
+#define MATCH_MULW 0x200003b
+#define MASK_MULW  0xfe00707f
+#define MATCH_DIVW 0x200403b
+#define MASK_DIVW  0xfe00707f
+#define MATCH_DIVUW 0x200503b
+#define MASK_DIVUW  0xfe00707f
+#define MATCH_REMW 0x200603b
+#define MASK_REMW  0xfe00707f
+#define MATCH_REMUW 0x200703b
+#define MASK_REMUW  0xfe00707f
+#define MATCH_AMOADD_W 0x202f
+#define MASK_AMOADD_W  0xf800707f
+#define MATCH_AMOXOR_W 0x2000202f
+#define MASK_AMOXOR_W  0xf800707f
+#define MATCH_AMOOR_W 0x4000202f
+#define MASK_AMOOR_W  0xf800707f
+#define MATCH_AMOAND_W 0x6000202f
+#define MASK_AMOAND_W  0xf800707f
+#define MATCH_AMOMIN_W 0x8000202f
+#define MASK_AMOMIN_W  0xf800707f
+#define MATCH_AMOMAX_W 0xa000202f
+#define MASK_AMOMAX_W  0xf800707f
+#define MATCH_AMOMINU_W 0xc000202f
+#define MASK_AMOMINU_W  0xf800707f
+#define MATCH_AMOMAXU_W 0xe000202f
+#define MASK_AMOMAXU_W  0xf800707f
+#define MATCH_AMOSWAP_W 0x800202f
+#define MASK_AMOSWAP_W  0xf800707f
+#define MATCH_LR_W 0x1000202f
+#define MASK_LR_W  0xf9f0707f
+#define MATCH_SC_W 0x1800202f
+#define MASK_SC_W  0xf800707f
+#define MATCH_AMOADD_D 0x302f
+#define MASK_AMOADD_D  0xf800707f
+#define MATCH_AMOXOR_D 0x2000302f
+#define MASK_AMOXOR_D  0xf800707f
+#define MATCH_AMOOR_D 0x4000302f
+#define MASK_AMOOR_D  0xf800707f
+#define MATCH_AMOAND_D 0x6000302f
+#define MASK_AMOAND_D  0xf800707f
+#define MATCH_AMOMIN_D 0x8000302f
+#define MASK_AMOMIN_D  0xf800707f
+#define MATCH_AMOMAX_D 0xa000302f
+#define MASK_AMOMAX_D  0xf800707f
+#define MATCH_AMOMINU_D 0xc000302f
+#define MASK_AMOMINU_D  0xf800707f
+#define MATCH_AMOMAXU_D 0xe000302f
+#define MASK_AMOMAXU_D  0xf800707f
+#define MATCH_AMOSWAP_D 0x800302f
+#define MASK_AMOSWAP_D  0xf800707f
+#define MATCH_LR_D 0x1000302f
+#define MASK_LR_D  0xf9f0707f
+#define MATCH_SC_D 0x1800302f
+#define MASK_SC_D  0xf800707f
+#define MATCH_ECALL 0x73
+#define MASK_ECALL  0xffffffff
+#define MATCH_EBREAK 0x100073
+#define MASK_EBREAK  0xffffffff
+#define MATCH_URET 0x200073
+#define MASK_URET  0xffffffff
+#define MATCH_SRET 0x10200073
+#define MASK_SRET  0xffffffff
+#define MATCH_MRET 0x30200073
+#define MASK_MRET  0xffffffff
+#define MATCH_DRET 0x7b200073
+#define MASK_DRET  0xffffffff
+#define MATCH_SFENCE_VMA 0x12000073
+#define MASK_SFENCE_VMA  0xfe007fff
+#define MATCH_WFI 0x10500073
+#define MASK_WFI  0xffffffff
+#define MATCH_CSRRW 0x1073
+#define MASK_CSRRW  0x707f
+#define MATCH_CSRRS 0x2073
+#define MASK_CSRRS  0x707f
+#define MATCH_CSRRC 0x3073
+#define MASK_CSRRC  0x707f
+#define MATCH_CSRRWI 0x5073
+#define MASK_CSRRWI  0x707f
+#define MATCH_CSRRSI 0x6073
+#define MASK_CSRRSI  0x707f
+#define MATCH_CSRRCI 0x7073
+#define MASK_CSRRCI  0x707f
+#define MATCH_FADD_S 0x53
+#define MASK_FADD_S  0xfe00007f
+#define MATCH_FSUB_S 0x8000053
+#define MASK_FSUB_S  0xfe00007f
+#define MATCH_FMUL_S 0x10000053
+#define MASK_FMUL_S  0xfe00007f
+#define MATCH_FDIV_S 0x18000053
+#define MASK_FDIV_S  0xfe00007f
+#define MATCH_FSGNJ_S 0x20000053
+#define MASK_FSGNJ_S  0xfe00707f
+#define MATCH_FSGNJN_S 0x20001053
+#define MASK_FSGNJN_S  0xfe00707f
+#define MATCH_FSGNJX_S 0x20002053
+#define MASK_FSGNJX_S  0xfe00707f
+#define MATCH_FMIN_S 0x28000053
+#define MASK_FMIN_S  0xfe00707f
+#define MATCH_FMAX_S 0x28001053
+#define MASK_FMAX_S  0xfe00707f
+#define MATCH_FSQRT_S 0x58000053
+#define MASK_FSQRT_S  0xfff0007f
+#define MATCH_FADD_D 0x2000053
+#define MASK_FADD_D  0xfe00007f
+#define MATCH_FSUB_D 0xa000053
+#define MASK_FSUB_D  0xfe00007f
+#define MATCH_FMUL_D 0x12000053
+#define MASK_FMUL_D  0xfe00007f
+#define MATCH_FDIV_D 0x1a000053
+#define MASK_FDIV_D  0xfe00007f
+#define MATCH_FSGNJ_D 0x22000053
+#define MASK_FSGNJ_D  0xfe00707f
+#define MATCH_FSGNJN_D 0x22001053
+#define MASK_FSGNJN_D  0xfe00707f
+#define MATCH_FSGNJX_D 0x22002053
+#define MASK_FSGNJX_D  0xfe00707f
+#define MATCH_FMIN_D 0x2a000053
+#define MASK_FMIN_D  0xfe00707f
+#define MATCH_FMAX_D 0x2a001053
+#define MASK_FMAX_D  0xfe00707f
+#define MATCH_FCVT_S_D 0x40100053
+#define MASK_FCVT_S_D  0xfff0007f
+#define MATCH_FCVT_D_S 0x42000053
+#define MASK_FCVT_D_S  0xfff0007f
+#define MATCH_FSQRT_D 0x5a000053
+#define MASK_FSQRT_D  0xfff0007f
+#define MATCH_FADD_Q 0x6000053
+#define MASK_FADD_Q  0xfe00007f
+#define MATCH_FSUB_Q 0xe000053
+#define MASK_FSUB_Q  0xfe00007f
+#define MATCH_FMUL_Q 0x16000053
+#define MASK_FMUL_Q  0xfe00007f
+#define MATCH_FDIV_Q 0x1e000053
+#define MASK_FDIV_Q  0xfe00007f
+#define MATCH_FSGNJ_Q 0x26000053
+#define MASK_FSGNJ_Q  0xfe00707f
+#define MATCH_FSGNJN_Q 0x26001053
+#define MASK_FSGNJN_Q  0xfe00707f
+#define MATCH_FSGNJX_Q 0x26002053
+#define MASK_FSGNJX_Q  0xfe00707f
+#define MATCH_FMIN_Q 0x2e000053
+#define MASK_FMIN_Q  0xfe00707f
+#define MATCH_FMAX_Q 0x2e001053
+#define MASK_FMAX_Q  0xfe00707f
+#define MATCH_FCVT_S_Q 0x40300053
+#define MASK_FCVT_S_Q  0xfff0007f
+#define MATCH_FCVT_Q_S 0x46000053
+#define MASK_FCVT_Q_S  0xfff0007f
+#define MATCH_FCVT_D_Q 0x42300053
+#define MASK_FCVT_D_Q  0xfff0007f
+#define MATCH_FCVT_Q_D 0x46100053
+#define MASK_FCVT_Q_D  0xfff0007f
+#define MATCH_FSQRT_Q 0x5e000053
+#define MASK_FSQRT_Q  0xfff0007f
+#define MATCH_FLE_S 0xa0000053
+#define MASK_FLE_S  0xfe00707f
+#define MATCH_FLT_S 0xa0001053
+#define MASK_FLT_S  0xfe00707f
+#define MATCH_FEQ_S 0xa0002053
+#define MASK_FEQ_S  0xfe00707f
+#define MATCH_FLE_D 0xa2000053
+#define MASK_FLE_D  0xfe00707f
+#define MATCH_FLT_D 0xa2001053
+#define MASK_FLT_D  0xfe00707f
+#define MATCH_FEQ_D 0xa2002053
+#define MASK_FEQ_D  0xfe00707f
+#define MATCH_FLE_Q 0xa6000053
+#define MASK_FLE_Q  0xfe00707f
+#define MATCH_FLT_Q 0xa6001053
+#define MASK_FLT_Q  0xfe00707f
+#define MATCH_FEQ_Q 0xa6002053
+#define MASK_FEQ_Q  0xfe00707f
+#define MATCH_FCVT_W_S 0xc0000053
+#define MASK_FCVT_W_S  0xfff0007f
+#define MATCH_FCVT_WU_S 0xc0100053
+#define MASK_FCVT_WU_S  0xfff0007f
+#define MATCH_FCVT_L_S 0xc0200053
+#define MASK_FCVT_L_S  0xfff0007f
+#define MATCH_FCVT_LU_S 0xc0300053
+#define MASK_FCVT_LU_S  0xfff0007f
+#define MATCH_FMV_X_W 0xe0000053
+#define MASK_FMV_X_W  0xfff0707f
+#define MATCH_FCLASS_S 0xe0001053
+#define MASK_FCLASS_S  0xfff0707f
+#define MATCH_FCVT_W_D 0xc2000053
+#define MASK_FCVT_W_D  0xfff0007f
+#define MATCH_FCVT_WU_D 0xc2100053
+#define MASK_FCVT_WU_D  0xfff0007f
+#define MATCH_FCVT_L_D 0xc2200053
+#define MASK_FCVT_L_D  0xfff0007f
+#define MATCH_FCVT_LU_D 0xc2300053
+#define MASK_FCVT_LU_D  0xfff0007f
+#define MATCH_FMV_X_D 0xe2000053
+#define MASK_FMV_X_D  0xfff0707f
+#define MATCH_FCLASS_D 0xe2001053
+#define MASK_FCLASS_D  0xfff0707f
+#define MATCH_FCVT_W_Q 0xc6000053
+#define MASK_FCVT_W_Q  0xfff0007f
+#define MATCH_FCVT_WU_Q 0xc6100053
+#define MASK_FCVT_WU_Q  0xfff0007f
+#define MATCH_FCVT_L_Q 0xc6200053
+#define MASK_FCVT_L_Q  0xfff0007f
+#define MATCH_FCVT_LU_Q 0xc6300053
+#define MASK_FCVT_LU_Q  0xfff0007f
+#define MATCH_FMV_X_Q 0xe6000053
+#define MASK_FMV_X_Q  0xfff0707f
+#define MATCH_FCLASS_Q 0xe6001053
+#define MASK_FCLASS_Q  0xfff0707f
+#define MATCH_FCVT_S_W 0xd0000053
+#define MASK_FCVT_S_W  0xfff0007f
+#define MATCH_FCVT_S_WU 0xd0100053
+#define MASK_FCVT_S_WU  0xfff0007f
+#define MATCH_FCVT_S_L 0xd0200053
+#define MASK_FCVT_S_L  0xfff0007f
+#define MATCH_FCVT_S_LU 0xd0300053
+#define MASK_FCVT_S_LU  0xfff0007f
+#define MATCH_FMV_W_X 0xf0000053
+#define MASK_FMV_W_X  0xfff0707f
+#define MATCH_FCVT_D_W 0xd2000053
+#define MASK_FCVT_D_W  0xfff0007f
+#define MATCH_FCVT_D_WU 0xd2100053
+#define MASK_FCVT_D_WU  0xfff0007f
+#define MATCH_FCVT_D_L 0xd2200053
+#define MASK_FCVT_D_L  0xfff0007f
+#define MATCH_FCVT_D_LU 0xd2300053
+#define MASK_FCVT_D_LU  0xfff0007f
+#define MATCH_FMV_D_X 0xf2000053
+#define MASK_FMV_D_X  0xfff0707f
+#define MATCH_FCVT_Q_W 0xd6000053
+#define MASK_FCVT_Q_W  0xfff0007f
+#define MATCH_FCVT_Q_WU 0xd6100053
+#define MASK_FCVT_Q_WU  0xfff0007f
+#define MATCH_FCVT_Q_L 0xd6200053
+#define MASK_FCVT_Q_L  0xfff0007f
+#define MATCH_FCVT_Q_LU 0xd6300053
+#define MASK_FCVT_Q_LU  0xfff0007f
+#define MATCH_FMV_Q_X 0xf6000053
+#define MASK_FMV_Q_X  0xfff0707f
+#define MATCH_FLW 0x2007
+#define MASK_FLW  0x707f
+#define MATCH_FLD 0x3007
+#define MASK_FLD  0x707f
+#define MATCH_FLQ 0x4007
+#define MASK_FLQ  0x707f
+#define MATCH_FSW 0x2027
+#define MASK_FSW  0x707f
+#define MATCH_FSD 0x3027
+#define MASK_FSD  0x707f
+#define MATCH_FSQ 0x4027
+#define MASK_FSQ  0x707f
+#define MATCH_FMADD_S 0x43
+#define MASK_FMADD_S  0x600007f
+#define MATCH_FMSUB_S 0x47
+#define MASK_FMSUB_S  0x600007f
+#define MATCH_FNMSUB_S 0x4b
+#define MASK_FNMSUB_S  0x600007f
+#define MATCH_FNMADD_S 0x4f
+#define MASK_FNMADD_S  0x600007f
+#define MATCH_FMADD_D 0x2000043
+#define MASK_FMADD_D  0x600007f
+#define MATCH_FMSUB_D 0x2000047
+#define MASK_FMSUB_D  0x600007f
+#define MATCH_FNMSUB_D 0x200004b
+#define MASK_FNMSUB_D  0x600007f
+#define MATCH_FNMADD_D 0x200004f
+#define MASK_FNMADD_D  0x600007f
+#define MATCH_FMADD_Q 0x6000043
+#define MASK_FMADD_Q  0x600007f
+#define MATCH_FMSUB_Q 0x6000047
+#define MASK_FMSUB_Q  0x600007f
+#define MATCH_FNMSUB_Q 0x600004b
+#define MASK_FNMSUB_Q  0x600007f
+#define MATCH_FNMADD_Q 0x600004f
+#define MASK_FNMADD_Q  0x600007f
+#define MATCH_C_NOP 0x1
+#define MASK_C_NOP  0xffff
+#define MATCH_C_ADDI16SP 0x6101
+#define MASK_C_ADDI16SP  0xef83
+#define MATCH_C_JR 0x8002
+#define MASK_C_JR  0xf07f
+#define MATCH_C_JALR 0x9002
+#define MASK_C_JALR  0xf07f
+#define MATCH_C_EBREAK 0x9002
+#define MASK_C_EBREAK  0xffff
+#define MATCH_C_LD 0x6000
+#define MASK_C_LD  0xe003
+#define MATCH_C_SD 0xe000
+#define MASK_C_SD  0xe003
+#define MATCH_C_ADDIW 0x2001
+#define MASK_C_ADDIW  0xe003
+#define MATCH_C_LDSP 0x6002
+#define MASK_C_LDSP  0xe003
+#define MATCH_C_SDSP 0xe002
+#define MASK_C_SDSP  0xe003
+#define MATCH_C_ADDI4SPN 0x0
+#define MASK_C_ADDI4SPN  0xe003
+#define MATCH_C_FLD 0x2000
+#define MASK_C_FLD  0xe003
+#define MATCH_C_LW 0x4000
+#define MASK_C_LW  0xe003
+#define MATCH_C_FLW 0x6000
+#define MASK_C_FLW  0xe003
+#define MATCH_C_FSD 0xa000
+#define MASK_C_FSD  0xe003
+#define MATCH_C_SW 0xc000
+#define MASK_C_SW  0xe003
+#define MATCH_C_FSW 0xe000
+#define MASK_C_FSW  0xe003
+#define MATCH_C_ADDI 0x1
+#define MASK_C_ADDI  0xe003
+#define MATCH_C_JAL 0x2001
+#define MASK_C_JAL  0xe003
+#define MATCH_C_LI 0x4001
+#define MASK_C_LI  0xe003
+#define MATCH_C_LUI 0x6001
+#define MASK_C_LUI  0xe003
+#define MATCH_C_SRLI 0x8001
+#define MASK_C_SRLI  0xec03
+#define MATCH_C_SRAI 0x8401
+#define MASK_C_SRAI  0xec03
+#define MATCH_C_ANDI 0x8801
+#define MASK_C_ANDI  0xec03
+#define MATCH_C_SUB 0x8c01
+#define MASK_C_SUB  0xfc63
+#define MATCH_C_XOR 0x8c21
+#define MASK_C_XOR  0xfc63
+#define MATCH_C_OR 0x8c41
+#define MASK_C_OR  0xfc63
+#define MATCH_C_AND 0x8c61
+#define MASK_C_AND  0xfc63
+#define MATCH_C_SUBW 0x9c01
+#define MASK_C_SUBW  0xfc63
+#define MATCH_C_ADDW 0x9c21
+#define MASK_C_ADDW  0xfc63
+#define MATCH_C_J 0xa001
+#define MASK_C_J  0xe003
+#define MATCH_C_BEQZ 0xc001
+#define MASK_C_BEQZ  0xe003
+#define MATCH_C_BNEZ 0xe001
+#define MASK_C_BNEZ  0xe003
+#define MATCH_C_SLLI 0x2
+#define MASK_C_SLLI  0xe003
+#define MATCH_C_FLDSP 0x2002
+#define MASK_C_FLDSP  0xe003
+#define MATCH_C_LWSP 0x4002
+#define MASK_C_LWSP  0xe003
+#define MATCH_C_FLWSP 0x6002
+#define MASK_C_FLWSP  0xe003
+#define MATCH_C_MV 0x8002
+#define MASK_C_MV  0xf003
+#define MATCH_C_ADD 0x9002
+#define MASK_C_ADD  0xf003
+#define MATCH_C_FSDSP 0xa002
+#define MASK_C_FSDSP  0xe003
+#define MATCH_C_SWSP 0xc002
+#define MASK_C_SWSP  0xe003
+#define MATCH_C_FSWSP 0xe002
+#define MASK_C_FSWSP  0xe003
+#define MATCH_CUSTOM0 0xb
+#define MASK_CUSTOM0  0x707f
+#define MATCH_CUSTOM0_RS1 0x200b
+#define MASK_CUSTOM0_RS1  0x707f
+#define MATCH_CUSTOM0_RS1_RS2 0x300b
+#define MASK_CUSTOM0_RS1_RS2  0x707f
+#define MATCH_CUSTOM0_RD 0x400b
+#define MASK_CUSTOM0_RD  0x707f
+#define MATCH_CUSTOM0_RD_RS1 0x600b
+#define MASK_CUSTOM0_RD_RS1  0x707f
+#define MATCH_CUSTOM0_RD_RS1_RS2 0x700b
+#define MASK_CUSTOM0_RD_RS1_RS2  0x707f
+#define MATCH_CUSTOM1 0x2b
+#define MASK_CUSTOM1  0x707f
+#define MATCH_CUSTOM1_RS1 0x202b
+#define MASK_CUSTOM1_RS1  0x707f
+#define MATCH_CUSTOM1_RS1_RS2 0x302b
+#define MASK_CUSTOM1_RS1_RS2  0x707f
+#define MATCH_CUSTOM1_RD 0x402b
+#define MASK_CUSTOM1_RD  0x707f
+#define MATCH_CUSTOM1_RD_RS1 0x602b
+#define MASK_CUSTOM1_RD_RS1  0x707f
+#define MATCH_CUSTOM1_RD_RS1_RS2 0x702b
+#define MASK_CUSTOM1_RD_RS1_RS2  0x707f
+#define MATCH_CUSTOM2 0x5b
+#define MASK_CUSTOM2  0x707f
+#define MATCH_CUSTOM2_RS1 0x205b
+#define MASK_CUSTOM2_RS1  0x707f
+#define MATCH_CUSTOM2_RS1_RS2 0x305b
+#define MASK_CUSTOM2_RS1_RS2  0x707f
+#define MATCH_CUSTOM2_RD 0x405b
+#define MASK_CUSTOM2_RD  0x707f
+#define MATCH_CUSTOM2_RD_RS1 0x605b
+#define MASK_CUSTOM2_RD_RS1  0x707f
+#define MATCH_CUSTOM2_RD_RS1_RS2 0x705b
+#define MASK_CUSTOM2_RD_RS1_RS2  0x707f
+#define MATCH_CUSTOM3 0x7b
+#define MASK_CUSTOM3  0x707f
+#define MATCH_CUSTOM3_RS1 0x207b
+#define MASK_CUSTOM3_RS1  0x707f
+#define MATCH_CUSTOM3_RS1_RS2 0x307b
+#define MASK_CUSTOM3_RS1_RS2  0x707f
+#define MATCH_CUSTOM3_RD 0x407b
+#define MASK_CUSTOM3_RD  0x707f
+#define MATCH_CUSTOM3_RD_RS1 0x607b
+#define MASK_CUSTOM3_RD_RS1  0x707f
+#define MATCH_CUSTOM3_RD_RS1_RS2 0x707b
+#define MASK_CUSTOM3_RD_RS1_RS2  0x707f
+#define CSR_FFLAGS 0x1
+#define CSR_FRM 0x2
+#define CSR_FCSR 0x3
+#define CSR_CYCLE 0xc00
+#define CSR_TIME 0xc01
+#define CSR_INSTRET 0xc02
+#define CSR_HPMCOUNTER3 0xc03
+#define CSR_HPMCOUNTER4 0xc04
+#define CSR_HPMCOUNTER5 0xc05
+#define CSR_HPMCOUNTER6 0xc06
+#define CSR_HPMCOUNTER7 0xc07
+#define CSR_HPMCOUNTER8 0xc08
+#define CSR_HPMCOUNTER9 0xc09
+#define CSR_HPMCOUNTER10 0xc0a
+#define CSR_HPMCOUNTER11 0xc0b
+#define CSR_HPMCOUNTER12 0xc0c
+#define CSR_HPMCOUNTER13 0xc0d
+#define CSR_HPMCOUNTER14 0xc0e
+#define CSR_HPMCOUNTER15 0xc0f
+#define CSR_HPMCOUNTER16 0xc10
+#define CSR_HPMCOUNTER17 0xc11
+#define CSR_HPMCOUNTER18 0xc12
+#define CSR_HPMCOUNTER19 0xc13
+#define CSR_HPMCOUNTER20 0xc14
+#define CSR_HPMCOUNTER21 0xc15
+#define CSR_HPMCOUNTER22 0xc16
+#define CSR_HPMCOUNTER23 0xc17
+#define CSR_HPMCOUNTER24 0xc18
+#define CSR_HPMCOUNTER25 0xc19
+#define CSR_HPMCOUNTER26 0xc1a
+#define CSR_HPMCOUNTER27 0xc1b
+#define CSR_HPMCOUNTER28 0xc1c
+#define CSR_HPMCOUNTER29 0xc1d
+#define CSR_HPMCOUNTER30 0xc1e
+#define CSR_HPMCOUNTER31 0xc1f
+#define CSR_SSTATUS 0x100
+#define CSR_SIE 0x104
+#define CSR_STVEC 0x105
+#define CSR_SCOUNTEREN 0x106
+#define CSR_SSCRATCH 0x140
+#define CSR_SEPC 0x141
+#define CSR_SCAUSE 0x142
+#define CSR_STVAL 0x143
+#define CSR_SIP 0x144
+#define CSR_SATP 0x180
+#define CSR_MSTATUS 0x300
+#define CSR_MISA 0x301
+#define CSR_MEDELEG 0x302
+#define CSR_MIDELEG 0x303
+#define CSR_MIE 0x304
+#define CSR_MTVEC 0x305
+#define CSR_MCOUNTEREN 0x306
+#define CSR_MSCRATCH 0x340
+#define CSR_MEPC 0x341
+#define CSR_MCAUSE 0x342
+#define CSR_MTVAL 0x343
+#define CSR_MIP 0x344
+#define CSR_PMPCFG0 0x3a0
+#define CSR_PMPCFG1 0x3a1
+#define CSR_PMPCFG2 0x3a2
+#define CSR_PMPCFG3 0x3a3
+#define CSR_PMPADDR0 0x3b0
+#define CSR_PMPADDR1 0x3b1
+#define CSR_PMPADDR2 0x3b2
+#define CSR_PMPADDR3 0x3b3
+#define CSR_PMPADDR4 0x3b4
+#define CSR_PMPADDR5 0x3b5
+#define CSR_PMPADDR6 0x3b6
+#define CSR_PMPADDR7 0x3b7
+#define CSR_PMPADDR8 0x3b8
+#define CSR_PMPADDR9 0x3b9
+#define CSR_PMPADDR10 0x3ba
+#define CSR_PMPADDR11 0x3bb
+#define CSR_PMPADDR12 0x3bc
+#define CSR_PMPADDR13 0x3bd
+#define CSR_PMPADDR14 0x3be
+#define CSR_PMPADDR15 0x3bf
+#define CSR_TSELECT 0x7a0
+#define CSR_TDATA1 0x7a1
+#define CSR_TDATA2 0x7a2
+#define CSR_TDATA3 0x7a3
+#define CSR_DCSR 0x7b0
+#define CSR_DPC 0x7b1
+#define CSR_DSCRATCH 0x7b2
+#define CSR_MCYCLE 0xb00
+#define CSR_MINSTRET 0xb02
+#define CSR_MHPMCOUNTER3 0xb03
+#define CSR_MHPMCOUNTER4 0xb04
+#define CSR_MHPMCOUNTER5 0xb05
+#define CSR_MHPMCOUNTER6 0xb06
+#define CSR_MHPMCOUNTER7 0xb07
+#define CSR_MHPMCOUNTER8 0xb08
+#define CSR_MHPMCOUNTER9 0xb09
+#define CSR_MHPMCOUNTER10 0xb0a
+#define CSR_MHPMCOUNTER11 0xb0b
+#define CSR_MHPMCOUNTER12 0xb0c
+#define CSR_MHPMCOUNTER13 0xb0d
+#define CSR_MHPMCOUNTER14 0xb0e
+#define CSR_MHPMCOUNTER15 0xb0f
+#define CSR_MHPMCOUNTER16 0xb10
+#define CSR_MHPMCOUNTER17 0xb11
+#define CSR_MHPMCOUNTER18 0xb12
+#define CSR_MHPMCOUNTER19 0xb13
+#define CSR_MHPMCOUNTER20 0xb14
+#define CSR_MHPMCOUNTER21 0xb15
+#define CSR_MHPMCOUNTER22 0xb16
+#define CSR_MHPMCOUNTER23 0xb17
+#define CSR_MHPMCOUNTER24 0xb18
+#define CSR_MHPMCOUNTER25 0xb19
+#define CSR_MHPMCOUNTER26 0xb1a
+#define CSR_MHPMCOUNTER27 0xb1b
+#define CSR_MHPMCOUNTER28 0xb1c
+#define CSR_MHPMCOUNTER29 0xb1d
+#define CSR_MHPMCOUNTER30 0xb1e
+#define CSR_MHPMCOUNTER31 0xb1f
+#define CSR_MHPMEVENT3 0x323
+#define CSR_MHPMEVENT4 0x324
+#define CSR_MHPMEVENT5 0x325
+#define CSR_MHPMEVENT6 0x326
+#define CSR_MHPMEVENT7 0x327
+#define CSR_MHPMEVENT8 0x328
+#define CSR_MHPMEVENT9 0x329
+#define CSR_MHPMEVENT10 0x32a
+#define CSR_MHPMEVENT11 0x32b
+#define CSR_MHPMEVENT12 0x32c
+#define CSR_MHPMEVENT13 0x32d
+#define CSR_MHPMEVENT14 0x32e
+#define CSR_MHPMEVENT15 0x32f
+#define CSR_MHPMEVENT16 0x330
+#define CSR_MHPMEVENT17 0x331
+#define CSR_MHPMEVENT18 0x332
+#define CSR_MHPMEVENT19 0x333
+#define CSR_MHPMEVENT20 0x334
+#define CSR_MHPMEVENT21 0x335
+#define CSR_MHPMEVENT22 0x336
+#define CSR_MHPMEVENT23 0x337
+#define CSR_MHPMEVENT24 0x338
+#define CSR_MHPMEVENT25 0x339
+#define CSR_MHPMEVENT26 0x33a
+#define CSR_MHPMEVENT27 0x33b
+#define CSR_MHPMEVENT28 0x33c
+#define CSR_MHPMEVENT29 0x33d
+#define CSR_MHPMEVENT30 0x33e
+#define CSR_MHPMEVENT31 0x33f
+#define CSR_MVENDORID 0xf11
+#define CSR_MARCHID 0xf12
+#define CSR_MIMPID 0xf13
+#define CSR_MHARTID 0xf14
+#define CSR_CYCLEH 0xc80
+#define CSR_TIMEH 0xc81
+#define CSR_INSTRETH 0xc82
+#define CSR_HPMCOUNTER3H 0xc83
+#define CSR_HPMCOUNTER4H 0xc84
+#define CSR_HPMCOUNTER5H 0xc85
+#define CSR_HPMCOUNTER6H 0xc86
+#define CSR_HPMCOUNTER7H 0xc87
+#define CSR_HPMCOUNTER8H 0xc88
+#define CSR_HPMCOUNTER9H 0xc89
+#define CSR_HPMCOUNTER10H 0xc8a
+#define CSR_HPMCOUNTER11H 0xc8b
+#define CSR_HPMCOUNTER12H 0xc8c
+#define CSR_HPMCOUNTER13H 0xc8d
+#define CSR_HPMCOUNTER14H 0xc8e
+#define CSR_HPMCOUNTER15H 0xc8f
+#define CSR_HPMCOUNTER16H 0xc90
+#define CSR_HPMCOUNTER17H 0xc91
+#define CSR_HPMCOUNTER18H 0xc92
+#define CSR_HPMCOUNTER19H 0xc93
+#define CSR_HPMCOUNTER20H 0xc94
+#define CSR_HPMCOUNTER21H 0xc95
+#define CSR_HPMCOUNTER22H 0xc96
+#define CSR_HPMCOUNTER23H 0xc97
+#define CSR_HPMCOUNTER24H 0xc98
+#define CSR_HPMCOUNTER25H 0xc99
+#define CSR_HPMCOUNTER26H 0xc9a
+#define CSR_HPMCOUNTER27H 0xc9b
+#define CSR_HPMCOUNTER28H 0xc9c
+#define CSR_HPMCOUNTER29H 0xc9d
+#define CSR_HPMCOUNTER30H 0xc9e
+#define CSR_HPMCOUNTER31H 0xc9f
+#define CSR_MCYCLEH 0xb80
+#define CSR_MINSTRETH 0xb82
+#define CSR_MHPMCOUNTER3H 0xb83
+#define CSR_MHPMCOUNTER4H 0xb84
+#define CSR_MHPMCOUNTER5H 0xb85
+#define CSR_MHPMCOUNTER6H 0xb86
+#define CSR_MHPMCOUNTER7H 0xb87
+#define CSR_MHPMCOUNTER8H 0xb88
+#define CSR_MHPMCOUNTER9H 0xb89
+#define CSR_MHPMCOUNTER10H 0xb8a
+#define CSR_MHPMCOUNTER11H 0xb8b
+#define CSR_MHPMCOUNTER12H 0xb8c
+#define CSR_MHPMCOUNTER13H 0xb8d
+#define CSR_MHPMCOUNTER14H 0xb8e
+#define CSR_MHPMCOUNTER15H 0xb8f
+#define CSR_MHPMCOUNTER16H 0xb90
+#define CSR_MHPMCOUNTER17H 0xb91
+#define CSR_MHPMCOUNTER18H 0xb92
+#define CSR_MHPMCOUNTER19H 0xb93
+#define CSR_MHPMCOUNTER20H 0xb94
+#define CSR_MHPMCOUNTER21H 0xb95
+#define CSR_MHPMCOUNTER22H 0xb96
+#define CSR_MHPMCOUNTER23H 0xb97
+#define CSR_MHPMCOUNTER24H 0xb98
+#define CSR_MHPMCOUNTER25H 0xb99
+#define CSR_MHPMCOUNTER26H 0xb9a
+#define CSR_MHPMCOUNTER27H 0xb9b
+#define CSR_MHPMCOUNTER28H 0xb9c
+#define CSR_MHPMCOUNTER29H 0xb9d
+#define CSR_MHPMCOUNTER30H 0xb9e
+#define CSR_MHPMCOUNTER31H 0xb9f
+#define CAUSE_MISALIGNED_FETCH 0x0
+#define CAUSE_FETCH_ACCESS 0x1
+#define CAUSE_ILLEGAL_INSTRUCTION 0x2
+#define CAUSE_BREAKPOINT 0x3
+#define CAUSE_MISALIGNED_LOAD 0x4
+#define CAUSE_LOAD_ACCESS 0x5
+#define CAUSE_MISALIGNED_STORE 0x6
+#define CAUSE_STORE_ACCESS 0x7
+#define CAUSE_USER_ECALL 0x8
+#define CAUSE_SUPERVISOR_ECALL 0x9
+#define CAUSE_HYPERVISOR_ECALL 0xa
+#define CAUSE_MACHINE_ECALL 0xb
+#define CAUSE_FETCH_PAGE_FAULT 0xc
+#define CAUSE_LOAD_PAGE_FAULT 0xd
+#define CAUSE_STORE_PAGE_FAULT 0xf
+#endif
+#ifdef DECLARE_INSN
+DECLARE_INSN(beq, MATCH_BEQ, MASK_BEQ)
+DECLARE_INSN(bne, MATCH_BNE, MASK_BNE)
+DECLARE_INSN(blt, MATCH_BLT, MASK_BLT)
+DECLARE_INSN(bge, MATCH_BGE, MASK_BGE)
+DECLARE_INSN(bltu, MATCH_BLTU, MASK_BLTU)
+DECLARE_INSN(bgeu, MATCH_BGEU, MASK_BGEU)
+DECLARE_INSN(jalr, MATCH_JALR, MASK_JALR)
+DECLARE_INSN(jal, MATCH_JAL, MASK_JAL)
+DECLARE_INSN(lui, MATCH_LUI, MASK_LUI)
+DECLARE_INSN(auipc, MATCH_AUIPC, MASK_AUIPC)
+DECLARE_INSN(addi, MATCH_ADDI, MASK_ADDI)
+DECLARE_INSN(slli, MATCH_SLLI, MASK_SLLI)
+DECLARE_INSN(slti, MATCH_SLTI, MASK_SLTI)
+DECLARE_INSN(sltiu, MATCH_SLTIU, MASK_SLTIU)
+DECLARE_INSN(xori, MATCH_XORI, MASK_XORI)
+DECLARE_INSN(srli, MATCH_SRLI, MASK_SRLI)
+DECLARE_INSN(srai, MATCH_SRAI, MASK_SRAI)
+DECLARE_INSN(ori, MATCH_ORI, MASK_ORI)
+DECLARE_INSN(andi, MATCH_ANDI, MASK_ANDI)
+DECLARE_INSN(add, MATCH_ADD, MASK_ADD)
+DECLARE_INSN(sub, MATCH_SUB, MASK_SUB)
+DECLARE_INSN(sll, MATCH_SLL, MASK_SLL)
+DECLARE_INSN(slt, MATCH_SLT, MASK_SLT)
+DECLARE_INSN(sltu, MATCH_SLTU, MASK_SLTU)
+DECLARE_INSN(xor, MATCH_XOR, MASK_XOR)
+DECLARE_INSN(srl, MATCH_SRL, MASK_SRL)
+DECLARE_INSN(sra, MATCH_SRA, MASK_SRA)
+DECLARE_INSN(or, MATCH_OR, MASK_OR)
+DECLARE_INSN(and, MATCH_AND, MASK_AND)
+DECLARE_INSN(addiw, MATCH_ADDIW, MASK_ADDIW)
+DECLARE_INSN(slliw, MATCH_SLLIW, MASK_SLLIW)
+DECLARE_INSN(srliw, MATCH_SRLIW, MASK_SRLIW)
+DECLARE_INSN(sraiw, MATCH_SRAIW, MASK_SRAIW)
+DECLARE_INSN(addw, MATCH_ADDW, MASK_ADDW)
+DECLARE_INSN(subw, MATCH_SUBW, MASK_SUBW)
+DECLARE_INSN(sllw, MATCH_SLLW, MASK_SLLW)
+DECLARE_INSN(srlw, MATCH_SRLW, MASK_SRLW)
+DECLARE_INSN(sraw, MATCH_SRAW, MASK_SRAW)
+DECLARE_INSN(lb, MATCH_LB, MASK_LB)
+DECLARE_INSN(lh, MATCH_LH, MASK_LH)
+DECLARE_INSN(lw, MATCH_LW, MASK_LW)
+DECLARE_INSN(ld, MATCH_LD, MASK_LD)
+DECLARE_INSN(lbu, MATCH_LBU, MASK_LBU)
+DECLARE_INSN(lhu, MATCH_LHU, MASK_LHU)
+DECLARE_INSN(lwu, MATCH_LWU, MASK_LWU)
+DECLARE_INSN(sb, MATCH_SB, MASK_SB)
+DECLARE_INSN(sh, MATCH_SH, MASK_SH)
+DECLARE_INSN(sw, MATCH_SW, MASK_SW)
+DECLARE_INSN(sd, MATCH_SD, MASK_SD)
+DECLARE_INSN(fence, MATCH_FENCE, MASK_FENCE)
+DECLARE_INSN(fence_i, MATCH_FENCE_I, MASK_FENCE_I)
+DECLARE_INSN(mul, MATCH_MUL, MASK_MUL)
+DECLARE_INSN(mulh, MATCH_MULH, MASK_MULH)
+DECLARE_INSN(mulhsu, MATCH_MULHSU, MASK_MULHSU)
+DECLARE_INSN(mulhu, MATCH_MULHU, MASK_MULHU)
+DECLARE_INSN(div, MATCH_DIV, MASK_DIV)
+DECLARE_INSN(divu, MATCH_DIVU, MASK_DIVU)
+DECLARE_INSN(rem, MATCH_REM, MASK_REM)
+DECLARE_INSN(remu, MATCH_REMU, MASK_REMU)
+DECLARE_INSN(mulw, MATCH_MULW, MASK_MULW)
+DECLARE_INSN(divw, MATCH_DIVW, MASK_DIVW)
+DECLARE_INSN(divuw, MATCH_DIVUW, MASK_DIVUW)
+DECLARE_INSN(remw, MATCH_REMW, MASK_REMW)
+DECLARE_INSN(remuw, MATCH_REMUW, MASK_REMUW)
+DECLARE_INSN(amoadd_w, MATCH_AMOADD_W, MASK_AMOADD_W)
+DECLARE_INSN(amoxor_w, MATCH_AMOXOR_W, MASK_AMOXOR_W)
+DECLARE_INSN(amoor_w, MATCH_AMOOR_W, MASK_AMOOR_W)
+DECLARE_INSN(amoand_w, MATCH_AMOAND_W, MASK_AMOAND_W)
+DECLARE_INSN(amomin_w, MATCH_AMOMIN_W, MASK_AMOMIN_W)
+DECLARE_INSN(amomax_w, MATCH_AMOMAX_W, MASK_AMOMAX_W)
+DECLARE_INSN(amominu_w, MATCH_AMOMINU_W, MASK_AMOMINU_W)
+DECLARE_INSN(amomaxu_w, MATCH_AMOMAXU_W, MASK_AMOMAXU_W)
+DECLARE_INSN(amoswap_w, MATCH_AMOSWAP_W, MASK_AMOSWAP_W)
+DECLARE_INSN(lr_w, MATCH_LR_W, MASK_LR_W)
+DECLARE_INSN(sc_w, MATCH_SC_W, MASK_SC_W)
+DECLARE_INSN(amoadd_d, MATCH_AMOADD_D, MASK_AMOADD_D)
+DECLARE_INSN(amoxor_d, MATCH_AMOXOR_D, MASK_AMOXOR_D)
+DECLARE_INSN(amoor_d, MATCH_AMOOR_D, MASK_AMOOR_D)
+DECLARE_INSN(amoand_d, MATCH_AMOAND_D, MASK_AMOAND_D)
+DECLARE_INSN(amomin_d, MATCH_AMOMIN_D, MASK_AMOMIN_D)
+DECLARE_INSN(amomax_d, MATCH_AMOMAX_D, MASK_AMOMAX_D)
+DECLARE_INSN(amominu_d, MATCH_AMOMINU_D, MASK_AMOMINU_D)
+DECLARE_INSN(amomaxu_d, MATCH_AMOMAXU_D, MASK_AMOMAXU_D)
+DECLARE_INSN(amoswap_d, MATCH_AMOSWAP_D, MASK_AMOSWAP_D)
+DECLARE_INSN(lr_d, MATCH_LR_D, MASK_LR_D)
+DECLARE_INSN(sc_d, MATCH_SC_D, MASK_SC_D)
+DECLARE_INSN(ecall, MATCH_ECALL, MASK_ECALL)
+DECLARE_INSN(ebreak, MATCH_EBREAK, MASK_EBREAK)
+DECLARE_INSN(uret, MATCH_URET, MASK_URET)
+DECLARE_INSN(sret, MATCH_SRET, MASK_SRET)
+DECLARE_INSN(mret, MATCH_MRET, MASK_MRET)
+DECLARE_INSN(dret, MATCH_DRET, MASK_DRET)
+DECLARE_INSN(sfence_vma, MATCH_SFENCE_VMA, MASK_SFENCE_VMA)
+DECLARE_INSN(wfi, MATCH_WFI, MASK_WFI)
+DECLARE_INSN(csrrw, MATCH_CSRRW, MASK_CSRRW)
+DECLARE_INSN(csrrs, MATCH_CSRRS, MASK_CSRRS)
+DECLARE_INSN(csrrc, MATCH_CSRRC, MASK_CSRRC)
+DECLARE_INSN(csrrwi, MATCH_CSRRWI, MASK_CSRRWI)
+DECLARE_INSN(csrrsi, MATCH_CSRRSI, MASK_CSRRSI)
+DECLARE_INSN(csrrci, MATCH_CSRRCI, MASK_CSRRCI)
+DECLARE_INSN(fadd_s, MATCH_FADD_S, MASK_FADD_S)
+DECLARE_INSN(fsub_s, MATCH_FSUB_S, MASK_FSUB_S)
+DECLARE_INSN(fmul_s, MATCH_FMUL_S, MASK_FMUL_S)
+DECLARE_INSN(fdiv_s, MATCH_FDIV_S, MASK_FDIV_S)
+DECLARE_INSN(fsgnj_s, MATCH_FSGNJ_S, MASK_FSGNJ_S)
+DECLARE_INSN(fsgnjn_s, MATCH_FSGNJN_S, MASK_FSGNJN_S)
+DECLARE_INSN(fsgnjx_s, MATCH_FSGNJX_S, MASK_FSGNJX_S)
+DECLARE_INSN(fmin_s, MATCH_FMIN_S, MASK_FMIN_S)
+DECLARE_INSN(fmax_s, MATCH_FMAX_S, MASK_FMAX_S)
+DECLARE_INSN(fsqrt_s, MATCH_FSQRT_S, MASK_FSQRT_S)
+DECLARE_INSN(fadd_d, MATCH_FADD_D, MASK_FADD_D)
+DECLARE_INSN(fsub_d, MATCH_FSUB_D, MASK_FSUB_D)
+DECLARE_INSN(fmul_d, MATCH_FMUL_D, MASK_FMUL_D)
+DECLARE_INSN(fdiv_d, MATCH_FDIV_D, MASK_FDIV_D)
+DECLARE_INSN(fsgnj_d, MATCH_FSGNJ_D, MASK_FSGNJ_D)
+DECLARE_INSN(fsgnjn_d, MATCH_FSGNJN_D, MASK_FSGNJN_D)
+DECLARE_INSN(fsgnjx_d, MATCH_FSGNJX_D, MASK_FSGNJX_D)
+DECLARE_INSN(fmin_d, MATCH_FMIN_D, MASK_FMIN_D)
+DECLARE_INSN(fmax_d, MATCH_FMAX_D, MASK_FMAX_D)
+DECLARE_INSN(fcvt_s_d, MATCH_FCVT_S_D, MASK_FCVT_S_D)
+DECLARE_INSN(fcvt_d_s, MATCH_FCVT_D_S, MASK_FCVT_D_S)
+DECLARE_INSN(fsqrt_d, MATCH_FSQRT_D, MASK_FSQRT_D)
+DECLARE_INSN(fadd_q, MATCH_FADD_Q, MASK_FADD_Q)
+DECLARE_INSN(fsub_q, MATCH_FSUB_Q, MASK_FSUB_Q)
+DECLARE_INSN(fmul_q, MATCH_FMUL_Q, MASK_FMUL_Q)
+DECLARE_INSN(fdiv_q, MATCH_FDIV_Q, MASK_FDIV_Q)
+DECLARE_INSN(fsgnj_q, MATCH_FSGNJ_Q, MASK_FSGNJ_Q)
+DECLARE_INSN(fsgnjn_q, MATCH_FSGNJN_Q, MASK_FSGNJN_Q)
+DECLARE_INSN(fsgnjx_q, MATCH_FSGNJX_Q, MASK_FSGNJX_Q)
+DECLARE_INSN(fmin_q, MATCH_FMIN_Q, MASK_FMIN_Q)
+DECLARE_INSN(fmax_q, MATCH_FMAX_Q, MASK_FMAX_Q)
+DECLARE_INSN(fcvt_s_q, MATCH_FCVT_S_Q, MASK_FCVT_S_Q)
+DECLARE_INSN(fcvt_q_s, MATCH_FCVT_Q_S, MASK_FCVT_Q_S)
+DECLARE_INSN(fcvt_d_q, MATCH_FCVT_D_Q, MASK_FCVT_D_Q)
+DECLARE_INSN(fcvt_q_d, MATCH_FCVT_Q_D, MASK_FCVT_Q_D)
+DECLARE_INSN(fsqrt_q, MATCH_FSQRT_Q, MASK_FSQRT_Q)
+DECLARE_INSN(fle_s, MATCH_FLE_S, MASK_FLE_S)
+DECLARE_INSN(flt_s, MATCH_FLT_S, MASK_FLT_S)
+DECLARE_INSN(feq_s, MATCH_FEQ_S, MASK_FEQ_S)
+DECLARE_INSN(fle_d, MATCH_FLE_D, MASK_FLE_D)
+DECLARE_INSN(flt_d, MATCH_FLT_D, MASK_FLT_D)
+DECLARE_INSN(feq_d, MATCH_FEQ_D, MASK_FEQ_D)
+DECLARE_INSN(fle_q, MATCH_FLE_Q, MASK_FLE_Q)
+DECLARE_INSN(flt_q, MATCH_FLT_Q, MASK_FLT_Q)
+DECLARE_INSN(feq_q, MATCH_FEQ_Q, MASK_FEQ_Q)
+DECLARE_INSN(fcvt_w_s, MATCH_FCVT_W_S, MASK_FCVT_W_S)
+DECLARE_INSN(fcvt_wu_s, MATCH_FCVT_WU_S, MASK_FCVT_WU_S)
+DECLARE_INSN(fcvt_l_s, MATCH_FCVT_L_S, MASK_FCVT_L_S)
+DECLARE_INSN(fcvt_lu_s, MATCH_FCVT_LU_S, MASK_FCVT_LU_S)
+DECLARE_INSN(fmv_x_w, MATCH_FMV_X_W, MASK_FMV_X_W)
+DECLARE_INSN(fclass_s, MATCH_FCLASS_S, MASK_FCLASS_S)
+DECLARE_INSN(fcvt_w_d, MATCH_FCVT_W_D, MASK_FCVT_W_D)
+DECLARE_INSN(fcvt_wu_d, MATCH_FCVT_WU_D, MASK_FCVT_WU_D)
+DECLARE_INSN(fcvt_l_d, MATCH_FCVT_L_D, MASK_FCVT_L_D)
+DECLARE_INSN(fcvt_lu_d, MATCH_FCVT_LU_D, MASK_FCVT_LU_D)
+DECLARE_INSN(fmv_x_d, MATCH_FMV_X_D, MASK_FMV_X_D)
+DECLARE_INSN(fclass_d, MATCH_FCLASS_D, MASK_FCLASS_D)
+DECLARE_INSN(fcvt_w_q, MATCH_FCVT_W_Q, MASK_FCVT_W_Q)
+DECLARE_INSN(fcvt_wu_q, MATCH_FCVT_WU_Q, MASK_FCVT_WU_Q)
+DECLARE_INSN(fcvt_l_q, MATCH_FCVT_L_Q, MASK_FCVT_L_Q)
+DECLARE_INSN(fcvt_lu_q, MATCH_FCVT_LU_Q, MASK_FCVT_LU_Q)
+DECLARE_INSN(fmv_x_q, MATCH_FMV_X_Q, MASK_FMV_X_Q)
+DECLARE_INSN(fclass_q, MATCH_FCLASS_Q, MASK_FCLASS_Q)
+DECLARE_INSN(fcvt_s_w, MATCH_FCVT_S_W, MASK_FCVT_S_W)
+DECLARE_INSN(fcvt_s_wu, MATCH_FCVT_S_WU, MASK_FCVT_S_WU)
+DECLARE_INSN(fcvt_s_l, MATCH_FCVT_S_L, MASK_FCVT_S_L)
+DECLARE_INSN(fcvt_s_lu, MATCH_FCVT_S_LU, MASK_FCVT_S_LU)
+DECLARE_INSN(fmv_w_x, MATCH_FMV_W_X, MASK_FMV_W_X)
+DECLARE_INSN(fcvt_d_w, MATCH_FCVT_D_W, MASK_FCVT_D_W)
+DECLARE_INSN(fcvt_d_wu, MATCH_FCVT_D_WU, MASK_FCVT_D_WU)
+DECLARE_INSN(fcvt_d_l, MATCH_FCVT_D_L, MASK_FCVT_D_L)
+DECLARE_INSN(fcvt_d_lu, MATCH_FCVT_D_LU, MASK_FCVT_D_LU)
+DECLARE_INSN(fmv_d_x, MATCH_FMV_D_X, MASK_FMV_D_X)
+DECLARE_INSN(fcvt_q_w, MATCH_FCVT_Q_W, MASK_FCVT_Q_W)
+DECLARE_INSN(fcvt_q_wu, MATCH_FCVT_Q_WU, MASK_FCVT_Q_WU)
+DECLARE_INSN(fcvt_q_l, MATCH_FCVT_Q_L, MASK_FCVT_Q_L)
+DECLARE_INSN(fcvt_q_lu, MATCH_FCVT_Q_LU, MASK_FCVT_Q_LU)
+DECLARE_INSN(fmv_q_x, MATCH_FMV_Q_X, MASK_FMV_Q_X)
+DECLARE_INSN(flw, MATCH_FLW, MASK_FLW)
+DECLARE_INSN(fld, MATCH_FLD, MASK_FLD)
+DECLARE_INSN(flq, MATCH_FLQ, MASK_FLQ)
+DECLARE_INSN(fsw, MATCH_FSW, MASK_FSW)
+DECLARE_INSN(fsd, MATCH_FSD, MASK_FSD)
+DECLARE_INSN(fsq, MATCH_FSQ, MASK_FSQ)
+DECLARE_INSN(fmadd_s, MATCH_FMADD_S, MASK_FMADD_S)
+DECLARE_INSN(fmsub_s, MATCH_FMSUB_S, MASK_FMSUB_S)
+DECLARE_INSN(fnmsub_s, MATCH_FNMSUB_S, MASK_FNMSUB_S)
+DECLARE_INSN(fnmadd_s, MATCH_FNMADD_S, MASK_FNMADD_S)
+DECLARE_INSN(fmadd_d, MATCH_FMADD_D, MASK_FMADD_D)
+DECLARE_INSN(fmsub_d, MATCH_FMSUB_D, MASK_FMSUB_D)
+DECLARE_INSN(fnmsub_d, MATCH_FNMSUB_D, MASK_FNMSUB_D)
+DECLARE_INSN(fnmadd_d, MATCH_FNMADD_D, MASK_FNMADD_D)
+DECLARE_INSN(fmadd_q, MATCH_FMADD_Q, MASK_FMADD_Q)
+DECLARE_INSN(fmsub_q, MATCH_FMSUB_Q, MASK_FMSUB_Q)
+DECLARE_INSN(fnmsub_q, MATCH_FNMSUB_Q, MASK_FNMSUB_Q)
+DECLARE_INSN(fnmadd_q, MATCH_FNMADD_Q, MASK_FNMADD_Q)
+DECLARE_INSN(c_nop, MATCH_C_NOP, MASK_C_NOP)
+DECLARE_INSN(c_addi16sp, MATCH_C_ADDI16SP, MASK_C_ADDI16SP)
+DECLARE_INSN(c_jr, MATCH_C_JR, MASK_C_JR)
+DECLARE_INSN(c_jalr, MATCH_C_JALR, MASK_C_JALR)
+DECLARE_INSN(c_ebreak, MATCH_C_EBREAK, MASK_C_EBREAK)
+DECLARE_INSN(c_ld, MATCH_C_LD, MASK_C_LD)
+DECLARE_INSN(c_sd, MATCH_C_SD, MASK_C_SD)
+DECLARE_INSN(c_addiw, MATCH_C_ADDIW, MASK_C_ADDIW)
+DECLARE_INSN(c_ldsp, MATCH_C_LDSP, MASK_C_LDSP)
+DECLARE_INSN(c_sdsp, MATCH_C_SDSP, MASK_C_SDSP)
+DECLARE_INSN(c_addi4spn, MATCH_C_ADDI4SPN, MASK_C_ADDI4SPN)
+DECLARE_INSN(c_fld, MATCH_C_FLD, MASK_C_FLD)
+DECLARE_INSN(c_lw, MATCH_C_LW, MASK_C_LW)
+DECLARE_INSN(c_flw, MATCH_C_FLW, MASK_C_FLW)
+DECLARE_INSN(c_fsd, MATCH_C_FSD, MASK_C_FSD)
+DECLARE_INSN(c_sw, MATCH_C_SW, MASK_C_SW)
+DECLARE_INSN(c_fsw, MATCH_C_FSW, MASK_C_FSW)
+DECLARE_INSN(c_addi, MATCH_C_ADDI, MASK_C_ADDI)
+DECLARE_INSN(c_jal, MATCH_C_JAL, MASK_C_JAL)
+DECLARE_INSN(c_li, MATCH_C_LI, MASK_C_LI)
+DECLARE_INSN(c_lui, MATCH_C_LUI, MASK_C_LUI)
+DECLARE_INSN(c_srli, MATCH_C_SRLI, MASK_C_SRLI)
+DECLARE_INSN(c_srai, MATCH_C_SRAI, MASK_C_SRAI)
+DECLARE_INSN(c_andi, MATCH_C_ANDI, MASK_C_ANDI)
+DECLARE_INSN(c_sub, MATCH_C_SUB, MASK_C_SUB)
+DECLARE_INSN(c_xor, MATCH_C_XOR, MASK_C_XOR)
+DECLARE_INSN(c_or, MATCH_C_OR, MASK_C_OR)
+DECLARE_INSN(c_and, MATCH_C_AND, MASK_C_AND)
+DECLARE_INSN(c_subw, MATCH_C_SUBW, MASK_C_SUBW)
+DECLARE_INSN(c_addw, MATCH_C_ADDW, MASK_C_ADDW)
+DECLARE_INSN(c_j, MATCH_C_J, MASK_C_J)
+DECLARE_INSN(c_beqz, MATCH_C_BEQZ, MASK_C_BEQZ)
+DECLARE_INSN(c_bnez, MATCH_C_BNEZ, MASK_C_BNEZ)
+DECLARE_INSN(c_slli, MATCH_C_SLLI, MASK_C_SLLI)
+DECLARE_INSN(c_fldsp, MATCH_C_FLDSP, MASK_C_FLDSP)
+DECLARE_INSN(c_lwsp, MATCH_C_LWSP, MASK_C_LWSP)
+DECLARE_INSN(c_flwsp, MATCH_C_FLWSP, MASK_C_FLWSP)
+DECLARE_INSN(c_mv, MATCH_C_MV, MASK_C_MV)
+DECLARE_INSN(c_add, MATCH_C_ADD, MASK_C_ADD)
+DECLARE_INSN(c_fsdsp, MATCH_C_FSDSP, MASK_C_FSDSP)
+DECLARE_INSN(c_swsp, MATCH_C_SWSP, MASK_C_SWSP)
+DECLARE_INSN(c_fswsp, MATCH_C_FSWSP, MASK_C_FSWSP)
+DECLARE_INSN(custom0, MATCH_CUSTOM0, MASK_CUSTOM0)
+DECLARE_INSN(custom0_rs1, MATCH_CUSTOM0_RS1, MASK_CUSTOM0_RS1)
+DECLARE_INSN(custom0_rs1_rs2, MATCH_CUSTOM0_RS1_RS2, MASK_CUSTOM0_RS1_RS2)
+DECLARE_INSN(custom0_rd, MATCH_CUSTOM0_RD, MASK_CUSTOM0_RD)
+DECLARE_INSN(custom0_rd_rs1, MATCH_CUSTOM0_RD_RS1, MASK_CUSTOM0_RD_RS1)
+DECLARE_INSN(custom0_rd_rs1_rs2, MATCH_CUSTOM0_RD_RS1_RS2, MASK_CUSTOM0_RD_RS1_RS2)
+DECLARE_INSN(custom1, MATCH_CUSTOM1, MASK_CUSTOM1)
+DECLARE_INSN(custom1_rs1, MATCH_CUSTOM1_RS1, MASK_CUSTOM1_RS1)
+DECLARE_INSN(custom1_rs1_rs2, MATCH_CUSTOM1_RS1_RS2, MASK_CUSTOM1_RS1_RS2)
+DECLARE_INSN(custom1_rd, MATCH_CUSTOM1_RD, MASK_CUSTOM1_RD)
+DECLARE_INSN(custom1_rd_rs1, MATCH_CUSTOM1_RD_RS1, MASK_CUSTOM1_RD_RS1)
+DECLARE_INSN(custom1_rd_rs1_rs2, MATCH_CUSTOM1_RD_RS1_RS2, MASK_CUSTOM1_RD_RS1_RS2)
+DECLARE_INSN(custom2, MATCH_CUSTOM2, MASK_CUSTOM2)
+DECLARE_INSN(custom2_rs1, MATCH_CUSTOM2_RS1, MASK_CUSTOM2_RS1)
+DECLARE_INSN(custom2_rs1_rs2, MATCH_CUSTOM2_RS1_RS2, MASK_CUSTOM2_RS1_RS2)
+DECLARE_INSN(custom2_rd, MATCH_CUSTOM2_RD, MASK_CUSTOM2_RD)
+DECLARE_INSN(custom2_rd_rs1, MATCH_CUSTOM2_RD_RS1, MASK_CUSTOM2_RD_RS1)
+DECLARE_INSN(custom2_rd_rs1_rs2, MATCH_CUSTOM2_RD_RS1_RS2, MASK_CUSTOM2_RD_RS1_RS2)
+DECLARE_INSN(custom3, MATCH_CUSTOM3, MASK_CUSTOM3)
+DECLARE_INSN(custom3_rs1, MATCH_CUSTOM3_RS1, MASK_CUSTOM3_RS1)
+DECLARE_INSN(custom3_rs1_rs2, MATCH_CUSTOM3_RS1_RS2, MASK_CUSTOM3_RS1_RS2)
+DECLARE_INSN(custom3_rd, MATCH_CUSTOM3_RD, MASK_CUSTOM3_RD)
+DECLARE_INSN(custom3_rd_rs1, MATCH_CUSTOM3_RD_RS1, MASK_CUSTOM3_RD_RS1)
+DECLARE_INSN(custom3_rd_rs1_rs2, MATCH_CUSTOM3_RD_RS1_RS2, MASK_CUSTOM3_RD_RS1_RS2)
+#endif
+#ifdef DECLARE_CSR
+DECLARE_CSR(fflags, CSR_FFLAGS)
+DECLARE_CSR(frm, CSR_FRM)
+DECLARE_CSR(fcsr, CSR_FCSR)
+DECLARE_CSR(cycle, CSR_CYCLE)
+DECLARE_CSR(time, CSR_TIME)
+DECLARE_CSR(instret, CSR_INSTRET)
+DECLARE_CSR(hpmcounter3, CSR_HPMCOUNTER3)
+DECLARE_CSR(hpmcounter4, CSR_HPMCOUNTER4)
+DECLARE_CSR(hpmcounter5, CSR_HPMCOUNTER5)
+DECLARE_CSR(hpmcounter6, CSR_HPMCOUNTER6)
+DECLARE_CSR(hpmcounter7, CSR_HPMCOUNTER7)
+DECLARE_CSR(hpmcounter8, CSR_HPMCOUNTER8)
+DECLARE_CSR(hpmcounter9, CSR_HPMCOUNTER9)
+DECLARE_CSR(hpmcounter10, CSR_HPMCOUNTER10)
+DECLARE_CSR(hpmcounter11, CSR_HPMCOUNTER11)
+DECLARE_CSR(hpmcounter12, CSR_HPMCOUNTER12)
+DECLARE_CSR(hpmcounter13, CSR_HPMCOUNTER13)
+DECLARE_CSR(hpmcounter14, CSR_HPMCOUNTER14)
+DECLARE_CSR(hpmcounter15, CSR_HPMCOUNTER15)
+DECLARE_CSR(hpmcounter16, CSR_HPMCOUNTER16)
+DECLARE_CSR(hpmcounter17, CSR_HPMCOUNTER17)
+DECLARE_CSR(hpmcounter18, CSR_HPMCOUNTER18)
+DECLARE_CSR(hpmcounter19, CSR_HPMCOUNTER19)
+DECLARE_CSR(hpmcounter20, CSR_HPMCOUNTER20)
+DECLARE_CSR(hpmcounter21, CSR_HPMCOUNTER21)
+DECLARE_CSR(hpmcounter22, CSR_HPMCOUNTER22)
+DECLARE_CSR(hpmcounter23, CSR_HPMCOUNTER23)
+DECLARE_CSR(hpmcounter24, CSR_HPMCOUNTER24)
+DECLARE_CSR(hpmcounter25, CSR_HPMCOUNTER25)
+DECLARE_CSR(hpmcounter26, CSR_HPMCOUNTER26)
+DECLARE_CSR(hpmcounter27, CSR_HPMCOUNTER27)
+DECLARE_CSR(hpmcounter28, CSR_HPMCOUNTER28)
+DECLARE_CSR(hpmcounter29, CSR_HPMCOUNTER29)
+DECLARE_CSR(hpmcounter30, CSR_HPMCOUNTER30)
+DECLARE_CSR(hpmcounter31, CSR_HPMCOUNTER31)
+DECLARE_CSR(sstatus, CSR_SSTATUS)
+DECLARE_CSR(sie, CSR_SIE)
+DECLARE_CSR(stvec, CSR_STVEC)
+DECLARE_CSR(scounteren, CSR_SCOUNTEREN)
+DECLARE_CSR(sscratch, CSR_SSCRATCH)
+DECLARE_CSR(sepc, CSR_SEPC)
+DECLARE_CSR(scause, CSR_SCAUSE)
+DECLARE_CSR(stval, CSR_STVAL)
+DECLARE_CSR(sip, CSR_SIP)
+DECLARE_CSR(satp, CSR_SATP)
+DECLARE_CSR(mstatus, CSR_MSTATUS)
+DECLARE_CSR(misa, CSR_MISA)
+DECLARE_CSR(medeleg, CSR_MEDELEG)
+DECLARE_CSR(mideleg, CSR_MIDELEG)
+DECLARE_CSR(mie, CSR_MIE)
+DECLARE_CSR(mtvec, CSR_MTVEC)
+DECLARE_CSR(mcounteren, CSR_MCOUNTEREN)
+DECLARE_CSR(mscratch, CSR_MSCRATCH)
+DECLARE_CSR(mepc, CSR_MEPC)
+DECLARE_CSR(mcause, CSR_MCAUSE)
+DECLARE_CSR(mtval, CSR_MTVAL)
+DECLARE_CSR(mip, CSR_MIP)
+DECLARE_CSR(pmpcfg0, CSR_PMPCFG0)
+DECLARE_CSR(pmpcfg1, CSR_PMPCFG1)
+DECLARE_CSR(pmpcfg2, CSR_PMPCFG2)
+DECLARE_CSR(pmpcfg3, CSR_PMPCFG3)
+DECLARE_CSR(pmpaddr0, CSR_PMPADDR0)
+DECLARE_CSR(pmpaddr1, CSR_PMPADDR1)
+DECLARE_CSR(pmpaddr2, CSR_PMPADDR2)
+DECLARE_CSR(pmpaddr3, CSR_PMPADDR3)
+DECLARE_CSR(pmpaddr4, CSR_PMPADDR4)
+DECLARE_CSR(pmpaddr5, CSR_PMPADDR5)
+DECLARE_CSR(pmpaddr6, CSR_PMPADDR6)
+DECLARE_CSR(pmpaddr7, CSR_PMPADDR7)
+DECLARE_CSR(pmpaddr8, CSR_PMPADDR8)
+DECLARE_CSR(pmpaddr9, CSR_PMPADDR9)
+DECLARE_CSR(pmpaddr10, CSR_PMPADDR10)
+DECLARE_CSR(pmpaddr11, CSR_PMPADDR11)
+DECLARE_CSR(pmpaddr12, CSR_PMPADDR12)
+DECLARE_CSR(pmpaddr13, CSR_PMPADDR13)
+DECLARE_CSR(pmpaddr14, CSR_PMPADDR14)
+DECLARE_CSR(pmpaddr15, CSR_PMPADDR15)
+DECLARE_CSR(tselect, CSR_TSELECT)
+DECLARE_CSR(tdata1, CSR_TDATA1)
+DECLARE_CSR(tdata2, CSR_TDATA2)
+DECLARE_CSR(tdata3, CSR_TDATA3)
+DECLARE_CSR(dcsr, CSR_DCSR)
+DECLARE_CSR(dpc, CSR_DPC)
+DECLARE_CSR(dscratch, CSR_DSCRATCH)
+DECLARE_CSR(mcycle, CSR_MCYCLE)
+DECLARE_CSR(minstret, CSR_MINSTRET)
+DECLARE_CSR(mhpmcounter3, CSR_MHPMCOUNTER3)
+DECLARE_CSR(mhpmcounter4, CSR_MHPMCOUNTER4)
+DECLARE_CSR(mhpmcounter5, CSR_MHPMCOUNTER5)
+DECLARE_CSR(mhpmcounter6, CSR_MHPMCOUNTER6)
+DECLARE_CSR(mhpmcounter7, CSR_MHPMCOUNTER7)
+DECLARE_CSR(mhpmcounter8, CSR_MHPMCOUNTER8)
+DECLARE_CSR(mhpmcounter9, CSR_MHPMCOUNTER9)
+DECLARE_CSR(mhpmcounter10, CSR_MHPMCOUNTER10)
+DECLARE_CSR(mhpmcounter11, CSR_MHPMCOUNTER11)
+DECLARE_CSR(mhpmcounter12, CSR_MHPMCOUNTER12)
+DECLARE_CSR(mhpmcounter13, CSR_MHPMCOUNTER13)
+DECLARE_CSR(mhpmcounter14, CSR_MHPMCOUNTER14)
+DECLARE_CSR(mhpmcounter15, CSR_MHPMCOUNTER15)
+DECLARE_CSR(mhpmcounter16, CSR_MHPMCOUNTER16)
+DECLARE_CSR(mhpmcounter17, CSR_MHPMCOUNTER17)
+DECLARE_CSR(mhpmcounter18, CSR_MHPMCOUNTER18)
+DECLARE_CSR(mhpmcounter19, CSR_MHPMCOUNTER19)
+DECLARE_CSR(mhpmcounter20, CSR_MHPMCOUNTER20)
+DECLARE_CSR(mhpmcounter21, CSR_MHPMCOUNTER21)
+DECLARE_CSR(mhpmcounter22, CSR_MHPMCOUNTER22)
+DECLARE_CSR(mhpmcounter23, CSR_MHPMCOUNTER23)
+DECLARE_CSR(mhpmcounter24, CSR_MHPMCOUNTER24)
+DECLARE_CSR(mhpmcounter25, CSR_MHPMCOUNTER25)
+DECLARE_CSR(mhpmcounter26, CSR_MHPMCOUNTER26)
+DECLARE_CSR(mhpmcounter27, CSR_MHPMCOUNTER27)
+DECLARE_CSR(mhpmcounter28, CSR_MHPMCOUNTER28)
+DECLARE_CSR(mhpmcounter29, CSR_MHPMCOUNTER29)
+DECLARE_CSR(mhpmcounter30, CSR_MHPMCOUNTER30)
+DECLARE_CSR(mhpmcounter31, CSR_MHPMCOUNTER31)
+DECLARE_CSR(mhpmevent3, CSR_MHPMEVENT3)
+DECLARE_CSR(mhpmevent4, CSR_MHPMEVENT4)
+DECLARE_CSR(mhpmevent5, CSR_MHPMEVENT5)
+DECLARE_CSR(mhpmevent6, CSR_MHPMEVENT6)
+DECLARE_CSR(mhpmevent7, CSR_MHPMEVENT7)
+DECLARE_CSR(mhpmevent8, CSR_MHPMEVENT8)
+DECLARE_CSR(mhpmevent9, CSR_MHPMEVENT9)
+DECLARE_CSR(mhpmevent10, CSR_MHPMEVENT10)
+DECLARE_CSR(mhpmevent11, CSR_MHPMEVENT11)
+DECLARE_CSR(mhpmevent12, CSR_MHPMEVENT12)
+DECLARE_CSR(mhpmevent13, CSR_MHPMEVENT13)
+DECLARE_CSR(mhpmevent14, CSR_MHPMEVENT14)
+DECLARE_CSR(mhpmevent15, CSR_MHPMEVENT15)
+DECLARE_CSR(mhpmevent16, CSR_MHPMEVENT16)
+DECLARE_CSR(mhpmevent17, CSR_MHPMEVENT17)
+DECLARE_CSR(mhpmevent18, CSR_MHPMEVENT18)
+DECLARE_CSR(mhpmevent19, CSR_MHPMEVENT19)
+DECLARE_CSR(mhpmevent20, CSR_MHPMEVENT20)
+DECLARE_CSR(mhpmevent21, CSR_MHPMEVENT21)
+DECLARE_CSR(mhpmevent22, CSR_MHPMEVENT22)
+DECLARE_CSR(mhpmevent23, CSR_MHPMEVENT23)
+DECLARE_CSR(mhpmevent24, CSR_MHPMEVENT24)
+DECLARE_CSR(mhpmevent25, CSR_MHPMEVENT25)
+DECLARE_CSR(mhpmevent26, CSR_MHPMEVENT26)
+DECLARE_CSR(mhpmevent27, CSR_MHPMEVENT27)
+DECLARE_CSR(mhpmevent28, CSR_MHPMEVENT28)
+DECLARE_CSR(mhpmevent29, CSR_MHPMEVENT29)
+DECLARE_CSR(mhpmevent30, CSR_MHPMEVENT30)
+DECLARE_CSR(mhpmevent31, CSR_MHPMEVENT31)
+DECLARE_CSR(mvendorid, CSR_MVENDORID)
+DECLARE_CSR(marchid, CSR_MARCHID)
+DECLARE_CSR(mimpid, CSR_MIMPID)
+DECLARE_CSR(mhartid, CSR_MHARTID)
+DECLARE_CSR(cycleh, CSR_CYCLEH)
+DECLARE_CSR(timeh, CSR_TIMEH)
+DECLARE_CSR(instreth, CSR_INSTRETH)
+DECLARE_CSR(hpmcounter3h, CSR_HPMCOUNTER3H)
+DECLARE_CSR(hpmcounter4h, CSR_HPMCOUNTER4H)
+DECLARE_CSR(hpmcounter5h, CSR_HPMCOUNTER5H)
+DECLARE_CSR(hpmcounter6h, CSR_HPMCOUNTER6H)
+DECLARE_CSR(hpmcounter7h, CSR_HPMCOUNTER7H)
+DECLARE_CSR(hpmcounter8h, CSR_HPMCOUNTER8H)
+DECLARE_CSR(hpmcounter9h, CSR_HPMCOUNTER9H)
+DECLARE_CSR(hpmcounter10h, CSR_HPMCOUNTER10H)
+DECLARE_CSR(hpmcounter11h, CSR_HPMCOUNTER11H)
+DECLARE_CSR(hpmcounter12h, CSR_HPMCOUNTER12H)
+DECLARE_CSR(hpmcounter13h, CSR_HPMCOUNTER13H)
+DECLARE_CSR(hpmcounter14h, CSR_HPMCOUNTER14H)
+DECLARE_CSR(hpmcounter15h, CSR_HPMCOUNTER15H)
+DECLARE_CSR(hpmcounter16h, CSR_HPMCOUNTER16H)
+DECLARE_CSR(hpmcounter17h, CSR_HPMCOUNTER17H)
+DECLARE_CSR(hpmcounter18h, CSR_HPMCOUNTER18H)
+DECLARE_CSR(hpmcounter19h, CSR_HPMCOUNTER19H)
+DECLARE_CSR(hpmcounter20h, CSR_HPMCOUNTER20H)
+DECLARE_CSR(hpmcounter21h, CSR_HPMCOUNTER21H)
+DECLARE_CSR(hpmcounter22h, CSR_HPMCOUNTER22H)
+DECLARE_CSR(hpmcounter23h, CSR_HPMCOUNTER23H)
+DECLARE_CSR(hpmcounter24h, CSR_HPMCOUNTER24H)
+DECLARE_CSR(hpmcounter25h, CSR_HPMCOUNTER25H)
+DECLARE_CSR(hpmcounter26h, CSR_HPMCOUNTER26H)
+DECLARE_CSR(hpmcounter27h, CSR_HPMCOUNTER27H)
+DECLARE_CSR(hpmcounter28h, CSR_HPMCOUNTER28H)
+DECLARE_CSR(hpmcounter29h, CSR_HPMCOUNTER29H)
+DECLARE_CSR(hpmcounter30h, CSR_HPMCOUNTER30H)
+DECLARE_CSR(hpmcounter31h, CSR_HPMCOUNTER31H)
+DECLARE_CSR(mcycleh, CSR_MCYCLEH)
+DECLARE_CSR(minstreth, CSR_MINSTRETH)
+DECLARE_CSR(mhpmcounter3h, CSR_MHPMCOUNTER3H)
+DECLARE_CSR(mhpmcounter4h, CSR_MHPMCOUNTER4H)
+DECLARE_CSR(mhpmcounter5h, CSR_MHPMCOUNTER5H)
+DECLARE_CSR(mhpmcounter6h, CSR_MHPMCOUNTER6H)
+DECLARE_CSR(mhpmcounter7h, CSR_MHPMCOUNTER7H)
+DECLARE_CSR(mhpmcounter8h, CSR_MHPMCOUNTER8H)
+DECLARE_CSR(mhpmcounter9h, CSR_MHPMCOUNTER9H)
+DECLARE_CSR(mhpmcounter10h, CSR_MHPMCOUNTER10H)
+DECLARE_CSR(mhpmcounter11h, CSR_MHPMCOUNTER11H)
+DECLARE_CSR(mhpmcounter12h, CSR_MHPMCOUNTER12H)
+DECLARE_CSR(mhpmcounter13h, CSR_MHPMCOUNTER13H)
+DECLARE_CSR(mhpmcounter14h, CSR_MHPMCOUNTER14H)
+DECLARE_CSR(mhpmcounter15h, CSR_MHPMCOUNTER15H)
+DECLARE_CSR(mhpmcounter16h, CSR_MHPMCOUNTER16H)
+DECLARE_CSR(mhpmcounter17h, CSR_MHPMCOUNTER17H)
+DECLARE_CSR(mhpmcounter18h, CSR_MHPMCOUNTER18H)
+DECLARE_CSR(mhpmcounter19h, CSR_MHPMCOUNTER19H)
+DECLARE_CSR(mhpmcounter20h, CSR_MHPMCOUNTER20H)
+DECLARE_CSR(mhpmcounter21h, CSR_MHPMCOUNTER21H)
+DECLARE_CSR(mhpmcounter22h, CSR_MHPMCOUNTER22H)
+DECLARE_CSR(mhpmcounter23h, CSR_MHPMCOUNTER23H)
+DECLARE_CSR(mhpmcounter24h, CSR_MHPMCOUNTER24H)
+DECLARE_CSR(mhpmcounter25h, CSR_MHPMCOUNTER25H)
+DECLARE_CSR(mhpmcounter26h, CSR_MHPMCOUNTER26H)
+DECLARE_CSR(mhpmcounter27h, CSR_MHPMCOUNTER27H)
+DECLARE_CSR(mhpmcounter28h, CSR_MHPMCOUNTER28H)
+DECLARE_CSR(mhpmcounter29h, CSR_MHPMCOUNTER29H)
+DECLARE_CSR(mhpmcounter30h, CSR_MHPMCOUNTER30H)
+DECLARE_CSR(mhpmcounter31h, CSR_MHPMCOUNTER31H)
+#endif
+#ifdef DECLARE_CAUSE
+DECLARE_CAUSE("misaligned fetch", CAUSE_MISALIGNED_FETCH)
+DECLARE_CAUSE("fetch access", CAUSE_FETCH_ACCESS)
+DECLARE_CAUSE("illegal instruction", CAUSE_ILLEGAL_INSTRUCTION)
+DECLARE_CAUSE("breakpoint", CAUSE_BREAKPOINT)
+DECLARE_CAUSE("misaligned load", CAUSE_MISALIGNED_LOAD)
+DECLARE_CAUSE("load access", CAUSE_LOAD_ACCESS)
+DECLARE_CAUSE("misaligned store", CAUSE_MISALIGNED_STORE)
+DECLARE_CAUSE("store access", CAUSE_STORE_ACCESS)
+DECLARE_CAUSE("user_ecall", CAUSE_USER_ECALL)
+DECLARE_CAUSE("supervisor_ecall", CAUSE_SUPERVISOR_ECALL)
+DECLARE_CAUSE("hypervisor_ecall", CAUSE_HYPERVISOR_ECALL)
+DECLARE_CAUSE("machine_ecall", CAUSE_MACHINE_ECALL)
+DECLARE_CAUSE("fetch page fault", CAUSE_FETCH_PAGE_FAULT)
+DECLARE_CAUSE("load page fault", CAUSE_LOAD_PAGE_FAULT)
+DECLARE_CAUSE("store page fault", CAUSE_STORE_PAGE_FAULT)
+#endif
diff --git a/include/sm/gm/big.h b/include/sm/gm/big.h
new file mode 100644
index 0000000..883ee66
--- /dev/null
+++ b/include/sm/gm/big.h
@@ -0,0 +1,88 @@
+#ifndef _BIG_H_
+#define _BIG_H_
+
+#include "sm/gm/typedef.h"
+
+#define VLI_DIGIT_BITS               64
+#define VLI_DIGIT_BYTES              (VLI_DIGIT_BITS/8)
+
+void vli_clear(u64 *vli, u8 ndigits);
+
+/* Returns true if vli == 0, false otherwise. */
+int vli_is_zero(u64 *vli, u8 ndigits);
+
+/* Returns nonzero if bit bit of vli is set. */
+u64 vli_test_bit(u64 *vli, u8 bit, u8 ndigits);
+
+/* Counts the number of 8-bit "digits" in vli. */
+u32 vli_num_digits(u64 *vli, u8 ndigits);
+
+/* Counts the number of bits required for vli. */
+u32 vli_num_bits(u64 *vli, u8 ndigits);
+/* Sets dest = src. */
+
+void vli_set(u64 *dest, u64 *src, u8 ndigits);
+
+/* Returns sign of left - right. */
+int vli_cmp(u64 *left, u64 *right, u8 ndigits);
+
+/* Computes result = in << c, returning carry. Can modify in place
+ * (if result == in). 0 < shift < 8.
+ */
+u64 vli_lshift(u64 *result, u64 *in, u32 shift, u8 ndigits);
+
+/* Computes result = (left * right) % curve->p. */
+void vli_mod_mult_fast(u64 *result, u64 *left, u64 *right, u64 *mod, u8 ndigits);
+
+/* Computes result = left^2 % curve->p. */
+void vli_mod_square_fast(u64 *result, u64 *left, u64 *mod, u8 ndigits);
+
+/* Computes result = in >> c, returning carry. Can modify in place
+ * (if result == in). 0 < shift < 64.
+ */
+u64 vli_rshift(u64 *result, u64 *in, u32 shift, u8 ndigits);
+
+/* Computes result = left + right, returning carry. Can modify in place. */
+u64 vli_add(u64 *result, u64 *left, u64 *right, u8 ndigits);
+
+/* Computes result = left - right, returning borrow. Can modify in place. */
+u64 vli_sub(u64 *result, u64 *left, u64 *right, u8 ndigits);
+
+/* Computes result = left * right. */
+void vli_mult(u64 *result, u64 *left, u64 *right, u8 ndigits);
+
+/* Computes result = left^2. */
+void vli_square(u64 *result, u64 *left, u8 ndigits);
+
+/* Computes result = (left + right) % mod.
+   Assumes that left < mod and right < mod, result != mod. */
+void vli_mod_add(u64 *result, u64 *left, u64 *right, u64 *mod, u8 ndigits);
+
+/* Computes result = (left - right) % mod.
+   Assumes that left < mod and right < mod, result != mod. */
+void vli_mod_sub(u64 *result, u64 *left, u64 *right, u64 *mod, u8 ndigits);
+
+/* Computes result = (left * right) % mod. */
+void vli_mod_mult(u64 *result, u64 *left, u64 *right, u64 *mod, u8 ndigits);
+
+/* Computes result = left^2 % mod. */
+void vli_mod_square(u64 *result, u64 *left, u64 *mod, u8 ndigits);
+
+/* Computes result = left^p % mod. */
+void vli_mod_exp(u64 *result, u64 *left, u64 *p, u64 *mod, u8 ndigits);
+
+/* Computes result = (product) % mod. */
+void vli_mod(u64 *result, u64 *product, u64 *mod, u8 ndigits);
+
+/* Computes result = (1 / input) % mod. All VLIs are the same size.
+ * See "From Euclid's GCD to Montgomery Multiplication to the Great Divide"
+ * https://labs.oracle.com/techrep/2001/smli_tr-2001-95.pdf
+ */
+void vli_mod_inv(u64 *result, u64 *input, u64 *mod, u8 ndigits);
+
+/* Computes result = (left / right).
+ * remainder = (left % right).
+ */
+void vli_div(u64 *result, u64 *remainder, u64 *left, u64 cdigits, u64 *right, u8 ddigits);
+
+#endif
diff --git a/include/sm/gm/ecc.h b/include/sm/gm/ecc.h
new file mode 100644
index 0000000..0d652de
--- /dev/null
+++ b/include/sm/gm/ecc.h
@@ -0,0 +1,38 @@
+#ifndef _ECC_H_
+#define _ECC_H_
+
+#include "sm/gm/typedef.h"
+
+#define ECC_WORDSIZE 8
+#define ECC_NUMBITS 256
+#define ECC_NUMWORD (ECC_NUMBITS/ECC_WORDSIZE) //32
+
+#define ECC_MAX_DIGITS  4
+
+#define SWAP(a,b) { u32 t = a; a = b; b = t;}
+
+typedef struct ecc_point
+{
+  u64 x[ECC_MAX_DIGITS];
+  u64 y[ECC_MAX_DIGITS];
+} ecc_point;
+
+struct ecc_curve {
+  u8 ndigits;
+  struct ecc_point g;
+  u64 p[ECC_MAX_DIGITS];
+  u64 n[ECC_MAX_DIGITS];
+  u64 h[ECC_MAX_DIGITS];
+  u64 a[ECC_MAX_DIGITS];
+  u64 b[ECC_MAX_DIGITS];
+};
+
+void ecc_bytes2native(u64 *native, void *bytes, u8 ndigits);
+void ecc_native2bytes(void *bytes, u64 *native, u8 ndigits);
+
+void ecc_point_add(struct ecc_curve *curve, ecc_point *result, ecc_point *x, ecc_point *y);
+void ecc_point_mult(struct ecc_curve *curve, ecc_point *result, ecc_point *point, u64 *scalar, u64 *initialZ);
+void ecc_point_mult2(struct ecc_curve *curve, ecc_point *result, ecc_point *g, ecc_point *p, u64 *s, u64 *t);
+int ecc_point_is_zero(struct ecc_curve *curve, ecc_point *point);
+
+#endif
diff --git a/include/sm/gm/random.h b/include/sm/gm/random.h
new file mode 100644
index 0000000..66bc9f4
--- /dev/null
+++ b/include/sm/gm/random.h
@@ -0,0 +1,8 @@
+#ifndef _RANDOM_H_
+#define _RANDOM_H_
+
+#include "sm/gm/typedef.h"
+
+int vli_get_random(u8 *p_data, u32 len);
+
+#endif
diff --git a/include/sm/gm/sm2.h b/include/sm/gm/sm2.h
new file mode 100644
index 0000000..6f10b40
--- /dev/null
+++ b/include/sm/gm/sm2.h
@@ -0,0 +1,34 @@
+#ifndef _SM2_H_
+#define _SM2_H_
+
+#include "sm/gm/typedef.h"
+#include "sm/gm/ecc.h"
+
+int sm2_make_prikey(u8 *prikey);
+int sm2_make_pubkey(u8 *prikey, ecc_point *pubkey);
+int sm2_make_keypair(u8 *prikey, ecc_point *pubkey);
+int sm2_sign(u8 *r, u8 *s, u8 *pri, u8 *hash);
+int sm2_verify(ecc_point *pubkey, u8 *hash, u8 *r, u8 *s);
+
+int sm2_encrypt(ecc_point *pubKey, u8 *M, u32 Mlen, u8 *C, u32 *Clen);
+int sm2_decrypt(u8 *prikey, u8 *C, u32 Clen, u8 *M, u32 *Mlen);
+
+void sm3_z(u8 *id, u32 idlen, ecc_point *pub, u8 *hash);
+int sm2_shared_point(u8* selfPriKey,  u8* selfTempPriKey, ecc_point* selfTempPubKey,
+		 ecc_point *otherPubKey, ecc_point* otherTempPubKey, ecc_point *key);
+int sm2_shared_key(ecc_point *point, u8 *ZA, u8 *ZB, u32 keyLen, u8 *key);
+int sm2_point_mult(ecc_point *G, u8 *k, ecc_point *P);
+
+
+int ECC_KeyEx_Init_I(u8 *pri, ecc_point *pub);
+
+int ECC_KeyEx_Re_I(u8 *rb, u8 *dB, ecc_point *RA, ecc_point *PA,
+		u8* ZA, u8 *ZB, u8 *K, u32 klen, ecc_point *RB,
+		ecc_point *V, u8* hash);
+
+int ECC_KeyEx_Init_II(u8* ra, u8* dA, ecc_point* RA, ecc_point* RB, ecc_point* PB, u8
+		ZA[],u8 ZB[],u8 SB[],u8 K[], u32 klen,u8 SA[]);
+
+int ECC_KeyEx_Re_II(ecc_point *V,ecc_point *RA,ecc_point *RB,u8 ZA[],u8 ZB[],u8 SA[]);
+
+#endif /* _SM2_H_ */
diff --git a/include/sm/gm/sm3.h b/include/sm/gm/sm3.h
new file mode 100644
index 0000000..7d1e9da
--- /dev/null
+++ b/include/sm/gm/sm3.h
@@ -0,0 +1,103 @@
+#ifndef _SM3_H
+#define _SM3_H
+
+#include "sm/gm/typedef.h"
+
+#define SM3_DATA_LEN    32
+
+/**
+ * \brief          SM3 context structure
+ */
+struct sm3_context
+{
+  unsigned long total[2];     /*!< number of bytes processed  */
+  unsigned long state[8];     /*!< intermediate digest state  */
+  unsigned char buffer[64];   /*!< data block being processed */
+
+  unsigned char ipad[64];     /*!< HMAC: inner padding        */
+  unsigned char opad[64];     /*!< HMAC: outer padding        */
+};
+
+/**
+ * \brief          SM3 context setup
+ *
+ * \param ctx      context to be initialized
+ */
+void sm3_init(struct sm3_context *ctx);
+
+/**
+ * \brief          SM3 process buffer
+ *
+ * \param ctx      SM3 context
+ * \param input    buffer holding the  data
+ * \param ilen     length of the input data
+ */
+void sm3_update(struct sm3_context *ctx, unsigned char *input, int ilen);
+
+/**
+ * \brief          SM3 final digest
+ *
+ * \param ctx      SM3 context
+ */
+void sm3_final(struct sm3_context *ctx, unsigned char output[32]);
+
+/**
+ * \brief          Output = SM3( input buffer )
+ *
+ * \param input    buffer holding the  data
+ * \param ilen     length of the input data
+ * \param output   SM3 checksum result
+ */
+void sm3(unsigned char *input, int ilen, unsigned char output[32]);
+
+/**
+ * \brief          Output = SM3( file contents )
+ *
+ * \param path     input file name
+ * \param output   SM3 checksum result
+ *
+ * \return         0 if successful, 1 if fopen failed,
+ *                 or 2 if fread failed
+ */
+int sm3_file(char *path, unsigned char output[32]);
+
+/**
+ * \brief          SM3 HMAC context setup
+ *
+ * \param ctx      HMAC context to be initialized
+ * \param key      HMAC secret key
+ * \param keylen   length of the HMAC key
+ */
+void sm3_hmac_init(struct sm3_context *ctx, unsigned char *key, int keylen);
+
+/**
+ * \brief          SM3 HMAC process buffer
+ *
+ * \param ctx      HMAC context
+ * \param input    buffer holding the  data
+ * \param ilen     length of the input data
+ */
+void sm3_hmac_update(struct sm3_context *ctx, unsigned char *input, int ilen);
+
+/**
+ * \brief          SM3 HMAC final digest
+ *
+ * \param ctx      HMAC context
+ * \param output   SM3 HMAC checksum result
+ */
+void sm3_hmac_final(struct sm3_context *ctx, unsigned char output[32]);
+
+/**
+ * \brief          Output = HMAC-SM3( hmac key, input buffer )
+ *
+ * \param key      HMAC secret key
+ * \param keylen   length of the HMAC key
+ * \param input    buffer holding the  data
+ * \param ilen     length of the input data
+ * \param output   HMAC-SM3 result
+ */
+void sm3_hmac(unsigned char *key, int keylen,
+    unsigned char *input, int ilen,
+    unsigned char output[32]);
+
+#endif /* _SM3_H */
diff --git a/include/sm/gm/typedef.h b/include/sm/gm/typedef.h
new file mode 100644
index 0000000..4e04895
--- /dev/null
+++ b/include/sm/gm/typedef.h
@@ -0,0 +1,164 @@
+#ifndef __TYPEDEF_H__
+#define __TYPEDEF_H__
+#include "sbi/sbi_types.h"
+
+typedef	unsigned char    u8;
+typedef	unsigned short  u16;
+typedef	unsigned int  u32;
+typedef	unsigned int  uint;
+// typedef	unsigned long long  u64;
+
+typedef	char   s8;
+typedef	short  s16;
+typedef	int  s32;
+// typedef	long long  s64;
+
+#define be64_to_le64(x) ((u64)(				\
+	(((u64)(x) & (u64)0x00000000000000ffULL) << 56) |	\
+	(((u64)(x) & (u64)0x000000000000ff00ULL) << 40) |	\
+	(((u64)(x) & (u64)0x0000000000ff0000ULL) << 24) |	\
+	(((u64)(x) & (u64)0x00000000ff000000ULL) <<  8) |	\
+	(((u64)(x) & (u64)0x000000ff00000000ULL) >>  8) |	\
+	(((u64)(x) & (u64)0x0000ff0000000000ULL) >> 24) |	\
+	(((u64)(x) & (u64)0x00ff000000000000ULL) >> 40) |	\
+	(((u64)(x) & (u64)0xff00000000000000ULL) >> 56)))
+
+#define le64_to_be64(x) ((u64)(				\
+	(((u64)(x) & (u64)0x00000000000000ffULL) << 56) |	\
+	(((u64)(x) & (u64)0x000000000000ff00ULL) << 40) |	\
+	(((u64)(x) & (u64)0x0000000000ff0000ULL) << 24) |	\
+	(((u64)(x) & (u64)0x00000000ff000000ULL) <<  8) |	\
+	(((u64)(x) & (u64)0x000000ff00000000ULL) >>  8) |	\
+	(((u64)(x) & (u64)0x0000ff0000000000ULL) >> 24) |	\
+	(((u64)(x) & (u64)0x00ff000000000000ULL) >> 40) |	\
+	(((u64)(x) & (u64)0xff00000000000000ULL) >> 56)))
+
+static inline u16 __get_unaligned_le16(const u8 *p)
+{
+  return p[0] | p[1] << 8;
+}
+
+static inline u32 __get_unaligned_le32(const u8 *p)
+{
+  return p[0] | p[1] << 8 | p[2] << 16 | p[3] << 24;
+}
+
+static inline u64 __get_unaligned_le64(const u8 *p)
+{
+  return (u64)__get_unaligned_le32(p + 4) << 32 
+    | __get_unaligned_le32(p);
+}
+
+static inline void __put_unaligned_le16(u16 val, u8 *p)
+{
+  *p++ = val;
+  *p++ = val >> 8;
+}
+
+static inline void __put_unaligned_le32(u32 val, u8 *p)
+{
+  __put_unaligned_le16(val >> 16, p + 2);
+  __put_unaligned_le16(val, p);
+}
+
+static inline void __put_unaligned_le64(u64 val, u8 *p)
+{
+  __put_unaligned_le32(val >> 32, p + 4);
+  __put_unaligned_le32(val, p);
+}
+
+static inline u16 get_unaligned_le16(const void *p)
+{
+  return __get_unaligned_le16((const u8 *)p);
+}
+
+static inline u32 get_unaligned_le32(const void *p)
+{
+  return __get_unaligned_le32((const u8 *)p);
+}
+
+static inline u64 get_unaligned_le64(const void *p)
+{
+  return __get_unaligned_le64((const u8 *)p);
+}
+
+static inline void put_unaligned_le16(u16 val, void *p)
+{
+  __put_unaligned_le16(val, p);
+}
+
+static inline void put_unaligned_le32(u32 val, void *p)
+{
+  __put_unaligned_le32(val, p);
+}
+
+static inline void put_unaligned_le64(u64 val, void *p)
+{
+  __put_unaligned_le64(val, p);
+}
+
+static inline u16 __get_unaligned_be16(const u8 *p)
+{
+  return p[0] << 8 | p[1];
+}
+
+static inline u32 __get_unaligned_be32(const u8 *p)
+{
+  return p[0] << 24 | p[1] << 16 | p[2] << 8 | p[3];
+}
+
+static inline u64 __get_unaligned_be64(const u8 *p)
+{
+  return (u64)__get_unaligned_be32(p) << 32 
+    | __get_unaligned_be32(p + 4);
+}
+
+static inline void __put_unaligned_be16(u16 val, u8 *p)
+{
+  *p++ = val >> 8;
+  *p++ = val;
+}
+
+static inline void __put_unaligned_be32(u32 val, u8 *p)
+{
+  __put_unaligned_be16(val >> 16, p);
+  __put_unaligned_be16(val, p + 2);
+}
+
+static inline void __put_unaligned_be64(u64 val, u8 *p)
+{
+  __put_unaligned_be32(val >> 32, p);
+  __put_unaligned_be32(val, p + 4);
+}
+
+static inline u16 get_unaligned_be16(const void *p)
+{
+  return __get_unaligned_be16((const u8 *)p);
+}
+
+static inline u32 get_unaligned_be32(const void *p)
+{
+  return __get_unaligned_be32((const u8 *)p);
+}
+
+static inline u64 get_unaligned_be64(const void *p)
+{
+  return __get_unaligned_be64((const u8 *)p);
+}
+
+static inline void put_unaligned_be16(u16 val, void *p)
+{
+  __put_unaligned_be16(val, p);
+}
+
+static inline void put_unaligned_be32(u32 val, void *p)
+{
+  __put_unaligned_be32(val, p);
+}
+
+static inline void put_unaligned_be64(u64 val, void *p)
+{
+  __put_unaligned_be64(val, p);
+}
+
+#endif
diff --git a/include/sm/ipi.h b/include/sm/ipi.h
new file mode 100644
index 0000000..e8316cd
--- /dev/null
+++ b/include/sm/ipi.h
@@ -0,0 +1,26 @@
+#ifndef _IPI_H
+#define _IPI_H
+
+#include "sbi/riscv_atomic.h"
+#include "sbi/riscv_locks.h"
+#include "sbi/sbi_types.h"
+
+#define IPI_PMP_SYNC         0x1
+#define IPI_STOP_ENCLAVE     0x2
+#define IPI_DESTROY_ENCLAVE  0x3
+
+struct ipi_mail_t
+{
+  uintptr_t event;
+  char data[40];
+};
+
+extern int ipi_mail_pending[];
+
+void send_ipi_mail(uintptr_t dest_hart, uintptr_t need_sync);
+
+void wait_pending_ipi(uintptr_t mask);
+
+void handle_ipi_mail(uintptr_t *regs);
+
+#endif /* _IPI_H */
diff --git a/include/sm/platform/pt_area/platform.h b/include/sm/platform/pt_area/platform.h
new file mode 100644
index 0000000..0a20068
--- /dev/null
+++ b/include/sm/platform/pt_area/platform.h
@@ -0,0 +1,9 @@
+#ifndef _PLATFORM_H
+#define _PLATFORM_H
+
+#include "sm/enclave_mm.h"
+#include "sm/platform/pt_area/platform_thread.h"
+
+int platform_init();
+
+#endif /* _PLATFORM_H */
diff --git a/include/sm/platform/pt_area/platform_thread.h b/include/sm/platform/pt_area/platform_thread.h
new file mode 100644
index 0000000..0a71c0b
--- /dev/null
+++ b/include/sm/platform/pt_area/platform_thread.h
@@ -0,0 +1,18 @@
+#ifndef _PLATFORM_THREAD_H
+#define _PLATFORM_THREAD_H
+
+#include "sm/thread.h"
+
+void platform_enter_enclave_world();
+
+void platform_exit_enclave_world();
+
+int platform_check_in_enclave_world();
+
+int platform_check_enclave_authentication();
+
+void platform_switch_to_enclave_ptbr(struct thread_state_t* thread, uintptr_t ptbr);
+
+void platform_switch_to_host_ptbr(struct thread_state_t* thread, uintptr_t ptbr);
+
+#endif /* _PLATFORM_THREAD_H */
diff --git a/include/sm/pmp.h b/include/sm/pmp.h
new file mode 100644
index 0000000..ca963c7
--- /dev/null
+++ b/include/sm/pmp.h
@@ -0,0 +1,66 @@
+#ifndef _PMP_H
+#define _PMP_H
+
+#include "sbi/sbi_types.h"
+#include "sbi/riscv_encoding.h"
+#include "sbi/sbi_hartmask.h"
+
+#define NPMP 8
+
+#define PMP_OFF   0x00
+#define PMP_NO_PERM  0
+
+#define PMPCFG_BIT_NUM            8
+#define PMPCFG_BITS               0xFF
+#define PMP_PER_CFG_REG           (sizeof(uintptr_t) * 8 / PMPCFG_BIT_NUM)
+
+#define PMP_SET(num, cfg_index, pmpaddr, pmpcfg) do { \
+  uintptr_t oldcfg = csr_read(CSR_PMPCFG##cfg_index); \
+  pmpcfg |= (oldcfg & ~((uintptr_t)PMPCFG_BITS << (uintptr_t)PMPCFG_BIT_NUM*(num%PMP_PER_CFG_REG))); \
+  asm volatile ("la t0, 1f\n\t" \
+                "csrrw t0, mtvec, t0\n\t" \
+                "csrw pmpaddr"#num", %0\n\t" \
+                "csrw pmpcfg"#cfg_index", %1\n\t" \
+                "sfence.vma\n\t"\
+                ".align 2\n\t" \
+                "1: csrw mtvec, t0 \n\t" \
+                : : "r" (pmpaddr), "r" (pmpcfg) : "t0"); \
+} while(0)
+
+#define PMP_READ(num, cfg_index, pmpaddr, pmpcfg) do { \
+  asm volatile("csrr %0, pmpaddr"#num : "=r"(pmpaddr) :); \
+  asm volatile("csrr %0, pmpcfg"#cfg_index : "=r"(pmpcfg) :); \
+} while(0)
+
+struct pmp_config_t
+{
+  uintptr_t paddr;
+  unsigned long size;
+  uintptr_t perm;
+  uintptr_t mode;
+};
+
+struct pmp_data_t
+{
+  struct pmp_config_t pmp_config_arg;
+  int pmp_idx_arg;
+  struct sbi_hartmask smask;
+};
+
+#define SBI_PMP_DATA_INIT(__ptr, __pmp_config_arg, __pmp_idx_arg, __src) \
+do { \
+	(__ptr)->pmp_config_arg = (__pmp_config_arg); \
+	(__ptr)->pmp_idx_arg = (__pmp_idx_arg); \
+	SBI_HARTMASK_INIT_EXCEPT(&(__ptr)->smask, (__src)); \
+} while (0)
+
+void set_pmp_and_sync(int pmp_idx, struct pmp_config_t);
+void clear_pmp_and_sync(int pmp_idx);
+void set_pmp(int pmp_idx, struct pmp_config_t);
+void clear_pmp(int pmp_idx);
+struct pmp_config_t get_pmp(int pmp_idx);
+int region_overlap(uintptr_t pa0, uintptr_t size0, uintptr_t pa1, uintptr_t size1);
+int region_contain(uintptr_t pa0, uintptr_t size0, uintptr_t pa1, uintptr_t size1);
+int illegal_pmp_addr(uintptr_t addr, uintptr_t size);
+
+#endif /* _PMP_H */
diff --git a/include/sm/relay_page.h b/include/sm/relay_page.h
new file mode 100644
index 0000000..92af036
--- /dev/null
+++ b/include/sm/relay_page.h
@@ -0,0 +1,12 @@
+#ifndef _RELAY_PAGE_H
+#define _RELAY_PAGE_H
+
+#include "sm/enclave.h"
+#include "sm/enclave_args.h"
+
+
+uintptr_t asyn_enclave_call(uintptr_t *regs, uintptr_t enclave_name, uintptr_t arg);
+uintptr_t split_mem_region(uintptr_t *regs, uintptr_t mem_addr, uintptr_t mem_size, uintptr_t split_addr);
+int free_all_relay_page(unsigned long *mm_arg_paddr, unsigned long *mm_arg_size);
+
+#endif /* _RELAY_PAGE_H */
diff --git a/include/sm/server_enclave.h b/include/sm/server_enclave.h
new file mode 100644
index 0000000..a87c704
--- /dev/null
+++ b/include/sm/server_enclave.h
@@ -0,0 +1,21 @@
+#ifndef _SERVER_ENCLAVE_H
+#define _SERVER_ENCLAVE_H
+
+#include "sm/enclave.h"
+#include "sm/enclave_args.h"
+
+struct server_enclave_t
+{
+  //FIXME: enclave has its own name now, so it need not to assign a server name to server enclave 
+  char server_name[NAME_LEN];
+  struct enclave_t* entity;
+};
+
+#define SERVERS_PER_METADATA_REGION 100
+
+uintptr_t create_server_enclave(enclave_create_param_t create_args);
+uintptr_t destroy_server_enclave(uintptr_t* regs, unsigned int eid);
+uintptr_t acquire_server_enclave(uintptr_t *regs, char *server_name);
+uintptr_t get_caller_id(uintptr_t* regs);
+
+#endif /* _SERVER_ENCLAVE_H */
diff --git a/include/sm/sm.h b/include/sm/sm.h
new file mode 100644
index 0000000..2e7a638
--- /dev/null
+++ b/include/sm/sm.h
@@ -0,0 +1,144 @@
+#ifndef _SM_H
+#define _SM_H
+
+#include "sbi/sbi_types.h"
+#include "sm/enclave_args.h"
+#include "sm/ipi.h"
+
+#define SM_BASE 0x80000000UL
+#define SM_SIZE 0x200000UL
+
+//SBI_CALL NUMBERS
+#define SBI_SET_PTE            101
+#define SBI_SET_PTE_ONE          1
+#define SBI_PTE_MEMSET           2
+#define SBI_PTE_MEMCPY           3
+#define SBI_SM_INIT            100
+#define SBI_CREATE_ENCLAVE      99
+#define SBI_ATTEST_ENCLAVE      98
+#define SBI_RUN_ENCLAVE         97
+#define SBI_STOP_ENCLAVE        96
+#define SBI_RESUME_ENCLAVE      95
+#define SBI_DESTROY_ENCLAVE     94
+#define SBI_MEMORY_EXTEND       92
+#define SBI_MEMORY_RECLAIM      91
+#define SBI_CREATE_SERVER_ENCLAVE         90
+#define SBI_DESTROY_SERVER_ENCLAVE        89
+
+#define SBI_SM_DEBUG_PRINT               88
+#define SBI_RUN_SHADOW_ENCLAVE           87
+#define SBI_CREATE_SHADOW_ENCLAVE        86
+
+#define SBI_SCHRODINGER_INIT             85
+#define SBI_SM_PT_AREA_SEPARATION        83
+#define SBI_SM_SPLIT_HUGE_PAGE           82
+#define SBI_SM_MAP_PTE                   81
+#define SBI_ATTEST_SHADOW_ENCLAVE 80
+
+//Error code of SBI_CREATE_ENCLAVE
+#define ENCLAVE_ERROR           -1
+#define ENCLAVE_NO_MEM          -2
+#define ENCLAVE_ATTESTATION          -3
+
+//The enclave return result 
+#define ENCLAVE_SUCCESS          0
+#define ENCLAVE_TIMER_IRQ        1
+#define ENCLAVE_OCALL            2
+#define ENCLAVE_YIELD            3
+
+//The function id of the resume reason
+#define RESUME_FROM_TIMER_IRQ    0
+#define RESUME_FROM_STOP         1
+#define RESUME_FROM_OCALL        2
+
+
+#define SBI_LEGAL_MAX            100UL
+//ENCLAVE_CALL NUMBERS
+#define SBI_EXIT_ENCLAVE         1
+#define SBI_ENCLAVE_OCALL        2
+#define SBI_ACQUIRE_SERVER       3
+#define SBI_CALL_ENCLAVE         4
+#define SBI_ENCLAVE_RETURN       5
+#define SBI_ASYN_ENCLAVE_CALL    6
+#define SBI_SPLIT_MEM_REGION     7
+#define SBI_GET_CALLER_ID        8
+#define SBI_YIELD                10 //reserve space for other enclave call operation
+
+//ENCLAVE OCALL NUMBERS
+#define OCALL_MMAP                   1
+#define OCALL_UNMAP                  2
+#define OCALL_SYS_WRITE              3
+#define OCALL_SBRK                   4
+#define OCALL_READ_SECT              5
+#define OCALL_WRITE_SECT             6
+#define OCALL_RETURN_RELAY_PAGE      7
+
+typedef int page_meta;
+#define NORMAL_PAGE                      ((page_meta)0x7FFFFFFF)
+#define ZERO_MAP_PAGE                    ((page_meta)0x7FFFFFFE)
+#define PRIVATE_PAGE                     ((page_meta)0x80000000)
+#define IS_PRIVATE_PAGE(meta)            (((page_meta)meta) & PRIVATE_PAGE)
+#define IS_PUBLIC_PAGE(meta)             (!IS_PRIVATE_PAGE(meta))
+#define IS_ZERO_MAP_PAGE(meta)           (((page_meta)meta & NORMAL_PAGE) == ZERO_MAP_PAGE)
+#define IS_SCHRODINGER_PAGE(meta)        (((page_meta)meta & NORMAL_PAGE) != NORMAL_PAGE)
+#define MAKE_PRIVATE_PAGE(meta)          ((page_meta)meta | PRIVATE_PAGE)
+#define MAKE_PUBLIC_PAGE(meta)           ((page_meta)meta & NORMAL_PAGE)
+#define MAKE_ZERO_MAP_PAGE(meta)         (((page_meta)meta & PRIVATE_PAGE) | ZERO_MAP_PAGE)
+#define MAKE_SCHRODINGER_PAGE(pri, pos)  (pri ? \
+    (PRIVATE_PAGE | ((page_meta)pos & NORMAL_PAGE)) \
+    : ((page_meta)pos & NORMAL_PAGE))
+#define SCHRODINGER_PTE_POS(meta)        (IS_ZERO_MAP_PAGE(meta) ? -1 : ((int)meta & (int)0x7FFFFFFF))
+
+void sm_init();
+
+int enable_enclave();
+//remember to acquire mbitmap_lock before using these functions
+int contain_private_range(uintptr_t pfn, uintptr_t pagenum);
+int test_public_range(uintptr_t pfn, uintptr_t pagenum);
+int set_private_range(uintptr_t pfn, uintptr_t pagenum);
+int set_public_range(uintptr_t pfn, uintptr_t pagenum);
+int unmap_mm_region(unsigned long paddr, unsigned long size);
+int remap_mm_region(unsigned long paddr, unsigned long size);
+
+int check_in_enclave_world();
+
+//called by host
+uintptr_t sm_sm_init(uintptr_t pt_area_base, uintptr_t pt_area_size, uintptr_t mbitmap_base, uintptr_t mbitmap_size);
+uintptr_t sm_pt_area_separation(uintptr_t pgd_order, uintptr_t pmd_order);
+uintptr_t sm_set_pte(uintptr_t flag, uintptr_t* pte_addr, uintptr_t pte_src, uintptr_t size);
+uintptr_t sm_split_huge_page(unsigned long paddr, unsigned long size, uintptr_t split_pte);
+uintptr_t sm_map_pte(uintptr_t* pte, uintptr_t* new_pte_addr);
+uintptr_t sm_mm_init(uintptr_t paddr, uintptr_t size);
+uintptr_t sm_mm_extend(uintptr_t paddr, uintptr_t size);
+uintptr_t sm_create_enclave(uintptr_t enclave_create_args);
+uintptr_t sm_attest_enclave(uintptr_t enclave_id, uintptr_t report, uintptr_t nonce);
+uintptr_t sm_attest_shadow_enclave(uintptr_t enclave_id, uintptr_t report, uintptr_t nonce);
+uintptr_t sm_run_enclave(uintptr_t *regs, uintptr_t enclave_id, uintptr_t addr, uintptr_t size);
+uintptr_t sm_stop_enclave(uintptr_t *regs, uintptr_t enclave_id);
+uintptr_t sm_resume_enclave(uintptr_t *regs, uintptr_t enclave_id);
+uintptr_t sm_destroy_enclave(uintptr_t *regs, uintptr_t enclave_id);
+uintptr_t sm_create_server_enclave(uintptr_t enclave_create_args);
+uintptr_t sm_destroy_server_enclave(uintptr_t *regs, uintptr_t enclave_id);
+
+uintptr_t sm_run_shadow_enclave(uintptr_t *regs, uintptr_t enclave_id, uintptr_t shadow_enclave_run_args, uintptr_t addr, uintptr_t size);
+uintptr_t sm_create_shadow_enclave(uintptr_t enclave_create_args);
+
+//called by enclave
+uintptr_t sm_enclave_ocall(uintptr_t *regs, uintptr_t ocall_func_id, uintptr_t arg0, uintptr_t arg1);
+uintptr_t sm_exit_enclave(uintptr_t *regs, uintptr_t retval);
+uintptr_t sm_server_enclave_acquire(uintptr_t *regs, uintptr_t server_name);
+uintptr_t sm_get_caller_id(uintptr_t *regs);
+uintptr_t sm_call_enclave(uintptr_t *regs, uintptr_t enclave_id, uintptr_t arg);
+uintptr_t sm_enclave_return(uintptr_t *regs, uintptr_t arg);
+uintptr_t sm_asyn_enclave_call(uintptr_t *regs, uintptr_t enclave_name, uintptr_t arg);
+uintptr_t sm_split_mem_region(uintptr_t *regs, uintptr_t mem_addr, uintptr_t mem_size, uintptr_t split_addr);
+
+//called when timer irq
+uintptr_t sm_do_timer_irq(uintptr_t *regs, uintptr_t mcause, uintptr_t mepc);
+uintptr_t sm_handle_yield(uintptr_t *regs, uintptr_t mcause, uintptr_t mepc);
+
+uintptr_t sm_schrodinger_init(uintptr_t paddr, uintptr_t size);
+
+uintptr_t sm_print(uintptr_t paddr, uintptr_t size);
+
+#endif /* _SM_H */
diff --git a/include/sm/thread.h b/include/sm/thread.h
new file mode 100644
index 0000000..9a42d56
--- /dev/null
+++ b/include/sm/thread.h
@@ -0,0 +1,66 @@
+#ifndef __THREAD_H__
+#define __THREAD_H__
+
+#include "sbi/sbi_types.h"
+
+/// \brief define the number of general registers
+#define N_GENERAL_REGISTERS 32
+
+struct general_registers_t
+{
+  uintptr_t slot;
+  uintptr_t ra;
+  uintptr_t sp;
+  uintptr_t gp;
+  uintptr_t tp;
+  uintptr_t t0;
+  uintptr_t t1;
+  uintptr_t t2;
+  uintptr_t s0;
+  uintptr_t s1;
+  uintptr_t a0;
+  uintptr_t a1;
+  uintptr_t a2;
+  uintptr_t a3;
+  uintptr_t a4;
+  uintptr_t a5;
+  uintptr_t a6;
+  uintptr_t a7;
+  uintptr_t s2;
+  uintptr_t s3;
+  uintptr_t s4;
+  uintptr_t s5;
+  uintptr_t s6;
+  uintptr_t s7;
+  uintptr_t s8;
+  uintptr_t s9;
+  uintptr_t s10;
+  uintptr_t s11;
+  uintptr_t t3;
+  uintptr_t t4;
+  uintptr_t t5;
+  uintptr_t t6;
+};
+
+/* enclave thread state */
+struct thread_state_t
+{
+  uintptr_t encl_ptbr;
+  uintptr_t prev_stvec;
+  uintptr_t prev_mie;
+  uintptr_t prev_mideleg;
+  uintptr_t prev_medeleg;
+  uintptr_t prev_mepc;
+  uintptr_t prev_cache_binding;
+  struct general_registers_t prev_state;
+};
+
+/* swap previous and current thread states */
+void swap_prev_state(struct thread_state_t* state, uintptr_t* regs);
+void swap_prev_mepc(struct thread_state_t* state, uintptr_t mepc);
+void swap_prev_stvec(struct thread_state_t* state, uintptr_t stvec);
+void swap_prev_cache_binding(struct thread_state_t* state, uintptr_t cache_binding);
+void swap_prev_mie(struct thread_state_t* state, uintptr_t mie);
+void swap_prev_mideleg(struct thread_state_t* state, uintptr_t mideleg);
+void swap_prev_medeleg(struct thread_state_t* state, uintptr_t medeleg);
+#endif /* thread */
diff --git a/include/sm/vm.h b/include/sm/vm.h
new file mode 100644
index 0000000..bc87a20
--- /dev/null
+++ b/include/sm/vm.h
@@ -0,0 +1,36 @@
+#ifndef _VM_H
+#define _VM_H
+
+#include "sbi/riscv_encoding.h"
+#include "sbi/sbi_bitops.h"
+#include "sbi/sbi_types.h"
+
+#define MEGAPAGE_SIZE ((uintptr_t)(RISCV_PGSIZE << RISCV_PGLEVEL_BITS))
+#if __riscv_xlen == 64
+# define SATP_MODE_CHOICE INSERT_FIELD(0, SATP64_MODE, SATP_MODE_SV39)
+# define VA_BITS 39
+# define GIGAPAGE_SIZE (MEGAPAGE_SIZE << RISCV_PGLEVEL_BITS)
+#else
+# define SATP_MODE_CHOICE INSERT_FIELD(0, SATP32_MODE, SATP_MODE_SV32)
+# define VA_BITS 32
+#endif
+
+typedef uintptr_t pte_t;
+extern pte_t* root_page_table;
+
+static inline void flush_tlb()
+{
+  asm volatile ("sfence.vma");
+}
+
+static inline pte_t pte_create(uintptr_t ppn, int type)
+{
+  return (ppn << PTE_PPN_SHIFT) | PTE_V | type;
+}
+
+static inline pte_t ptd_create(uintptr_t ppn)
+{
+  return pte_create(ppn, PTE_V);
+}
+
+#endif
diff --git a/lib/sbi/objects.mk b/lib/sbi/objects.mk
index fa808a0..ecf7ba9 100644
--- a/lib/sbi/objects.mk
+++ b/lib/sbi/objects.mk
@@ -21,6 +21,7 @@ libsbi-objs-y += sbi_ecall_hsm.o
 libsbi-objs-y += sbi_ecall_legacy.o
 libsbi-objs-y += sbi_ecall_replace.o
 libsbi-objs-y += sbi_ecall_vendor.o
+libsbi-objs-y += sbi_ecall_penglai.o
 libsbi-objs-y += sbi_emulate_csr.o
 libsbi-objs-y += sbi_fifo.o
 libsbi-objs-y += sbi_hart.o
@@ -40,3 +41,22 @@ libsbi-objs-y += sbi_tlb.o
 libsbi-objs-y += sbi_trap.o
 libsbi-objs-y += sbi_unpriv.o
 libsbi-objs-y += sbi_expected_trap.o
+libsbi-objs-y += sbi_pmp.o
+libsbi-objs-y += sbi_tvm.o
+libsbi-objs-y += sbi_ipi_destroy_enclave.o
+libsbi-objs-y += sm/enclave_mm.o
+libsbi-objs-y += sm/platform/pt_area/platform.o
+libsbi-objs-y += sm/platform/pt_area/platform_thread.o
+libsbi-objs-y += sm/enclave_vm.o
+libsbi-objs-y += sm/enclave.o
+libsbi-objs-y += sm/pmp.o
+libsbi-objs-y += sm/relay_page.o
+libsbi-objs-y += sm/server_enclave.o
+libsbi-objs-y += sm/sm.o
+libsbi-objs-y += sm/thread.o
+libsbi-objs-y += sm/attest.o
+libsbi-objs-y += sm/gm/big.o
+libsbi-objs-y += sm/gm/ecc.o
+libsbi-objs-y += sm/gm/random.o
+libsbi-objs-y += sm/gm/sm3.o
+libsbi-objs-y += sm/gm/sm2.o
\ No newline at end of file
diff --git a/lib/sbi/sbi_ecall.c b/lib/sbi/sbi_ecall.c
index 64c9933..7ccfe59 100644
--- a/lib/sbi/sbi_ecall.c
+++ b/lib/sbi/sbi_ecall.c
@@ -12,6 +12,8 @@
 #include <sbi/sbi_ecall_interface.h>
 #include <sbi/sbi_error.h>
 #include <sbi/sbi_trap.h>
+#include "sm/enclave.h"
+#include "sm/sm.h"
 
 u16 sbi_ecall_version_major(void)
 {
@@ -92,6 +94,64 @@ void sbi_ecall_unregister_extension(struct sbi_ecall_extension *ext)
 		sbi_list_del_init(&ext->head);
 }
 
+int enclave_call_trap(struct sbi_trap_regs* regs)
+{
+	unsigned long retval;
+	if(check_in_enclave_world() < 0){
+		retval = SBI_ERR_FAILED;
+		regs->mepc += 4;
+		regs->a0 = retval;
+		sbi_printf("M mode: %s check in enclave world is failed \n", __func__);
+		return 0;
+	}
+
+	uintptr_t n = regs->a7;
+	csr_write(CSR_MEPC, regs->mepc + 4);
+	uintptr_t arg0 = regs->a0, arg1 = regs->a1, arg2 = regs->a2;
+	switch (n)
+	{
+		case SBI_EXIT_ENCLAVE:
+			retval = sm_exit_enclave((uintptr_t*)regs, arg0);
+			break;
+		case SBI_ENCLAVE_OCALL:
+			retval = sm_enclave_ocall((uintptr_t*)regs, arg0, arg1, arg2);
+			break;
+		case SBI_ACQUIRE_SERVER:
+			retval = sm_server_enclave_acquire((uintptr_t*)regs, arg0);
+			break;
+		case SBI_GET_CALLER_ID:
+			retval = sm_get_caller_id((uintptr_t*)regs);
+			break;
+		case SBI_CALL_ENCLAVE:
+			retval = sm_call_enclave((uintptr_t*)regs, arg0, arg1);
+			break;
+		case SBI_ENCLAVE_RETURN:
+			retval = sm_enclave_return((uintptr_t*)regs, arg0);
+			break;
+		case SBI_ASYN_ENCLAVE_CALL:
+			retval = sm_asyn_enclave_call((uintptr_t*)regs, arg0, arg1);
+			break;
+		case SBI_SPLIT_MEM_REGION:
+			retval = sm_split_mem_region((uintptr_t*)regs, arg0, arg1, arg2);
+			break;
+		default:
+			retval = SBI_ERR_FAILED;
+			break;
+	}
+	regs->a0 = retval;
+	if (!cpu_in_enclave(csr_read(CSR_MHARTID)))
+	{
+		if ((retval >= 0UL) && (retval <= SBI_LEGAL_MAX))
+		{
+			regs->a0 = SBI_OK;
+			regs->a1 = retval;
+		}
+	}
+	regs->mepc = csr_read(CSR_MEPC);
+	regs->mstatus = csr_read(CSR_MSTATUS);
+	return 0;
+}
+
 int sbi_ecall_handler(struct sbi_trap_regs *regs)
 {
 	int ret = 0;
@@ -109,23 +169,32 @@ int sbi_ecall_handler(struct sbi_trap_regs *regs)
 	args[3] = regs->a3;
 	args[4] = regs->a4;
 	args[5] = regs->a5;
-
+	// sbi_printf("SBI ECALL extension_id is %lx func_id is %lx\n", extension_id, func_id);
 	ext = sbi_ecall_find_extension(extension_id);
-	if (ext && ext->handle) {
+	if (extension_id != SBI_EXT_PENGLAI)
+	{
+		if (ext && ext->handle) {
+			ret = ext->handle(extension_id, func_id,
+					args, &out_val, &trap);
+			if (extension_id >= SBI_EXT_0_1_SET_TIMER &&
+				extension_id <= SBI_EXT_0_1_SHUTDOWN)
+				is_0_1_spec = 1;
+		} else {
+			ret = SBI_ENOTSUPP;
+		}
+	}
+	else
+	{
 		ret = ext->handle(extension_id, func_id,
-				  args, &out_val, &trap);
-		if (extension_id >= SBI_EXT_0_1_SET_TIMER &&
-		    extension_id <= SBI_EXT_0_1_SHUTDOWN)
-			is_0_1_spec = 1;
-	} else {
-		ret = SBI_ENOTSUPP;
+					(unsigned long *)regs, &out_val, &trap);
 	}
+	
 
-	if (ret == SBI_ETRAP) {
+	if ((ret == SBI_ETRAP) && (extension_id != SBI_EXT_PENGLAI)) {
 		trap.epc = regs->mepc;
 		sbi_trap_redirect(regs, &trap);
 	} else {
-		if (ret < SBI_LAST_ERR) {
+		if ((ret < SBI_LAST_ERR) && (extension_id != SBI_EXT_PENGLAI)) {
 			sbi_printf("%s: Invalid error %d for ext=0x%lx "
 				   "func=0x%lx\n", __func__, ret,
 				   extension_id, func_id);
@@ -140,10 +209,25 @@ int sbi_ecall_handler(struct sbi_trap_regs *regs)
 		 * accordingly for now. Once fatal errors are defined, that
 		 * case should be handled differently.
 		 */
-		regs->mepc += 4;
-		regs->a0 = ret;
-		if (!is_0_1_spec)
-			regs->a1 = out_val;
+		if (extension_id != SBI_EXT_PENGLAI)
+		{
+			regs->mepc += 4;
+			regs->a0 = ret;
+			if (!is_0_1_spec)
+				regs->a1 = out_val;
+		}
+		else
+		{
+			regs->a0 = out_val;
+			if (!cpu_in_enclave(csr_read(CSR_MHARTID)))
+			{
+				if ((out_val >= 0UL) && (out_val <= SBI_LEGAL_MAX))
+				{
+					regs->a0 = SBI_OK;
+					regs->a1 = out_val;
+				}
+			}
+		}
 	}
 
 	return 0;
@@ -173,6 +257,9 @@ int sbi_ecall_init(void)
 	if (ret)
 		return ret;
 	ret = sbi_ecall_register_extension(&ecall_vendor);
+	if (ret)
+		return ret;
+	ret = sbi_ecall_register_extension(&ecall_pengali);
 	if (ret)
 		return ret;
 
diff --git a/lib/sbi/sbi_ecall_penglai.c b/lib/sbi/sbi_ecall_penglai.c
new file mode 100644
index 0000000..9f97cfa
--- /dev/null
+++ b/lib/sbi/sbi_ecall_penglai.c
@@ -0,0 +1,98 @@
+/*
+ * SPDX-License-Identifier: BSD-2-Clause
+ *
+ * Copyright (c) 2020 Western Digital Corporation or its affiliates.
+ *
+ * Authors:
+ *   Anup Patel <anup.patel@wdc.com>
+ *   Atish Patra <atish.patra@wdc.com>
+ */
+
+#include <sbi/sbi_ecall.h>
+#include <sbi/sbi_ecall_interface.h>
+#include <sbi/sbi_error.h>
+#include <sbi/sbi_version.h>
+#include <sbi/riscv_asm.h>
+#include <sbi/sbi_console.h>
+#include "sm/sm.h"
+
+
+static int sbi_ecall_penglai_handler(unsigned long extid, unsigned long funcid,
+				  unsigned long *args, unsigned long *out_val,
+				  struct sbi_trap_info *out_trap)
+{
+  uintptr_t arg0 = args[10], arg1 = args[11], arg2 = args[12], arg3 = args[13], retval;
+  csr_write(CSR_MEPC, args[32] + 4);
+  switch (funcid) {
+    case SBI_SET_PTE:
+      retval = sm_set_pte(arg0, (uintptr_t*)arg1, arg2, arg3);
+      break;
+    case SBI_SM_INIT:
+      retval = sm_sm_init(arg0, arg1, arg2, arg3);
+      break;
+    case SBI_SM_PT_AREA_SEPARATION:
+      retval = sm_pt_area_separation(arg0, arg1);
+      break;
+    case SBI_SM_SPLIT_HUGE_PAGE:
+      retval = sm_split_huge_page(arg0, arg1, arg2);
+      break;
+    case SBI_SM_MAP_PTE:
+      retval = sm_map_pte((uintptr_t *)arg0, (uintptr_t *)arg1);
+      break;
+    case SBI_MEMORY_EXTEND:
+      retval = sm_mm_extend(arg0, arg1);
+      break;
+    case SBI_CREATE_ENCLAVE:
+      retval = sm_create_enclave(arg0);
+      break;
+    case SBI_RUN_ENCLAVE:
+      retval = sm_run_enclave(args, arg0, arg1, arg2);
+      break;
+    case SBI_STOP_ENCLAVE:
+      retval = sm_stop_enclave(args, arg0);
+      break;
+    case SBI_RESUME_ENCLAVE:
+      retval = sm_resume_enclave(args, arg0);
+      break;
+    case SBI_DESTROY_ENCLAVE:
+      retval = sm_destroy_enclave(args, arg0);
+      break;
+    case SBI_ATTEST_ENCLAVE:
+      retval = sm_attest_enclave(arg0, arg1, arg2);
+      break;
+    case SBI_CREATE_SERVER_ENCLAVE:
+      retval = sm_create_server_enclave(arg0);
+      break;
+    case SBI_CREATE_SHADOW_ENCLAVE:
+      retval = sm_create_shadow_enclave(arg0);
+      break;
+    case SBI_RUN_SHADOW_ENCLAVE:
+      retval = sm_run_shadow_enclave(args, arg0, arg1, arg2, arg3);
+      break;
+    case SBI_ATTEST_SHADOW_ENCLAVE:
+      retval = sm_attest_shadow_enclave(arg0, arg1, arg2);
+      break;
+    case SBI_DESTROY_SERVER_ENCLAVE:
+      retval = sm_destroy_server_enclave(args, arg0);
+      break;
+    case SBI_SCHRODINGER_INIT:
+      retval = sm_schrodinger_init(arg0, arg1);
+      break;
+    case 84:
+      retval = sm_print(arg0, arg1);
+      break;
+	
+	default:
+		retval = SBI_ENOTSUPP;
+	}
+  args[32] = csr_read(CSR_MEPC);
+  args[33] = csr_read(CSR_MSTATUS);
+  *out_val = retval;
+	return retval;
+}
+
+struct sbi_ecall_extension ecall_pengali = {
+	.extid_start = SBI_EXT_PENGLAI,
+	.extid_end = SBI_EXT_PENGLAI,
+	.handle = sbi_ecall_penglai_handler,
+};
diff --git a/lib/sbi/sbi_hart.c b/lib/sbi/sbi_hart.c
index fa20bd2..af93fd6 100644
--- a/lib/sbi/sbi_hart.c
+++ b/lib/sbi/sbi_hart.c
@@ -19,6 +19,7 @@
 #include <sbi/sbi_math.h>
 #include <sbi/sbi_platform.h>
 #include <sbi/sbi_string.h>
+#include <sm/sm.h>
 
 extern void __sbi_expected_trap(void);
 extern void __sbi_expected_trap_hext(void);
@@ -440,7 +441,9 @@ void __attribute__((noreturn)) sbi_hart_hang(void)
 		wfi();
 	__builtin_unreachable();
 }
-
+/*
+ * Switch the m mode into the s mode
+ */
 void __attribute__((noreturn))
 sbi_hart_switch_mode(unsigned long arg0, unsigned long arg1,
 		     unsigned long next_addr, unsigned long next_mode,
@@ -500,7 +503,9 @@ sbi_hart_switch_mode(unsigned long arg0, unsigned long arg1,
 		csr_write(CSR_USCRATCH, 0);
 		csr_write(CSR_UIE, 0);
 	}
-
+	//TODO:
+	//Set the pmp register to protect the monitor itself 
+	sm_init();
 	register unsigned long a0 asm("a0") = arg0;
 	register unsigned long a1 asm("a1") = arg1;
 	__asm__ __volatile__("mret" : : "r"(a0), "r"(a1));
diff --git a/lib/sbi/sbi_illegal_insn.c b/lib/sbi/sbi_illegal_insn.c
index 0e5523f..2a8f810 100644
--- a/lib/sbi/sbi_illegal_insn.c
+++ b/lib/sbi/sbi_illegal_insn.c
@@ -15,6 +15,7 @@
 #include <sbi/sbi_illegal_insn.h>
 #include <sbi/sbi_trap.h>
 #include <sbi/sbi_unpriv.h>
+#include <sbi/sbi_console.h>
 
 typedef int (*illegal_insn_func)(ulong insn, struct sbi_trap_regs *regs);
 
@@ -114,9 +115,67 @@ static illegal_insn_func illegal_insn_table[32] = {
 	truly_illegal_insn  /* 31 */
 };
 
+
+extern uintptr_t pt_area_base;
+extern uintptr_t pt_area_size;
+extern uintptr_t mbitmap_base;
+extern uintptr_t mbitmap_size;
+extern uintptr_t pgd_order;
+extern uintptr_t pmd_order;
+
 int sbi_illegal_insn_handler(ulong insn, struct sbi_trap_regs *regs)
 {
 	struct sbi_trap_info uptrap;
+	struct sbi_trap_info uptrap2;
+	ulong inst;
+	if (insn == 0)
+		inst = sbi_get_insn(regs->mepc, &uptrap2);
+	else
+		inst = insn;
+	
+	// Emulate the TVM
+	unsigned long mepc = regs->mepc;
+	/* Case1: write sptbr trapped by TVM */
+	if ((((inst>>20) & 0xfff) == 0x180)
+		&&((inst & 0x7f) == 0b1110011)
+		&& (((inst>>12) & 0x3) == 0b001))
+	{
+		// printm("here0 %d\r\n",((inst>>15) & 0x1f));
+		signed long val = *((unsigned long *)regs + ((inst>>15) & 0x1f));
+		unsigned long pa = (val & 0x3fffff)<<12;
+		if((pt_area_base < pa) && ((pt_area_base + (1<<pgd_order)*4096) > pa))
+		{
+			asm volatile ("csrrw x0, sptbr, %0":: "rK"(val));
+			csr_write(CSR_MEPC, mepc + 4);
+			regs->mepc = csr_read(CSR_MEPC);
+			return 0 ;
+		}
+	}
+	/* Case2: read sptbr trapped by TVM */
+	if((((inst>>20) & 0xfff) == 0x180)
+	&&((inst & 0x7f) == 0b1110011)
+	&& (((inst>>12) & 0x3) == 0b010))
+	{
+		// printm("here3 %d\r\n",((inst>>7) & 0x1f));
+		int idx = ((inst>>7) & 0x1f);
+		unsigned long __tmp;
+		asm volatile ("csrrs %0, sptbr, x0":"=r"(__tmp));
+		csr_write(CSR_MEPC, mepc + 4);
+		*((unsigned long *)regs + idx) = __tmp;
+		regs->mepc = csr_read(CSR_MEPC);
+		return 0 ;
+	}
+	/* Case3: sfence.vma trapped by TVM */
+	if((((inst>>25) & 0x7f) == 0b0001001)
+	&&((inst & 0x7fff) == 0b1110011))
+	{
+		// printm("here5 %d\r\n",((inst>>7) & 0x1f));
+		asm volatile ("sfence.vma");
+		csr_write(CSR_MEPC, mepc + 4);
+		regs->mepc = csr_read(CSR_MEPC);
+		return 0 ;
+	}
+	// End of the TVM trap handler 
 
 	if (unlikely((insn & 3) != 3)) {
 		if (insn == 0) {
diff --git a/lib/sbi/sbi_init.c b/lib/sbi/sbi_init.c
index a7fb848..3dab863 100644
--- a/lib/sbi/sbi_init.c
+++ b/lib/sbi/sbi_init.c
@@ -21,6 +21,9 @@
 #include <sbi/sbi_string.h>
 #include <sbi/sbi_timer.h>
 #include <sbi/sbi_tlb.h>
+#include <sbi/sbi_pmp.h>
+#include <sbi/sbi_tvm.h>
+#include <sbi/sbi_ipi_destroy_enclave.h>
 #include <sbi/sbi_version.h>
 
 #define BANNER                                              \
@@ -194,6 +197,18 @@ static void __noreturn init_coldboot(struct sbi_scratch *scratch, u32 hartid)
 	rc = sbi_tlb_init(scratch, TRUE);
 	if (rc)
 		sbi_hart_hang();
+	
+	rc = sbi_pmp_init(scratch, TRUE);
+	if (rc)
+		sbi_hart_hang();
+
+	rc = sbi_tvm_init(scratch, TRUE);
+	if (rc)
+		sbi_hart_hang();
+
+	rc = sbi_ipi_destroy_enclave_init(scratch, TRUE);
+	if (rc)
+		sbi_hart_hang();
 
 	rc = sbi_timer_init(scratch, TRUE);
 	if (rc)
@@ -254,6 +269,18 @@ static void __noreturn init_warmboot(struct sbi_scratch *scratch, u32 hartid)
 	rc = sbi_tlb_init(scratch, FALSE);
 	if (rc)
 		sbi_hart_hang();
+	
+	rc = sbi_pmp_init(scratch, FALSE);
+	if (rc)
+		sbi_hart_hang();
+
+	rc = sbi_tvm_init(scratch, FALSE);
+	if (rc)
+		sbi_hart_hang();
+
+	rc = sbi_ipi_destroy_enclave_init(scratch, FALSE);
+	if (rc)
+		sbi_hart_hang();
 
 	rc = sbi_timer_init(scratch, FALSE);
 	if (rc)
diff --git a/lib/sbi/sbi_ipi.c b/lib/sbi/sbi_ipi.c
index a27dea0..c9acc6d 100644
--- a/lib/sbi/sbi_ipi.c
+++ b/lib/sbi/sbi_ipi.c
@@ -18,6 +18,7 @@
 #include <sbi/sbi_init.h>
 #include <sbi/sbi_ipi.h>
 #include <sbi/sbi_platform.h>
+#include <sbi/sbi_pmp.h>
 
 struct sbi_ipi_data {
 	unsigned long ipi_type;
@@ -199,6 +200,35 @@ skip:
 	};
 }
 
+void sbi_ipi_process_in_enclave(struct sbi_trap_regs* regs)
+{
+	unsigned long ipi_type;
+	unsigned int ipi_event;
+	const struct sbi_ipi_event_ops *ipi_ops;
+	struct sbi_scratch *scratch = sbi_scratch_thishart_ptr();
+	const struct sbi_platform *plat = sbi_platform_ptr(scratch);
+	struct sbi_ipi_data *ipi_data =
+			sbi_scratch_offset_ptr(scratch, ipi_data_off);
+
+	u32 hartid = current_hartid();
+	sbi_platform_ipi_clear(plat, hartid);
+
+	ipi_type = atomic_raw_xchg_ulong(&ipi_data->ipi_type, 0);
+	ipi_event = 0;
+	while (ipi_type) {
+		if (!(ipi_type & 1UL))
+			goto skip;
+
+		ipi_ops = ipi_ops_array[ipi_event];
+		if (ipi_ops && ipi_ops->e_process)
+			ipi_ops->e_process(scratch, regs);
+
+skip:
+		ipi_type = ipi_type >> 1;
+		ipi_event++;
+	};
+}
+
 int sbi_ipi_init(struct sbi_scratch *scratch, bool cold_boot)
 {
 	int ret;
diff --git a/lib/sbi/sbi_ipi_destroy_enclave.c b/lib/sbi/sbi_ipi_destroy_enclave.c
new file mode 100644
index 0000000..8a748b3
--- /dev/null
+++ b/lib/sbi/sbi_ipi_destroy_enclave.c
@@ -0,0 +1,142 @@
+#include "sbi/sbi_ipi_destroy_enclave.h"
+#include "sm/ipi.h"
+#include <sbi/riscv_asm.h>
+#include <sbi/riscv_atomic.h>
+#include <sbi/riscv_barrier.h>
+#include <sbi/sbi_error.h>
+#include <sbi/sbi_fifo.h>
+#include <sbi/sbi_ipi_destroy_enclave.h>
+#include <sbi/sbi_hart.h>
+#include <sbi/sbi_ipi.h>
+#include <sbi/sbi_scratch.h>
+#include <sbi/sbi_tlb.h>
+#include <sbi/sbi_hfence.h>
+#include <sbi/sbi_string.h>
+#include <sbi/sbi_console.h>
+#include <sbi/sbi_platform.h>
+#include <sbi/sbi_hartmask.h>
+#include <sm/enclave.h>
+
+static unsigned long ipi_destroy_enclave_data_offset;
+static unsigned long ipi_destroy_enclave_sync_offset;
+
+#define SBI_IPI_DESTROY_ENCLAVE_DATA_INIT(__p, __host_ptbr, __enclave_id, __src) \
+do { \
+	(__p)->host_ptbr = (__host_ptbr); \
+	(__p)->enclave_id = (__enclave_id); \
+	SBI_HARTMASK_INIT_EXCEPT(&(__p)->smask, (__src)); \
+} while (0)
+
+void set_ipi_destroy_enclave_and_sync(u32 remote_hart, ulong host_ptbr, int enclave_id)
+{
+  struct ipi_destroy_enclave_data_t ipi_destroy_enclave_data;
+  u32 source_hart = current_hartid();
+
+  //sync all other harts
+  SBI_IPI_DESTROY_ENCLAVE_DATA_INIT(&ipi_destroy_enclave_data, host_ptbr, enclave_id, source_hart);
+  sbi_send_ipi_destroy_enclave((0x1<<remote_hart), 0, &ipi_destroy_enclave_data);
+  return;
+}
+
+static void sbi_eprocess_ipi_destroy_enclave(struct sbi_scratch *scratch, struct sbi_trap_regs* regs)
+{
+	struct ipi_destroy_enclave_data_t *data = sbi_scratch_offset_ptr(scratch, ipi_destroy_enclave_data_offset);
+	struct sbi_scratch *rscratch = NULL;
+	u32 rhartid;
+	unsigned long *ipi_destroy_enclave_sync = NULL;
+	ipi_destroy_enclave((uintptr_t *)regs, data->host_ptbr, data->enclave_id);
+	//sync
+	sbi_hartmask_for_each_hart(rhartid, &data->smask) {
+		rscratch = sbi_hartid_to_scratch(rhartid);
+		if (!rscratch)
+			continue;
+		ipi_destroy_enclave_sync = sbi_scratch_offset_ptr(rscratch, ipi_destroy_enclave_sync_offset);
+		while (atomic_raw_xchg_ulong(ipi_destroy_enclave_sync, 1));
+	}
+}
+
+static void sbi_process_ipi_destroy_enclave(struct sbi_scratch *scratch)
+{
+	sbi_bug("M mode: sbi_process_ipi_destroy_enclave error\n");
+	return;
+}
+
+static int sbi_update_ipi_destroy_enclave(struct sbi_scratch *scratch,
+			  struct sbi_scratch *remote_scratch,
+			  u32 remote_hartid, void *data)
+{
+	struct ipi_destroy_enclave_data_t *ipi_destroy_enclave_data = NULL;
+	u32 curr_hartid = current_hartid();
+
+	if (remote_hartid == curr_hartid) {
+		sbi_bug("M mode: sbi_update_ipi_destroy_enclave: remote_hartid is current hartid\n");
+		return -1;
+	}
+
+	ipi_destroy_enclave_data = sbi_scratch_offset_ptr(remote_scratch, ipi_destroy_enclave_data_offset);
+	//update the remote hart ipi_destroy_enclave data
+	sbi_memcpy(ipi_destroy_enclave_data, data, sizeof(struct ipi_destroy_enclave_data_t));
+
+	return 0;
+}
+
+static void sbi_ipi_destroy_enclave_sync(struct sbi_scratch *scratch)
+{
+	unsigned long *ipi_destroy_enclave_sync =
+			sbi_scratch_offset_ptr(scratch, ipi_destroy_enclave_sync_offset);
+	//wait the remote hart process the ipi_destroy_enclave signal
+	while (!atomic_raw_xchg_ulong(ipi_destroy_enclave_sync, 0));
+	return;
+}
+
+static struct sbi_ipi_event_ops ipi_destroy_enclave_ops = {
+	.name = "IPI_DESTROY_ENCLAVE",
+	.update = sbi_update_ipi_destroy_enclave,
+	.sync = sbi_ipi_destroy_enclave_sync,
+	.process = sbi_process_ipi_destroy_enclave,
+	.e_process = sbi_eprocess_ipi_destroy_enclave,
+};
+
+static u32 ipi_destroy_enclave_event = SBI_IPI_EVENT_MAX;
+
+int sbi_send_ipi_destroy_enclave(ulong hmask, ulong hbase, struct ipi_destroy_enclave_data_t* ipi_destroy_enclave_data)
+{
+	return sbi_ipi_send_many(hmask, hbase, ipi_destroy_enclave_event, ipi_destroy_enclave_data);
+}
+
+int sbi_ipi_destroy_enclave_init(struct sbi_scratch *scratch, bool cold_boot)
+{
+	int ret;
+	struct ipi_destroy_enclave_data_t *ipi_destroy_enclave_data;
+	unsigned long *ipi_destroy_enclave_sync;
+	if (cold_boot) {
+        // Define the ipi_destroy_enclave data offset in the scratch
+		ipi_destroy_enclave_data_offset = sbi_scratch_alloc_offset(sizeof(*ipi_destroy_enclave_data),
+							    "IPI_DESTROY_ENCLAVE_DATA");
+		if (!ipi_destroy_enclave_data_offset)
+			return SBI_ENOMEM;
+
+		ipi_destroy_enclave_sync_offset = sbi_scratch_alloc_offset(sizeof(*ipi_destroy_enclave_sync),
+							    "IPI_DESTROY_ENCLAVE_SYNC");
+		if (!ipi_destroy_enclave_sync_offset)
+			return SBI_ENOMEM;
+
+		ipi_destroy_enclave_data = sbi_scratch_offset_ptr(scratch,
+						       ipi_destroy_enclave_data_offset);
+
+		ipi_destroy_enclave_sync = sbi_scratch_offset_ptr(scratch,
+						       ipi_destroy_enclave_sync_offset);
+
+		*ipi_destroy_enclave_sync = 0;
+
+		ret = sbi_ipi_event_create(&ipi_destroy_enclave_ops);
+		if (ret < 0) {
+			sbi_bug("M mode: sbi_ipi_destroy_enclave_init: init5\n");
+			sbi_scratch_free_offset(ipi_destroy_enclave_data_offset);
+			return ret;
+		}
+		ipi_destroy_enclave_event = ret;
+	} else {
+	}
+	return 0;
+}
\ No newline at end of file
diff --git a/lib/sbi/sbi_pmp.c b/lib/sbi/sbi_pmp.c
new file mode 100644
index 0000000..e121d0e
--- /dev/null
+++ b/lib/sbi/sbi_pmp.c
@@ -0,0 +1,123 @@
+#include "sbi/sbi_pmp.h"
+#include "sm/ipi.h"
+#include <sbi/riscv_asm.h>
+#include <sbi/riscv_atomic.h>
+#include <sbi/riscv_barrier.h>
+#include <sbi/sbi_error.h>
+#include <sbi/sbi_fifo.h>
+#include <sbi/sbi_hart.h>
+#include <sbi/sbi_ipi.h>
+#include <sbi/sbi_scratch.h>
+#include <sbi/sbi_tlb.h>
+#include <sbi/sbi_hfence.h>
+#include <sbi/sbi_string.h>
+#include <sbi/sbi_console.h>
+#include <sbi/sbi_platform.h>
+#include <sbi/sbi_hartmask.h>
+
+static unsigned long pmp_data_offset;
+static unsigned long pmp_sync_offset;
+
+static void sbi_process_pmp(struct sbi_scratch *scratch)
+{
+	struct pmp_data_t *data = sbi_scratch_offset_ptr(scratch, pmp_data_offset);
+	struct pmp_config_t pmp_config = *(struct pmp_config_t*)(data);
+	struct sbi_scratch *rscratch = NULL;
+	u32 rhartid;
+	unsigned long *pmp_sync = NULL;
+	int pmp_idx = data->pmp_idx_arg;
+	set_pmp(pmp_idx, pmp_config);
+	
+	//sync
+	sbi_hartmask_for_each_hart(rhartid, &data->smask) {
+		rscratch = sbi_hartid_to_scratch(rhartid);
+		if (!rscratch)
+			continue;
+		pmp_sync = sbi_scratch_offset_ptr(rscratch, pmp_sync_offset);
+		while (atomic_raw_xchg_ulong(pmp_sync, 1));
+	}
+}
+
+static int sbi_update_pmp(struct sbi_scratch *scratch,
+			  struct sbi_scratch *remote_scratch,
+			  u32 remote_hartid, void *data)
+{
+	struct pmp_data_t *pmp_data = NULL;
+	int pmp_idx = 0;
+	u32 curr_hartid = current_hartid();
+
+	if (remote_hartid == curr_hartid) {
+		//update the pmp register locally
+		struct pmp_config_t pmp_config = *(struct pmp_config_t*)(data);
+		pmp_idx = pmp_data->pmp_idx_arg;
+		set_pmp(pmp_idx, pmp_config);
+		return -1;
+	}
+
+	pmp_data = sbi_scratch_offset_ptr(remote_scratch, pmp_data_offset);
+	//update the remote hart pmp data
+	sbi_memcpy(pmp_data, data, sizeof(struct pmp_data_t));
+
+	return 0;
+}
+
+static void sbi_pmp_sync(struct sbi_scratch *scratch)
+{
+	unsigned long *pmp_sync =
+			sbi_scratch_offset_ptr(scratch, pmp_sync_offset);
+	//wait the remote hart process the pmp signal
+	while (!atomic_raw_xchg_ulong(pmp_sync, 0));
+	return;
+}
+
+static struct sbi_ipi_event_ops pmp_ops = {
+	.name = "IPI_PMP",
+	.update = sbi_update_pmp,
+	.sync = sbi_pmp_sync,
+	.process = sbi_process_pmp,
+};
+
+static u32 pmp_event = SBI_IPI_EVENT_MAX;
+
+int sbi_send_pmp(ulong hmask, ulong hbase, struct pmp_data_t* pmp_data)
+{
+	return sbi_ipi_send_many(hmask, hbase, pmp_event, pmp_data);
+}
+
+int sbi_pmp_init(struct sbi_scratch *scratch, bool cold_boot)
+{
+	int ret;
+	struct pmp_data_t *pmpdata;
+	unsigned long *pmp_sync;
+
+	if (cold_boot) {
+        //Define the pmp data offset in the scratch
+		pmp_data_offset = sbi_scratch_alloc_offset(sizeof(*pmpdata),
+							    "PMP_DATA");
+		if (!pmp_data_offset)
+			return SBI_ENOMEM;
+
+		pmp_sync_offset = sbi_scratch_alloc_offset(sizeof(*pmp_sync),
+							    "PMP_SYNC");
+		if (!pmp_sync_offset)
+			return SBI_ENOMEM;
+
+		pmpdata = sbi_scratch_offset_ptr(scratch,
+						       pmp_data_offset);
+
+		pmp_sync = sbi_scratch_offset_ptr(scratch,
+						       pmp_sync_offset);
+
+		*pmp_sync = 0;
+
+		ret = sbi_ipi_event_create(&pmp_ops);
+		if (ret < 0) {
+			sbi_scratch_free_offset(pmp_data_offset);
+			return ret;
+		}
+		pmp_event = ret;
+	} else {
+	}
+
+	return 0;
+}
\ No newline at end of file
diff --git a/lib/sbi/sbi_trap.c b/lib/sbi/sbi_trap.c
index 930119d..838e537 100644
--- a/lib/sbi/sbi_trap.c
+++ b/lib/sbi/sbi_trap.c
@@ -18,8 +18,13 @@
 #include <sbi/sbi_misaligned_ldst.h>
 #include <sbi/sbi_timer.h>
 #include <sbi/sbi_trap.h>
+#include <sm/sm.h>
+#include <sm/enclave.h>
+#define read_csr(reg) ({ unsigned long __tmp; \
+  asm volatile ("csrr %0, " #reg : "=r"(__tmp)); \
+  __tmp; })
 
-static void __noreturn sbi_trap_error(const char *msg, int rc,
+static void sbi_trap_error(const char *msg, int rc,
 				      ulong mcause, ulong mtval, ulong mtval2,
 				      ulong mtinst, struct sbi_trap_regs *regs)
 {
@@ -68,7 +73,15 @@ static void __noreturn sbi_trap_error(const char *msg, int rc,
 	sbi_printf("%s: hart%d: %s=0x%" PRILX "\n", __func__, hartid, "t6",
 		   regs->t6);
 
+	if(check_in_enclave_world() == 0)
+	{
+		destroy_enclave((uintptr_t *)regs, get_curr_enclave_id());
+		regs->mepc = csr_read(CSR_MEPC);
+		regs->mstatus = csr_read(CSR_MSTATUS);
+		return;
+	}
 	sbi_hart_hang();
+	return;
 }
 
 /**
@@ -82,6 +95,10 @@ static void __noreturn sbi_trap_error(const char *msg, int rc,
 int sbi_trap_redirect(struct sbi_trap_regs *regs,
 		      struct sbi_trap_info *trap)
 {
+	sbi_printf("SBI: sbi_trap_redirect casuse %lx\n", trap->cause);
+	return 1;
+
+
 	ulong hstatus, vsstatus, prev_mode;
 #if __riscv_xlen == 32
 	bool prev_virt = (regs->mstatusH & MSTATUSH_MPV) ? TRUE : FALSE;
@@ -193,6 +210,18 @@ int sbi_trap_redirect(struct sbi_trap_regs *regs,
 	return 0;
 }
 
+void handle_timer_irq(struct sbi_trap_regs *regs, uintptr_t mcause, uintptr_t mepc)
+{
+	
+	if(check_in_enclave_world() < 0)
+	{
+		csr_read_clear(CSR_MIE, MIP_MTIP);
+		csr_read_set(CSR_MIP, MIP_STIP);
+		return;
+	}
+	sm_do_timer_irq((uintptr_t *)regs, mcause, mepc);
+}
+
 /**
  * Handle trap/interrupt
  *
@@ -214,9 +243,10 @@ void sbi_trap_handler(struct sbi_trap_regs *regs)
 	int rc = SBI_ENOTSUPP;
 	const char *msg = "trap handler failed";
 	ulong mcause = csr_read(CSR_MCAUSE);
+	ulong mepc = csr_read(CSR_MEPC);
 	ulong mtval = csr_read(CSR_MTVAL), mtval2 = 0, mtinst = 0;
 	struct sbi_trap_info trap;
-
+	
 	if (misa_extension('H')) {
 		mtval2 = csr_read(CSR_MTVAL2);
 		mtinst = csr_read(CSR_MTINST);
@@ -226,10 +256,27 @@ void sbi_trap_handler(struct sbi_trap_regs *regs)
 		mcause &= ~(1UL << (__riscv_xlen - 1));
 		switch (mcause) {
 		case IRQ_M_TIMER:
-			sbi_timer_process();
+			if (check_in_enclave_world() == 0)
+			{
+				handle_timer_irq(regs, mcause, mepc);
+				regs->mepc = csr_read(CSR_MEPC);
+				regs->mstatus = csr_read(CSR_MSTATUS);
+			}
+			else
+				sbi_timer_process();
 			break;
 		case IRQ_M_SOFT:
-			sbi_ipi_process();
+			if(check_in_enclave_world() < 0)
+			{
+				sbi_ipi_process();
+			}
+			else
+			{
+				//TODO: just consider the ipi for destroying the enclave
+				sbi_ipi_process_in_enclave(regs);
+				regs->mepc = csr_read(CSR_MEPC);
+				regs->mstatus = csr_read(CSR_MSTATUS);
+			}
 			break;
 		default:
 			msg = "unhandled external interrupt";
@@ -251,10 +298,17 @@ void sbi_trap_handler(struct sbi_trap_regs *regs)
 		rc  = sbi_misaligned_store_handler(mtval, mtval2, mtinst, regs);
 		msg = "misaligned store handler failed";
 		break;
+	case CAUSE_USER_ECALL:
+		rc  = enclave_call_trap(regs);
+		msg = "ecall handler failed";
+		break;
 	case CAUSE_SUPERVISOR_ECALL:
+		rc  = 1;
+		msg = "supervisor ecall trap failed";
+		break;
 	case CAUSE_HYPERVISOR_ECALL:
 		rc  = sbi_ecall_handler(regs);
-		msg = "ecall handler failed";
+		msg = "enclave call trap failed";
 		break;
 	default:
 		/* If the trap came from S or U mode, redirect it there */
@@ -266,7 +320,7 @@ void sbi_trap_handler(struct sbi_trap_regs *regs)
 		rc = sbi_trap_redirect(regs, &trap);
 		break;
 	};
-
+	
 trap_error:
 	if (rc)
 		sbi_trap_error(msg, rc, mcause, mtval, mtval2, mtinst, regs);
diff --git a/lib/sbi/sbi_tvm.c b/lib/sbi/sbi_tvm.c
new file mode 100644
index 0000000..41ef614
--- /dev/null
+++ b/lib/sbi/sbi_tvm.c
@@ -0,0 +1,141 @@
+#include "sbi/sbi_tvm.h"
+#include "sm/ipi.h"
+#include <sbi/riscv_asm.h>
+#include <sbi/riscv_atomic.h>
+#include <sbi/riscv_barrier.h>
+#include <sbi/sbi_error.h>
+#include <sbi/sbi_fifo.h>
+#include <sbi/sbi_tvm.h>
+#include <sbi/sbi_hart.h>
+#include <sbi/sbi_ipi.h>
+#include <sbi/sbi_scratch.h>
+#include <sbi/sbi_tlb.h>
+#include <sbi/sbi_hfence.h>
+#include <sbi/sbi_string.h>
+#include <sbi/sbi_console.h>
+#include <sbi/sbi_platform.h>
+#include <sbi/sbi_hartmask.h>
+
+static unsigned long tvm_data_offset;
+static unsigned long tvm_sync_offset;
+
+#define SBI_TVM_DATA_INIT(__p, __src) \
+do { \
+	SBI_HARTMASK_INIT_EXCEPT(&(__p)->smask, (__src)); \
+} while (0)
+
+void set_tvm_and_sync()
+{
+  struct tvm_data_t tvm_data;
+  u32 source_hart = current_hartid();
+
+  //sync all other harts
+  SBI_TVM_DATA_INIT(&tvm_data,  source_hart);
+  sbi_send_tvm(0xFFFFFFFF&(~(1<<source_hart)), 0, &tvm_data);
+  return;
+}
+
+static void sbi_process_tvm(struct sbi_scratch *scratch)
+{
+	struct tvm_data_t *data = sbi_scratch_offset_ptr(scratch, tvm_data_offset);
+	struct sbi_scratch *rscratch = NULL;
+	u32 rhartid;
+	unsigned long *tvm_sync = NULL;
+	uintptr_t mstatus = csr_read(CSR_MSTATUS);
+	/* Enable TVM here */
+	mstatus = INSERT_FIELD(mstatus, MSTATUS_TVM, 1);
+	csr_write(CSR_MSTATUS, mstatus);
+	
+	//sync
+	sbi_hartmask_for_each_hart(rhartid, &data->smask) {
+		rscratch = sbi_hartid_to_scratch(rhartid);
+		if (!rscratch)
+			continue;
+		tvm_sync = sbi_scratch_offset_ptr(rscratch, tvm_sync_offset);
+		while (atomic_raw_xchg_ulong(tvm_sync, 1));
+	}
+}
+
+static int sbi_update_tvm(struct sbi_scratch *scratch,
+			  struct sbi_scratch *remote_scratch,
+			  u32 remote_hartid, void *data)
+{
+	struct tvm_data_t *tvm_data = NULL;
+	u32 curr_hartid = current_hartid();
+
+	if (remote_hartid == curr_hartid) {
+		// update the tvm register locally
+		uintptr_t mstatus = csr_read(CSR_MSTATUS);
+		/* Enable TVM here */
+		mstatus = INSERT_FIELD(mstatus, MSTATUS_TVM, 1);
+		csr_write(CSR_MSTATUS, mstatus);
+		return -1;
+	}
+
+	tvm_data = sbi_scratch_offset_ptr(remote_scratch, tvm_data_offset);
+	//update the remote hart tvm data
+	sbi_memcpy(tvm_data, data, sizeof(struct tvm_data_t));
+
+	return 0;
+}
+
+static void sbi_tvm_sync(struct sbi_scratch *scratch)
+{
+	unsigned long *tvm_sync =
+			sbi_scratch_offset_ptr(scratch, tvm_sync_offset);
+	//wait the remote hart process the tvm signal
+	while (!atomic_raw_xchg_ulong(tvm_sync, 0));
+	return;
+}
+
+static struct sbi_ipi_event_ops tvm_ops = {
+	.name = "IPI_TVM",
+	.update = sbi_update_tvm,
+	.sync = sbi_tvm_sync,
+	.process = sbi_process_tvm,
+};
+
+static u32 tvm_event = SBI_IPI_EVENT_MAX;
+
+int sbi_send_tvm(ulong hmask, ulong hbase, struct tvm_data_t* tvm_data)
+{
+	return sbi_ipi_send_many(hmask, hbase, tvm_event, tvm_data);
+}
+
+int sbi_tvm_init(struct sbi_scratch *scratch, bool cold_boot)
+{
+	int ret;
+	struct tvm_data_t *tvmdata;
+	unsigned long *tvm_sync;
+
+	if (cold_boot) {
+        //Define the tvm data offset in the scratch
+		tvm_data_offset = sbi_scratch_alloc_offset(sizeof(*tvmdata),
+							    "TVM_DATA");
+		if (!tvm_data_offset)
+			return SBI_ENOMEM;
+
+		tvm_sync_offset = sbi_scratch_alloc_offset(sizeof(*tvm_sync),
+							    "TVM_SYNC");
+		if (!tvm_sync_offset)
+			return SBI_ENOMEM;
+
+		tvmdata = sbi_scratch_offset_ptr(scratch,
+						       tvm_data_offset);
+
+		tvm_sync = sbi_scratch_offset_ptr(scratch,
+						       tvm_sync_offset);
+
+		*tvm_sync = 0;
+
+		ret = sbi_ipi_event_create(&tvm_ops);
+		if (ret < 0) {
+			sbi_scratch_free_offset(tvm_data_offset);
+			return ret;
+		}
+		tvm_event = ret;
+	} else {
+	}
+
+	return 0;
+}
\ No newline at end of file
diff --git a/lib/sbi/sm/.gitignore b/lib/sbi/sm/.gitignore
new file mode 100644
index 0000000..751553b
--- /dev/null
+++ b/lib/sbi/sm/.gitignore
@@ -0,0 +1 @@
+*.bak
diff --git a/lib/sbi/sm/attest.c b/lib/sbi/sm/attest.c
new file mode 100644
index 0000000..6dbe703
--- /dev/null
+++ b/lib/sbi/sm/attest.c
@@ -0,0 +1,124 @@
+#include "sm/attest.h"
+#include "sm/gm/sm3.h"
+#include "sm/gm/sm2.h"
+#include "sbi/riscv_encoding.h"
+
+static int hash_enclave_mem(struct sm3_context *hash_ctx, pte_t* ptes, int level, uintptr_t va, int hash_va)
+{
+  uintptr_t pte_per_page = RISCV_PGSIZE/sizeof(pte_t);
+  pte_t *pte;
+  uintptr_t i = 0;
+  int hash_curr_va = hash_va;
+
+  //should never happen
+  if(level <= 0)
+    return 1;
+
+  for(pte = ptes, i = 0; i < pte_per_page; pte += 1, i += 1)
+  {
+    if(!(*pte & PTE_V))
+    {
+      hash_curr_va = 1;
+      continue;
+    }
+
+    uintptr_t curr_va = 0;
+    if(level == ((VA_BITS - RISCV_PGSHIFT) / RISCV_PGLEVEL_BITS))
+      curr_va = (uintptr_t)(-1UL << VA_BITS) + (i << (VA_BITS - RISCV_PGLEVEL_BITS));
+    else
+      curr_va = va + (i << ((level-1) * RISCV_PGLEVEL_BITS + RISCV_PGSHIFT));
+    uintptr_t pa = (*pte >> PTE_PPN_SHIFT) << RISCV_PGSHIFT;
+
+    //found leaf pte
+    if((*pte & PTE_R) || (*pte & PTE_X))
+    {
+      if(hash_curr_va)
+      {
+        sm3_update(hash_ctx, (unsigned char*)&curr_va, sizeof(uintptr_t));
+        //update hash with  page attribution
+        sm3_update(hash_ctx, (unsigned char*)pte+7, 1);
+        hash_curr_va = 0;
+      }
+
+      //4K page
+      if(level == 1)
+      {
+        sm3_update(hash_ctx, (void*)pa, 1 << RISCV_PGSHIFT);
+      }
+      //2M page
+      else if(level == 2)
+      {
+        sm3_update(hash_ctx, (void*)pa, 1 << (RISCV_PGSHIFT + RISCV_PGLEVEL_BITS));
+      }
+    }
+    else
+    {
+      hash_curr_va = hash_enclave_mem(hash_ctx, (pte_t*)pa, level - 1, curr_va, hash_curr_va);
+    }
+  }
+
+  return hash_curr_va;
+}
+
+void hash_enclave(struct enclave_t *enclave, void* hash, uintptr_t nonce_arg)
+{
+  struct sm3_context hash_ctx;
+  uintptr_t nonce = nonce_arg;
+
+  sm3_init(&hash_ctx);
+
+  sm3_update(&hash_ctx, (unsigned char*)(&(enclave->entry_point)), sizeof(unsigned long));
+
+  hash_enclave_mem(&hash_ctx, (pte_t*)(enclave->thread_context.encl_ptbr << RISCV_PGSHIFT),
+      (VA_BITS - RISCV_PGSHIFT) / RISCV_PGLEVEL_BITS, 0, 1);
+
+  sm3_update(&hash_ctx, (unsigned char*)(&nonce), sizeof(uintptr_t));
+
+  sm3_final(&hash_ctx, hash);
+}
+
+void hash_shadow_enclave(struct shadow_enclave_t *enclave, void* hash, uintptr_t nonce_arg)
+{
+  struct sm3_context hash_ctx;
+  uintptr_t nonce = nonce_arg;
+
+  sm3_init(&hash_ctx);
+
+  sm3_update(&hash_ctx, (unsigned char*)(&(enclave->entry_point)), sizeof(unsigned long));
+
+  hash_enclave_mem(&hash_ctx, (pte_t*)(enclave->thread_context.encl_ptbr << RISCV_PGSHIFT),
+      (VA_BITS - RISCV_PGSHIFT) / RISCV_PGLEVEL_BITS, 0, 1);
+
+  sm3_update(&hash_ctx, (unsigned char*)(&nonce), sizeof(uintptr_t));
+
+  sm3_final(&hash_ctx, hash);
+}
+
+void update_hash_shadow_enclave(struct shadow_enclave_t *enclave, void* hash, uintptr_t nonce_arg)
+{
+  struct sm3_context hash_ctx;
+  uintptr_t nonce = nonce_arg;
+
+  sm3_init(&hash_ctx);
+
+  sm3_update(&hash_ctx, (unsigned char*)(hash), HASH_SIZE);
+
+  sm3_update(&hash_ctx, (unsigned char*)(&nonce), sizeof(uintptr_t));
+
+  sm3_final(&hash_ctx, hash);
+}
+void sign_enclave(void* signature_arg, void* hash)
+{
+  struct signature_t *signature = (struct signature_t*)signature_arg;
+  sm2_sign((void*)(signature->r), (void*)(signature->s), (void*)SM_PRI_KEY, hash);
+}
+
+int verify_enclave(void* signature_arg, void* hash)
+{
+  int ret = 0;
+  struct signature_t *signature = (struct signature_t*)signature_arg;
+
+  ret = sm2_verify((void*)SM_PUB_KEY, hash, (void*)(signature->r), (void*)(signature->s));
+
+  return ret;
+}
diff --git a/lib/sbi/sm/enclave.c b/lib/sbi/sm/enclave.c
new file mode 100644
index 0000000..01940e0
--- /dev/null
+++ b/lib/sbi/sm/enclave.c
@@ -0,0 +1,2725 @@
+#include "sbi/riscv_encoding.h"
+#include "sbi/sbi_math.h"
+#include "sbi/riscv_locks.h"
+#include "sbi/sbi_bitops.h" 
+#include "sbi/sbi_ipi_destroy_enclave.h"
+#include "sbi/sbi_console.h"
+#include "sm/enclave.h"
+#include "sm/enclave_vm.h"
+#include "sm/enclave_mm.h"
+#include "sm/sm.h"
+#include "sm/platform/pt_area/platform_thread.h"
+#include "sm/ipi.h"
+#include "sm/relay_page.h"
+#include "sm/attest.h"
+#include <sbi/sbi_tlb.h>
+
+int eapp_args = 0;
+
+static struct cpu_state_t cpus[MAX_HARTS] = {{0,}, };
+
+//whether cpu is in enclave-mode
+int cpu_in_enclave(int i)
+{
+  return cpus[i].in_enclave;
+}
+
+//the eid of current cpu (if it is in enclave mode)
+int cpu_eid(int i)
+{
+  return cpus[i].eid;
+}
+
+//spinlock
+static spinlock_t enclave_metadata_lock = SPINLOCK_INIT;
+void acquire_enclave_metadata_lock()
+{
+  spin_lock(&enclave_metadata_lock);
+}
+void release_enclave_metadata_lock()
+{
+  spin_unlock(&enclave_metadata_lock);
+}
+
+//enclave metadata
+struct link_mem_t* enclave_metadata_head = NULL;
+struct link_mem_t* enclave_metadata_tail = NULL;
+
+struct link_mem_t* shadow_enclave_metadata_head = NULL;
+struct link_mem_t* shadow_enclave_metadata_tail = NULL;
+
+struct link_mem_t* relay_page_head = NULL;
+struct link_mem_t* relay_page_tail = NULL;
+
+/**
+ * \brief Compare the enclave name.
+ * 
+ * \param name1 The given enclave name1. 
+ * \param name2 The given enclave name2. 
+ */
+static int enclave_name_cmp(char* name1, char* name2)
+{
+  for(int i=0; i<NAME_LEN; ++i)
+  {
+    if(name1[i] != name2[i])
+    {
+      return 1;
+    }
+    if(name1[i] == 0)
+    {
+      return 0;
+    }
+  }
+  return 0;
+}
+
+//copy data from host
+uintptr_t copy_from_host(void* dest, void* src, size_t size)
+{
+  sbi_memcpy(dest, src, size);
+  return 0;
+}
+
+// copy data to host
+uintptr_t copy_to_host(void* dest, void* src, size_t size)
+{
+  sbi_memcpy(dest, src, size);
+  return 0;
+}
+
+// Copy a word value to the host 
+int copy_word_to_host(unsigned int* ptr, uintptr_t value)
+{
+  *ptr = value;
+  return 0;
+}
+
+// Copy double word to the host
+int copy_dword_to_host(uintptr_t* ptr, uintptr_t value)
+{
+  *ptr = value;
+  return 0;
+}
+
+// Should only be called after acquire enclave_metadata_lock
+static void enter_enclave_world(int eid)
+{
+  cpus[csr_read(CSR_MHARTID)].in_enclave = ENCLAVE_MODE;
+  cpus[csr_read(CSR_MHARTID)].eid = eid;
+
+  platform_enter_enclave_world();
+}
+
+// Get the current enclave id
+int get_curr_enclave_id()
+{
+  return cpus[csr_read(CSR_MHARTID)].eid;
+}
+
+// Should only be called after acquire enclave_metadata_lock
+static void exit_enclave_world()
+{
+  cpus[csr_read(CSR_MHARTID)].in_enclave = NORMAL_MODE;
+  cpus[csr_read(CSR_MHARTID)].eid = -1;
+
+  platform_exit_enclave_world();
+}
+
+// check whether we are in enclave-world through in_enclave state
+int check_in_enclave_world()
+{
+  if(!(cpus[csr_read(CSR_MHARTID)].in_enclave))
+    return -1;
+
+  if(platform_check_in_enclave_world() < 0)
+    return -1;
+
+  return 0;
+}
+
+// Invoke the platform-specific authentication
+static int check_enclave_authentication()
+{
+  if(platform_check_enclave_authentication() != 0)
+    return -1;
+
+  return 0;
+}
+
+// Wrapper of the platform-specific switch func
+static void switch_to_enclave_ptbr(struct thread_state_t* thread, uintptr_t ptbr)
+{
+  platform_switch_to_enclave_ptbr(thread, ptbr);
+}
+
+// Wrapper of the platform-specific switch func
+static void switch_to_host_ptbr(struct thread_state_t* thread, uintptr_t ptbr)
+{
+  platform_switch_to_host_ptbr(thread, ptbr);
+}
+
+/**
+ * \brief it creates a new link_mem_t list, with the total size (mem_size), each 
+ * 	entry is slab_size.
+ * 
+ * \param mem_size Init link memory size.
+ * \param slab_size The slab size for the link memmory 
+ */
+struct link_mem_t* init_mem_link(unsigned long mem_size, unsigned long slab_size)
+{
+  struct link_mem_t* head;
+  unsigned long resp_size = 0;
+  head = (struct link_mem_t*)mm_alloc(mem_size, &resp_size);
+  
+  if(head == NULL)
+    return NULL;
+  else
+    sbi_memset((void*)head, 0, resp_size);
+
+  if(resp_size <= sizeof(struct link_mem_t) + slab_size)
+  {
+    mm_free(head, resp_size);
+    sbi_debug("M mode: init_mem_link: The monitor has not reserved enough secure memory\n");
+    return NULL;
+  }
+
+  head->mem_size = resp_size;
+  head->slab_size = slab_size;
+  head->slab_num = (resp_size - sizeof(struct link_mem_t)) / slab_size;
+  void* align_addr = (char*)head + sizeof(struct link_mem_t);
+  head->addr = (char*)size_up_align((unsigned long)align_addr, slab_size);
+  head->next_link_mem = NULL;
+
+  return head;
+}
+
+/**
+ * \brief Create a new link_mem_t entry and append it into tail.
+ * 
+ * \param tail Return value, The tail of the link memory.
+ */
+struct link_mem_t* add_link_mem(struct link_mem_t** tail)
+{
+  struct link_mem_t* new_link_mem;
+  unsigned long resp_size = 0;
+
+  new_link_mem = (struct link_mem_t*)mm_alloc((*tail)->mem_size, &resp_size);
+
+  if (new_link_mem == NULL)
+    return NULL;
+  else
+    sbi_memset((void*)new_link_mem, 0, resp_size);
+
+  if(resp_size <= sizeof(struct link_mem_t) + (*tail)->slab_size)
+  {
+    mm_free(new_link_mem, resp_size);
+  }
+
+  (*tail)->next_link_mem = new_link_mem;
+  new_link_mem->mem_size = resp_size;
+  new_link_mem->slab_num = (resp_size - sizeof(struct link_mem_t)) / (*tail)->slab_size;
+  new_link_mem->slab_size = (*tail)->slab_size;
+  void* align_addr = (char*)new_link_mem + sizeof(struct link_mem_t);
+  new_link_mem->addr = (char*)size_up_align((unsigned long)align_addr, (*tail)->slab_size);
+  new_link_mem->next_link_mem = NULL;
+  
+  *tail = new_link_mem;
+
+  return new_link_mem;
+}
+
+/**
+ * \brief Remove the entry (indicated by ptr) in the head's list.
+ * \param head Head of the link memory.
+ * \param pte The removed link memory ptr.
+ */
+int remove_link_mem(struct link_mem_t** head, struct link_mem_t* ptr)
+{
+  struct link_mem_t *cur_link_mem, *tmp_link_mem;
+  int retval =0;
+
+  cur_link_mem = *head;
+  if (cur_link_mem == ptr)
+  {
+    *head = cur_link_mem->next_link_mem;
+    mm_free(cur_link_mem, cur_link_mem->mem_size);
+    return retval;
+  }
+
+  for(; cur_link_mem != NULL; cur_link_mem = cur_link_mem->next_link_mem)
+  {
+    if (cur_link_mem->next_link_mem == ptr)
+    {
+      tmp_link_mem = cur_link_mem->next_link_mem;
+      cur_link_mem->next_link_mem = cur_link_mem->next_link_mem->next_link_mem;
+      mm_free(tmp_link_mem, tmp_link_mem->mem_size);
+      return retval;
+    }
+  }
+
+  return retval;
+}
+
+/** 
+ * \brief alloc an enclave_t structure from encalve_metadata_head.
+ * Eid represents the location in the list.
+ */
+struct enclave_t* __alloc_enclave()
+{
+  struct link_mem_t *cur, *next;
+  struct enclave_t* enclave = NULL;
+  int i = 0, found = 0, eid = 0;
+
+  //enclave metadata list hasn't be initialized yet
+  if(enclave_metadata_head == NULL)
+  {
+    enclave_metadata_head = init_mem_link(ENCLAVE_METADATA_REGION_SIZE, sizeof(struct enclave_t));
+    if(!enclave_metadata_head)
+    {
+      sbi_printf("M mode: __alloc_enclave: don't have enough mempry\n");
+      goto alloc_eid_out;
+    }
+    enclave_metadata_tail = enclave_metadata_head;
+  }
+
+  for(cur = enclave_metadata_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(i = 0; i < (cur->slab_num); i++)
+    {
+      enclave = (struct enclave_t*)(cur->addr) + i;
+      if(enclave->state == INVALID)
+      {
+        sbi_memset((void*)enclave, 0, sizeof(struct enclave_t));
+        enclave->state = FRESH;
+        enclave->eid = eid;
+        found = 1;
+        break;
+      }
+      eid++;
+    }
+    if(found)
+      break;
+  }
+
+  //don't have enough enclave metadata
+  if(!found)
+  {
+    next = add_link_mem(&enclave_metadata_tail);
+    if(next == NULL)
+    {
+      sbi_bug("M mode: __alloc_enclave: add new link memory is failed\n");
+      enclave = NULL;
+      goto alloc_eid_out;
+    }
+    enclave = (struct enclave_t*)(next->addr);
+    sbi_memset((void*)enclave, 0, sizeof(struct enclave_t));
+    enclave->state = FRESH;  
+    enclave->eid = eid;
+  }
+
+alloc_eid_out:
+  return enclave;
+}
+ 
+/** 
+ * \brief Free the enclave with the given eid in the enclave list.
+ * 
+ * \param eid enclave id, and represents the location in the list.
+ */
+int __free_enclave(int eid)
+{
+  struct link_mem_t *cur;
+  struct enclave_t *enclave = NULL;
+  int found=0 , count=0, ret_val=0;
+
+  for(cur = enclave_metadata_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    if(eid < (count + cur->slab_num))
+    {
+      enclave = (struct enclave_t*)(cur->addr) + (eid - count);
+      sbi_memset((void*)enclave, 0, sizeof(struct enclave_t));
+      enclave->state = INVALID;
+      found = 1;
+      ret_val = 0;
+      break;
+    }
+    count += cur->slab_num;
+  }
+
+  //haven't alloc this eid 
+  if(!found)
+  {
+    sbi_bug("M mode: __free_enclave: haven't alloc this eid\n");
+    ret_val = -1;
+  }
+
+  return ret_val;
+}
+
+/** 
+ * \brief Get the enclave with the given eid.
+ * 
+ * \param eid enclave id, and represents the location in the list.
+ */
+struct enclave_t* __get_enclave(int eid)
+{
+  struct link_mem_t *cur;
+  struct enclave_t *enclave;
+  int found=0, count=0;
+
+  for(cur = enclave_metadata_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    if(eid < (count + cur->slab_num))
+    {
+      enclave = (struct enclave_t*)(cur->addr) + (eid - count);
+      found = 1;
+      break;
+    }
+
+    count += cur->slab_num;
+  }
+
+  //haven't alloc this eid 
+  if(!found)
+  {
+    sbi_bug("M mode: __get_enclave: haven't alloc this enclave\n");
+    enclave = NULL;
+  }
+
+  return enclave;
+}
+
+/** 
+ * \brief Check whether the enclave name is duplicated
+ * return 0 if the enclave name is unique, otherwise
+ * return -1.
+ * 
+ * \param enclave_name Checked enclave name.
+ * \param target_eid The target enclave id
+ */
+int check_enclave_name(char *enclave_name, int target_eid)
+{
+  struct link_mem_t *cur;
+  struct enclave_t* enclave = NULL;
+  int i=0, eid=0;
+  for(cur = enclave_metadata_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(i = 0; i < (cur->slab_num); i++)
+    {
+      enclave = (struct enclave_t*)(cur->addr) + i;
+      if((enclave->state > INVALID) &&(enclave_name_cmp(enclave_name, enclave->enclave_name)==0) && (target_eid != eid))
+      {
+        sbi_bug("M mode: check enclave name: enclave name is already existed, enclave name is %s\n", enclave_name);
+        return -1;
+      }
+      eid++;
+    }
+  }
+  return 0;
+}
+
+/** 
+ * \brief Alloc shadow enclave (seid) in the shadow enclave list.
+ */
+static struct shadow_enclave_t* __alloc_shadow_enclave()
+{
+  struct link_mem_t *cur, *next;
+  struct shadow_enclave_t* shadow_enclave = NULL;
+  int i=0, found=0, eid=0;
+
+  //enclave metadata list hasn't be initialized yet
+  if(shadow_enclave_metadata_head == NULL)
+  {
+    shadow_enclave_metadata_head = init_mem_link(SHADOW_ENCLAVE_METADATA_REGION_SIZE, sizeof(struct shadow_enclave_t));
+    if(!shadow_enclave_metadata_head)
+    {
+      sbi_printf("M mode: __alloc_enclave: don't have enough memory\n");
+      goto alloc_eid_out;
+    }
+    shadow_enclave_metadata_tail = shadow_enclave_metadata_head;
+  }
+
+  for(cur = shadow_enclave_metadata_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(i = 0; i < (cur->slab_num); i++)
+    {
+      shadow_enclave = (struct shadow_enclave_t*)(cur->addr) + i;
+      if(shadow_enclave->state == INVALID)
+      {
+        sbi_memset((void*)shadow_enclave, 0, sizeof(struct shadow_enclave_t));
+        shadow_enclave->state = FRESH;
+        shadow_enclave->eid = eid;
+        found = 1;
+        break;
+      }
+      eid++;
+    }
+    if(found)
+      break;
+  }
+
+  // don't have enough enclave metadata
+  if(!found)
+  {
+    next = add_link_mem(&shadow_enclave_metadata_tail);
+    if(next == NULL)
+    {
+      sbi_printf("M mode: __alloc_shadow_enclave: don't have enough mem\n");
+      shadow_enclave = NULL;
+      goto alloc_eid_out;
+    }
+    shadow_enclave = (struct shadow_enclave_t*)(next->addr);
+    sbi_memset((void*)shadow_enclave, 0, sizeof(struct shadow_enclave_t));
+    shadow_enclave->state = FRESH;  
+    shadow_enclave->eid = eid;
+  }
+
+alloc_eid_out:
+  return shadow_enclave;
+}
+
+/** 
+ * \brief Get the shadow enclave structure with the given eid.
+ * 
+ * \param eid the shadow enclave id.
+ */
+static struct shadow_enclave_t* __get_shadow_enclave(int eid)
+{
+  struct link_mem_t *cur;
+  struct shadow_enclave_t *shadow_enclave;
+  int found=0, count=0;
+
+  for(cur = shadow_enclave_metadata_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    if(eid < (count + cur->slab_num))
+    {
+      shadow_enclave = (struct shadow_enclave_t*)(cur->addr) + (eid - count);
+      found = 1;
+      break;
+    }
+
+    count += cur->slab_num;
+  }
+
+  //haven't alloc this eid 
+  if(!found)
+  {
+    sbi_bug("M mode: __get_enclave: haven't alloc this shadow_enclave\n");
+    shadow_enclave = NULL;
+  }
+
+  return shadow_enclave;
+}
+
+/**
+ * \brief this function is used to handle IPC in enclave,
+ * 	  it will return the last enclave in the chain.
+ * 	  This is used to help us identify the real executing encalve.
+ * 
+ * \param eid The enclave id.
+ */
+struct enclave_t* __get_real_enclave(int eid)
+{
+  struct enclave_t* enclave = __get_enclave(eid);
+  if(!enclave)
+    return NULL;
+
+  struct enclave_t* real_enclave = NULL;
+  if(enclave->cur_callee_eid == -1)
+    real_enclave = enclave;
+  else
+    real_enclave = __get_enclave(enclave->cur_callee_eid);
+
+  return real_enclave;
+}
+
+
+/********************************************/
+/*                   Relay Page             */
+/********************************************/
+
+/*
+  allocate a new entry in the link memory, if link head is NULL, we initialize the link memory.
+  When the ownership of  relay page is changed, we need first destroy the old relay page entry which
+  records the out of  data ownership of relay page, and then allocate the new relay page entry with
+  new ownership.
+
+  Return value:
+  relay_pagfe_entry @ allocate the relay page successfully
+  NULL @ allcate the relay page is failed
+ */
+
+/**
+ * \brief Alloc a relay page entry in the relay page list.
+ * 
+ * \param enclave_name The enclave name (specified by the user).
+ * \param relay_page_addr The relay page address for the given enclave (enclave_name).
+ * \param relay_page_size The relay page size for the given enclave (enclave_name).
+ */
+struct relay_page_entry_t* __alloc_relay_page_entry(char *enclave_name, unsigned long relay_page_addr, unsigned long relay_page_size)
+{
+  struct link_mem_t *cur, *next;
+  struct relay_page_entry_t* relay_page_entry = NULL;
+  int found = 0, link_mem_index = 0;
+
+  //relay_page_entry metadata list hasn't be initialized yet
+  if(relay_page_head == NULL)
+  {
+    relay_page_head = init_mem_link(sizeof(struct relay_page_entry_t)*ENTRY_PER_RELAY_PAGE_REGION, sizeof(struct relay_page_entry_t));
+    
+    if(!relay_page_head)
+      goto failed;
+    
+    relay_page_tail = relay_page_head;
+  }
+
+  //check whether relay page is owned by another enclave
+  for(cur = relay_page_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(int i = 0; i < (cur->slab_num); i++)
+    {
+      relay_page_entry = (struct relay_page_entry_t*)(cur->addr) + i;
+      if(relay_page_entry->addr == relay_page_addr)
+      {
+        sbi_bug("M mode: __alloc_relay_page_entry: the relay page is owned by another enclave\n");
+        relay_page_entry = (void*)(-1UL);
+        goto failed;
+      }
+    }
+  }
+  //traverse the link memory and check whether there is an empty entry in the link memoy
+  for(cur = relay_page_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(int i = 0; i < (cur->slab_num); i++)
+    {
+      relay_page_entry = (struct relay_page_entry_t*)(cur->addr) + i;
+      //address in the relay page entry remains zero which means the entry is not used
+      if(relay_page_entry->addr == 0)
+      {
+        sbi_memcpy(relay_page_entry->enclave_name, enclave_name, NAME_LEN);
+        relay_page_entry->addr = relay_page_addr;
+        relay_page_entry->size = relay_page_size;
+        found = 1;
+        break;
+      }
+    }
+    if(found)
+      break;
+    link_mem_index = link_mem_index + 1; 
+  }
+
+  //don't have enough memory to allocate a new entry in current link memory, so allocate a new link memory 
+  if(!found)
+  {
+    next = add_link_mem(&relay_page_tail);
+    if(next == NULL)
+    {
+      sbi_bug("M mode: __alloc_relay_page_entry: don't have enough mem\n");
+      relay_page_entry = NULL;
+      goto failed;
+    }
+    relay_page_entry = (struct relay_page_entry_t*)(next->addr);
+    sbi_memcpy(relay_page_entry->enclave_name, enclave_name, NAME_LEN);
+    relay_page_entry->addr = relay_page_addr;
+    relay_page_entry->size = relay_page_size;
+  }
+
+  return relay_page_entry;
+
+failed:
+  if(relay_page_entry)
+    sbi_memset((void*)relay_page_entry, 0, sizeof(struct relay_page_entry_t));
+
+  return NULL;
+}
+
+/**
+ * \brief Free the relay page indexed by the the given enclave name.
+ * now we just set the address in the relay page netry to zero
+ * which means this relay page entry is not used.
+ * 
+ * return value:
+ * 0 : free the relay_page successfully
+ * -1 : can not find the corresponding relay page
+ * 
+ * \param relay_page_addr The relay page address.
+ * \param relay_page_size The relay page size.
+ */
+int __free_relay_page_entry(unsigned long relay_page_addr, unsigned long relay_page_size)
+{
+  struct link_mem_t *cur;
+  struct relay_page_entry_t *relay_page_entry = NULL;
+  int found = 0, ret_val = 0;
+
+  for(cur = relay_page_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(int i = 0; i < (cur->slab_num); i++)
+    {
+      relay_page_entry = (struct relay_page_entry_t*)(cur->addr) + i;
+      //find the corresponding relay page entry by given address and size
+      if((relay_page_entry->addr >= relay_page_addr) && ((relay_page_entry->addr + relay_page_entry->size) <= (relay_page_addr + relay_page_size)))
+      {
+        found = 1;
+        sbi_memset(relay_page_entry->enclave_name, 0, NAME_LEN);
+        relay_page_entry->addr = 0;
+        relay_page_entry->size = 0;
+      }
+    }
+  }
+  //haven't alloc this relay page
+  if(!found)
+  {
+    sbi_bug("M mode: __free_relay_page_entry: relay page  [%lx : %lx + %lx]is not existed \n", relay_page_addr, relay_page_addr, relay_page_size);
+    ret_val = -1;
+  }
+
+  return ret_val;
+}
+
+/**
+ * \brief Retrieve the relay page entry by given the enclave name.
+ * 
+ * \param enclave_name: Get the relay page entry with given enclave name.
+ * \param slab_index: Find the corresponding relay page entry and return the slab index in the link memory.
+ * \param link_mem_index: Find the corresponding relay page entry and return the link mem index in the link memory.
+ */
+struct relay_page_entry_t* __get_relay_page_by_name(char* enclave_name, int *slab_index, int *link_mem_index)
+{
+  struct link_mem_t *cur;
+  struct relay_page_entry_t *relay_page_entry = NULL;
+  int i, k, found=0;
+
+  cur = relay_page_head;
+  for (k  = 0; k < (*link_mem_index); k++)
+    cur = cur->next_link_mem;
+  
+  i = *slab_index;
+  for(; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(; i < (cur->slab_num); ++i)
+    {
+      relay_page_entry = (struct relay_page_entry_t*)(cur->addr) + i;
+      if((relay_page_entry->addr != 0) && enclave_name_cmp(relay_page_entry->enclave_name, enclave_name)==0)
+      {
+        found = 1;
+        *slab_index = i+1;
+        //check whether slab_index is overflow
+        if ((i+1) >= (cur->slab_num))
+        {
+          *slab_index = 0;
+          *link_mem_index = (*link_mem_index) + 1;
+        }
+        break;
+      }
+    }
+    if(found)
+      break;
+    *link_mem_index = (*link_mem_index) + 1;
+    i=0;
+  }
+
+  //haven't alloc this eid 
+  if(!found)
+  {
+    sbi_bug("M mode: __get_relay_page_by_name: haven't alloc this enclave:%s\n", enclave_name);
+    return NULL;
+  }
+
+  return relay_page_entry;
+}
+
+/**
+ * \brief  Change the relay page ownership, delete the old relay page entry in the link memory
+ * and add an entry with new ownership .
+ * If the relay page is not existed, reture error.
+ * 
+ * \param relay_page_addr: Relay page address.
+ * \param relay_page_size: Relay page size.
+ * \param enclave_name: The new ownership (specified by the enclave name) for the relay page.
+ */
+uintptr_t change_relay_page_ownership(unsigned long relay_page_addr, unsigned long relay_page_size, char *enclave_name)
+{
+  uintptr_t ret_val = 0;
+  if ( __free_relay_page_entry( relay_page_addr,  relay_page_size) < 0)
+  {
+    sbi_bug("M mode: change_relay_page_ownership: can not free relay page which needs transfer the ownership\n");
+    ret_val = -1;
+    return ret_val;
+  }
+
+  // This relay page entry allocation can not be failed
+  if (__alloc_relay_page_entry(enclave_name, relay_page_addr, relay_page_size) == NULL)
+  {
+    sbi_bug("M mode: change_relay_page_ownership: can not alloc relay page entry, addr is %lx\n", relay_page_addr);
+  }
+
+  return ret_val;
+}
+
+/**
+ * \brief Swap states from host to enclaves, e.g., satp, stvec, etc.
+ * 	  it is used when we run/resume enclave/shadow-encalves.
+ * 
+ * \param host_regs The host regs ptr.
+ * \param enclave The given enclave.
+ */
+static int swap_from_host_to_enclave(uintptr_t* host_regs, struct enclave_t* enclave)
+{
+  //grant encalve access to memory
+  if(grant_enclave_access(enclave) < 0)
+    return -1;
+
+  //save host context
+  swap_prev_state(&(enclave->thread_context), host_regs);
+
+  //different platforms have differnt ptbr switch methods
+  switch_to_enclave_ptbr(&(enclave->thread_context), enclave->thread_context.encl_ptbr);
+
+  //save host trap vector
+  swap_prev_stvec(&(enclave->thread_context), csr_read(CSR_STVEC));
+
+  //TODO: save host cache binding
+  //swap_prev_cache_binding(&enclave -> threads[0], csr_read(0x356));
+
+  //disable interrupts
+  swap_prev_mie(&(enclave->thread_context), csr_read(CSR_MIE));
+  csr_read_clear(CSR_MIP, MIP_MTIP);
+  csr_read_clear(CSR_MIP, MIP_STIP);
+  csr_read_clear(CSR_MIP, MIP_SSIP);
+  csr_read_clear(CSR_MIP, MIP_SEIP);
+
+  //disable interrupts/exceptions delegation
+  swap_prev_mideleg(&(enclave->thread_context), csr_read(CSR_MIDELEG));
+  swap_prev_medeleg(&(enclave->thread_context), csr_read(CSR_MEDELEG));
+
+  //swap the mepc to transfer control to the enclave
+  swap_prev_mepc(&(enclave->thread_context), csr_read(CSR_MEPC)); 
+
+  //set mstatus to transfer control to u mode
+  uintptr_t mstatus = csr_read(CSR_MSTATUS);
+  mstatus = INSERT_FIELD(mstatus, MSTATUS_MPP, PRV_U);
+  csr_write(CSR_MSTATUS, mstatus);
+
+  //mark that cpu is in enclave world now
+  enter_enclave_world(enclave->eid);
+
+  __asm__ __volatile__ ("sfence.vma" : : : "memory");
+
+  return 0;
+}
+
+/**
+ * \brief Similiar to swap_from_host_to_enclave.
+ * 
+ * \param host_regs The host regs ptr.
+ * \param enclave The given enclave.
+ */
+static int swap_from_enclave_to_host(uintptr_t* regs, struct enclave_t* enclave)
+{
+  //retrieve enclave access to memory
+  retrieve_enclave_access(enclave);
+
+  //restore host context
+  swap_prev_state(&(enclave->thread_context), regs);
+
+  //restore host's ptbr
+  switch_to_host_ptbr(&(enclave->thread_context), enclave->host_ptbr);
+
+  //restore host stvec
+  swap_prev_stvec(&(enclave->thread_context), csr_read(CSR_STVEC));
+
+  //TODO: restore host cache binding
+  //swap_prev_cache_binding(&(enclave->thread_context), );
+  
+  //restore interrupts
+  swap_prev_mie(&(enclave->thread_context), csr_read(CSR_MIE));
+
+  //restore interrupts/exceptions delegation
+  swap_prev_mideleg(&(enclave->thread_context), csr_read(CSR_MIDELEG));
+  swap_prev_medeleg(&(enclave->thread_context), csr_read(CSR_MEDELEG));
+
+  //transfer control back to kernel
+  swap_prev_mepc(&(enclave->thread_context), csr_read(CSR_MEPC));
+
+  //restore mstatus
+  uintptr_t mstatus = csr_read(CSR_MSTATUS);
+  mstatus = INSERT_FIELD(mstatus, MSTATUS_MPP, PRV_S);
+  csr_write(CSR_MSTATUS, mstatus);
+
+  //mark that cpu is out of enclave world now
+  exit_enclave_world();
+
+  __asm__ __volatile__ ("sfence.vma" : : : "memory");
+
+  return 0;
+}
+
+static inline int tlb_remote_sfence()
+{
+  int ret;
+  struct sbi_tlb_info tlb_info;
+	u32 source_hart = current_hartid();
+  SBI_TLB_INFO_INIT(&tlb_info, 0, 0, 0, 0,
+				  SBI_TLB_FLUSH_VMA, source_hart);
+	ret = sbi_tlb_request(0xFFFFFFFF&(~(1<<source_hart)), 0, &tlb_info);
+  return ret;
+}
+
+/**
+ * \brief The auxiliary function for the enclave call.
+ * 
+ * \param regs The reg argument.
+ * \param top_caller_enclave The toppest enclave in the enclave calling stack.
+ * \param caller_enclave The caller enclave.
+ * \param callee_enclave The callee enclave.
+ */
+static int __enclave_call(uintptr_t* regs, struct enclave_t* top_caller_enclave, struct enclave_t* caller_enclave, struct enclave_t* callee_enclave)
+{
+  //move caller's host context to callee's host context
+  uintptr_t encl_ptbr = callee_enclave->thread_context.encl_ptbr;
+  sbi_memcpy((void*)(&(callee_enclave->thread_context)), (void*)(&(caller_enclave->thread_context)), sizeof(struct thread_state_t));
+  callee_enclave->thread_context.encl_ptbr = encl_ptbr;
+  callee_enclave->host_ptbr = caller_enclave->host_ptbr;
+  callee_enclave->ocall_func_id = caller_enclave->ocall_func_id;
+  callee_enclave->ocall_arg0 = caller_enclave->ocall_arg0;
+  callee_enclave->ocall_arg1 = caller_enclave->ocall_arg1;
+  callee_enclave->ocall_syscall_num = caller_enclave->ocall_syscall_num; 
+
+  //save caller's enclave context on its prev_state
+  swap_prev_state(&(caller_enclave->thread_context), regs);
+  caller_enclave->thread_context.prev_stvec = csr_read(CSR_STVEC);
+  caller_enclave->thread_context.prev_mie = csr_read(CSR_MIE);
+  caller_enclave->thread_context.prev_mideleg = csr_read(CSR_MIDELEG);
+  caller_enclave->thread_context.prev_medeleg = csr_read(CSR_MEDELEG);
+  caller_enclave->thread_context.prev_mepc = csr_read(CSR_MEPC);
+
+  //clear callee's enclave context
+  sbi_memset((void*)regs, 0, sizeof(struct general_registers_t));
+
+  //different platforms have differnt ptbr switch methods
+  switch_to_enclave_ptbr(&(callee_enclave->thread_context), callee_enclave->thread_context.encl_ptbr);
+
+  //callee use caller's stvec
+
+  //callee use caller's cache binding
+
+  //callee use caller's mie/mip
+  csr_read_clear(CSR_MIP, MIP_MTIP);
+  csr_read_clear(CSR_MIP, MIP_STIP);
+  csr_read_clear(CSR_MIP, MIP_SSIP);
+  csr_read_clear(CSR_MIP, MIP_SEIP);
+
+  //callee use caller's interrupts/exceptions delegation
+
+  //transfer control to the callee enclave
+  csr_write(CSR_MEPC, callee_enclave->entry_point);
+
+  //callee use caller's mstatus
+
+  //mark that cpu is in callee enclave world now
+  enter_enclave_world(callee_enclave->eid);
+
+  top_caller_enclave->cur_callee_eid = callee_enclave->eid;
+  caller_enclave->cur_callee_eid = callee_enclave->eid;
+  callee_enclave->caller_eid = caller_enclave->eid;
+  callee_enclave->top_caller_eid = top_caller_enclave->eid;
+
+  __asm__ __volatile__ ("sfence.vma" : : : "memory");
+
+  return 0;
+}
+
+/**
+ * \brief The auxiliary function for the enclave return.
+ * 
+ * \param regs The reg argument.
+ * \param top_caller_enclave The toppest enclave in the enclave calling stack.
+ * \param caller_enclave The caller enclave.
+ * \param callee_enclave The callee enclave.
+ */
+static int __enclave_return(uintptr_t* regs, struct enclave_t* callee_enclave, struct enclave_t* caller_enclave, struct enclave_t* top_caller_enclave)
+{
+  //restore caller's context
+  sbi_memcpy((void*)regs, (void*)(&(caller_enclave->thread_context.prev_state)), sizeof(struct general_registers_t));
+  swap_prev_stvec(&(caller_enclave->thread_context), callee_enclave->thread_context.prev_stvec);
+  swap_prev_mie(&(caller_enclave->thread_context), callee_enclave->thread_context.prev_mie);
+  swap_prev_mideleg(&(caller_enclave->thread_context), callee_enclave->thread_context.prev_mideleg);
+  swap_prev_medeleg(&(caller_enclave->thread_context), callee_enclave->thread_context.prev_medeleg);
+  swap_prev_mepc(&(caller_enclave->thread_context), callee_enclave->thread_context.prev_mepc);
+
+  //restore caller's host context
+  sbi_memcpy((void*)(&(caller_enclave->thread_context.prev_state)), (void*)(&(callee_enclave->thread_context.prev_state)), sizeof(struct general_registers_t));
+
+  //clear callee's enclave context
+  uintptr_t encl_ptbr = callee_enclave->thread_context.encl_ptbr;
+  sbi_memset((void*)(&(callee_enclave->thread_context)), 0, sizeof(struct thread_state_t));
+  callee_enclave->thread_context.encl_ptbr = encl_ptbr;
+  callee_enclave->host_ptbr = 0;
+  callee_enclave->ocall_func_id = NULL;
+  callee_enclave->ocall_arg0 = NULL;
+  callee_enclave->ocall_arg1 = NULL;
+  callee_enclave->ocall_syscall_num = NULL;
+
+  //different platforms have differnt ptbr switch methods
+  switch_to_enclave_ptbr(&(caller_enclave->thread_context), caller_enclave->thread_context.encl_ptbr);
+
+  csr_read_clear(CSR_MIP, MIP_MTIP);
+  csr_read_clear(CSR_MIP, MIP_STIP);
+  csr_read_clear(CSR_MIP, MIP_SSIP);
+  csr_read_clear(CSR_MIP, MIP_SEIP);
+
+  //mark that cpu is in caller enclave world now
+  enter_enclave_world(caller_enclave->eid);
+  top_caller_enclave->cur_callee_eid = caller_enclave->eid;
+  caller_enclave->cur_callee_eid = -1;
+  callee_enclave->caller_eid = -1;
+  callee_enclave->top_caller_eid = -1;
+
+  __asm__ __volatile__ ("sfence.vma" : : : "memory");
+
+  return 0;
+}
+
+/**
+ * \brief free a list of memory indicated by pm_area_struct.
+ * 	  the pages are zero-ed and turned back to host.
+ * 
+ * \param pma The pma structure of the free memory. 
+ */
+void free_enclave_memory(struct pm_area_struct *pma)
+{
+  uintptr_t paddr = 0;
+  uintptr_t size = 0;
+
+  extern spinlock_t mbitmap_lock;
+  spin_lock(&mbitmap_lock);
+
+  while(pma)
+  {
+    paddr = pma->paddr;
+    size = pma->size;
+    pma = pma->pm_next;
+    //we can not clear the first page as it will be used to free mem by host
+    sbi_memset((void*)(paddr + RISCV_PGSIZE), 0, size - RISCV_PGSIZE);
+    __free_secure_memory(paddr, size);
+  }
+
+  spin_unlock(&mbitmap_lock);
+}
+
+void initilze_va_struct(struct pm_area_struct* pma, struct vm_area_struct* vma, struct enclave_t* enclave)
+{
+  pma->pm_next = NULL;
+  enclave->pma_list = pma;
+  traverse_vmas(enclave->root_page_table, vma);
+  //FIXME: here we assume there are exactly text(include text/data/bss) vma and stack vma
+  while(vma)
+  {
+    if(vma->va_start == ENCLAVE_DEFAULT_TEXT_BASE)
+    {
+      enclave->text_vma = vma;
+    }
+    if(vma->va_end == ENCLAVE_DEFAULT_STACK_BASE)
+    {
+      enclave->stack_vma = vma;
+      enclave->_stack_top = enclave->stack_vma->va_start;
+    }
+    vma->pma = pma;
+    vma = vma->vm_next;
+  }
+  if(enclave->text_vma)
+    enclave->text_vma->vm_next = NULL;
+  if(enclave->stack_vma)
+    enclave->stack_vma->vm_next = NULL;
+  enclave->_heap_top = ENCLAVE_DEFAULT_HEAP_BASE;
+  enclave->heap_vma = NULL;
+  enclave->mmap_vma = NULL;
+}
+
+/**************************************************************/
+/*                   called by host                           */
+/**************************************************************/
+
+/**
+ * \brief Create a new enclave with the create_args.
+ * 
+ * \param create_args The arguments for creating a new enclave. 
+ */
+uintptr_t create_enclave(enclave_create_param_t create_args)
+{
+  struct enclave_t* enclave = NULL;
+  struct pm_area_struct* pma = NULL;
+  struct vm_area_struct* vma = NULL;
+  uintptr_t ret = 0, free_mem = 0;
+  int need_free_secure_memory = 0;
+
+  acquire_enclave_metadata_lock();
+
+  if(!enable_enclave())
+  {
+    ret = ENCLAVE_ERROR;
+    sbi_bug("M mode: %s: cannot enable enclave \n", __func__);
+    goto failed;
+  }
+
+  //check enclave memory layout
+  if(check_and_set_secure_memory(create_args.paddr, create_args.size) != 0)
+  {
+    ret = ENCLAVE_ERROR;
+    sbi_bug("M mode: %s: check and set secure memory is failaed\n", __func__);
+    goto failed;
+  }
+  need_free_secure_memory = 1;
+
+  //check enclave memory layout
+  if(check_enclave_layout(create_args.paddr + RISCV_PGSIZE, 0, -1UL, create_args.paddr, create_args.paddr + create_args.size) != 0)
+  {
+    ret = ENCLAVE_ERROR;
+    sbi_bug("M mode: %s: check memory layout is failed\n", __func__);
+    goto failed;
+  }
+
+  //Sync and flush the remote TLB entry.
+  tlb_remote_sfence();
+
+  enclave = __alloc_enclave();
+  if(!enclave)
+  {
+    ret = ENCLAVE_NO_MEM;
+    sbi_printf("M mode: %s: alloc enclave is failed \n", __func__);
+    goto failed;
+  }
+
+  SET_ENCLAVE_METADATA(create_args.entry_point, enclave, &create_args, enclave_create_param_t *, paddr);
+
+  //traverse vmas
+  pma = (struct pm_area_struct*)(create_args.paddr);
+  vma = (struct vm_area_struct*)(create_args.paddr + sizeof(struct pm_area_struct));
+  pma->paddr = create_args.paddr;
+  pma->size = create_args.size;
+  pma->free_mem = create_args.free_mem;
+  if(pma->free_mem < pma->paddr || pma->free_mem >= pma->paddr+pma->size
+      || pma->free_mem & ((1<<RISCV_PGSHIFT) - 1))
+  {
+    ret = ENCLAVE_ERROR;
+    sbi_bug("M mode: %s: pma free_mem is failed\n", __func__);
+    goto failed;
+  }
+
+  initilze_va_struct(pma, vma, enclave);
+
+  enclave->free_pages = NULL;
+  enclave->free_pages_num = 0;
+  free_mem = create_args.paddr + create_args.size - RISCV_PGSIZE;
+
+  // Reserve the first two entries for free memory page
+  while(free_mem >= create_args.free_mem)
+  {
+    struct page_t *page = (struct page_t*)free_mem;
+    page->paddr = free_mem;
+    page->next = enclave->free_pages;
+    enclave->free_pages = page;
+    enclave->free_pages_num += 1;
+    free_mem -= RISCV_PGSIZE;
+  }
+  //check kbuffer
+  if(create_args.kbuffer_size < RISCV_PGSIZE || create_args.kbuffer & (RISCV_PGSIZE-1) || create_args.kbuffer_size & (RISCV_PGSIZE-1))
+  {
+    ret = ENCLAVE_ERROR;
+    sbi_bug("M mode: %s: kbuffer check is failed\n", __func__);
+    goto failed;
+  }
+  mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), ENCLAVE_DEFAULT_KBUFFER, create_args.kbuffer, create_args.kbuffer_size);
+
+  //check shm
+  if(create_args.shm_paddr && create_args.shm_size &&
+      !(create_args.shm_paddr & (RISCV_PGSIZE-1)) && !(create_args.shm_size & (RISCV_PGSIZE-1)))
+  {
+    mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), ENCLAVE_DEFAULT_SHM_BASE, create_args.shm_paddr, create_args.shm_size);
+    enclave->shm_paddr = create_args.shm_paddr;
+    enclave->shm_size = create_args.shm_size;
+  }
+  else
+  {
+    enclave->shm_paddr = 0;
+    enclave->shm_size = 0;
+  }
+  
+  hash_enclave(enclave, (void*)(enclave->hash), 0);
+  copy_word_to_host((unsigned int*)create_args.eid_ptr, enclave->eid);
+  release_enclave_metadata_lock();
+  return ret;
+
+failed:
+  if(need_free_secure_memory)
+  {
+    free_secure_memory(create_args.paddr, create_args.size);
+  }
+  if(enclave)
+  {
+    __free_enclave(enclave->eid);
+  }
+  release_enclave_metadata_lock();
+  return ret;
+}
+
+/**
+ * \brief Create a new shadow enclave with the create_args.
+ * 
+ * \param create_args The arguments for creating a new shadow enclave. 
+ */
+uintptr_t create_shadow_enclave(enclave_create_param_t create_args)
+{
+  uintptr_t ret = 0;
+  int need_free_secure_memory = 0;
+  acquire_enclave_metadata_lock();
+  eapp_args = 0;
+  if(!enable_enclave())
+  {
+    ret = ENCLAVE_ERROR;
+    goto failed;
+  }
+
+  //check enclave memory layout
+  if(check_and_set_secure_memory(create_args.paddr, create_args.size) != 0)
+  {
+    ret = ENCLAVE_ERROR;
+    goto failed;
+  }
+
+  //Sync and flush the remote TLB entry.
+  tlb_remote_sfence();
+
+  need_free_secure_memory = 1;
+  //check enclave memory layout
+  if(check_enclave_layout(create_args.paddr + RISCV_PGSIZE, 0, -1UL, create_args.paddr, create_args.paddr + create_args.size) != 0)
+  {
+    ret = ENCLAVE_ERROR;
+    goto failed;
+  }
+  struct shadow_enclave_t* shadow_enclave;
+  shadow_enclave = __alloc_shadow_enclave();
+  if(!shadow_enclave)
+  {
+    sbi_bug("M mode: create shadow enclave: no enough memory to alloc_shadow_enclave\n");
+    ret = ENCLAVE_NO_MEM;
+    goto failed;
+  }
+  shadow_enclave->entry_point = create_args.entry_point;
+  //first page is reserve for page link
+  shadow_enclave->root_page_table = create_args.paddr + RISCV_PGSIZE;
+  shadow_enclave->thread_context.encl_ptbr = ((create_args.paddr+RISCV_PGSIZE) >> RISCV_PGSHIFT) | SATP_MODE_CHOICE;
+  
+  hash_shadow_enclave(shadow_enclave, (void*)(shadow_enclave->hash), 0);
+  copy_word_to_host((unsigned int*)create_args.eid_ptr, shadow_enclave->eid);
+  spin_unlock(&enclave_metadata_lock);
+  return ret;
+
+failed:
+  if(need_free_secure_memory)
+  {
+    free_secure_memory(create_args.paddr, create_args.size);
+  }
+  spin_unlock(&enclave_metadata_lock);
+  return ret;
+}
+
+uintptr_t map_relay_page(unsigned int eid, uintptr_t mm_arg_addr, uintptr_t mm_arg_size, uintptr_t* mmap_offset, struct enclave_t* enclave, struct relay_page_entry_t* relay_page_entry)
+{
+  uintptr_t retval = 0;
+  // If the mm_arg_size is zero but mm_arg_addr is not zero, it means the relay page is transfer from other enclave 
+  if(mm_arg_addr && !mm_arg_size)
+  {
+    int slab_index = 0, link_mem_index = 0, kk = 0;
+    if(check_enclave_name(enclave->enclave_name, eid) < 0)
+    {
+      sbi_bug("M modemap_relay_page: check enclave name is failed\n");
+      retval = -1UL;
+      return retval;
+    }
+    while((relay_page_entry = __get_relay_page_by_name(enclave->enclave_name, &slab_index, &link_mem_index)) != NULL)
+    {
+      mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), ENCLAVE_DEFAULT_MM_ARG_BASE + *mmap_offset, relay_page_entry->addr, relay_page_entry->size);
+      *mmap_offset = *mmap_offset + relay_page_entry->size;
+      if (enclave->mm_arg_paddr[0] == 0)
+      {
+        enclave->mm_arg_paddr[kk] = relay_page_entry->addr;
+        enclave->mm_arg_size[kk] = relay_page_entry->size;
+      }
+      else
+      {
+        // enclave->mm_arg_size = enclave->mm_arg_size + relay_page_entry->size;
+        enclave->mm_arg_paddr[kk] = relay_page_entry->addr;
+        enclave->mm_arg_size[kk] = relay_page_entry->size;
+      }
+      kk = kk + 1;
+    }
+    if ((relay_page_entry == NULL) && (enclave->mm_arg_paddr[0] == 0))
+    {
+      sbi_bug("M mode: map_relay_page: get relay page by name is failed \n");
+      retval = -1UL;
+      return retval;
+    }
+  }
+  else if(mm_arg_addr && mm_arg_size)
+  {
+    //check whether the enclave name is duplicated
+    if (check_enclave_name(enclave->enclave_name, eid) < 0)
+    {
+      sbi_bug("M modemap_relay_page: check enclave name is failed\n");
+      retval = -1UL;
+      return retval;
+    }
+    if (__alloc_relay_page_entry(enclave->enclave_name, mm_arg_addr, mm_arg_size) ==NULL)
+    {
+      sbi_printf("M mode: map_relay_page: lack of the secure memory for the relay page entries\n");
+      retval = ENCLAVE_NO_MEM;
+      return retval;
+    }
+    //check the relay page is not mapping in other enclave, and unmap the relay page for host
+    if(check_and_set_secure_memory(mm_arg_addr, mm_arg_size) != 0)
+    {
+      sbi_bug("M mode: map_relay_page: check_and_set_secure_memory is failed\n");
+      retval = -1UL;
+      return retval;
+    }
+    enclave->mm_arg_paddr[0] = mm_arg_addr;
+    enclave->mm_arg_size[0] = mm_arg_size;
+    *mmap_offset = mm_arg_size;
+    mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), ENCLAVE_DEFAULT_MM_ARG_BASE, mm_arg_addr, mm_arg_size);
+    
+    //Sync and flush the remote TLB entry.
+    tlb_remote_sfence();
+    return 0;
+  }
+
+  return retval;
+}
+
+/**
+ * \brief Run enclave with the given eid.
+ * 
+ * \param regs The host reg need to saved.
+ * \param eid The given enclave id.
+ * \param mm_arg_addr The relay page address for this enclave, map before enclave run.
+ * \param mm_arg_size The relay page size for this enclave, map before enclave run.  
+ */
+uintptr_t run_enclave(uintptr_t* regs, unsigned int eid, uintptr_t mm_arg_addr, uintptr_t mm_arg_size)
+{
+  struct enclave_t* enclave;
+  uintptr_t retval = 0, mmap_offset = 0;
+  struct relay_page_entry_t* relay_page_entry = NULL;
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_enclave(eid);
+  if(!enclave || enclave->state != FRESH || enclave->type == SERVER_ENCLAVE)
+  {
+    sbi_bug("M mode: run_enclave: enclave%d can not be accessed!\n", eid);
+    retval = -1UL;
+    goto run_enclave_out;
+  }
+
+  /** We bind a host process (host_ptbr) during run_enclave, which will be checked during resume */
+  enclave->host_ptbr = csr_read(CSR_SATP);
+  
+  if((retval =map_relay_page(eid, mm_arg_addr, mm_arg_size, &mmap_offset, enclave, relay_page_entry)) < 0)
+  {
+    if (retval == ENCLAVE_NO_MEM)
+      goto run_enclave_out;
+    else
+      goto run_enclave_out;
+  }
+  //the relay page is transfered from another enclave
+
+  if(swap_from_host_to_enclave(regs, enclave) < 0)
+  {
+    sbi_bug("M mode: run_enclave: enclave can not be run\n");
+    retval = -1UL;
+    goto run_enclave_out;
+  }
+
+  //set return address to enclave
+  csr_write(CSR_MEPC, (uintptr_t)(enclave->entry_point));
+
+  //enable timer interrupt
+  csr_read_set(CSR_MIE, MIP_MTIP);
+  csr_read_set(CSR_MIE, MIP_MSIP);
+
+  //set default stack
+  regs[2] = ENCLAVE_DEFAULT_STACK_BASE;
+
+  //pass parameters
+  if(enclave->shm_paddr)
+    regs[10] = ENCLAVE_DEFAULT_SHM_BASE;
+  else
+    regs[10] = 0;
+  retval = regs[10];
+  regs[11] = enclave->shm_size;
+  regs[12] = 1;
+  if(enclave->mm_arg_paddr[0])
+    regs[13] = ENCLAVE_DEFAULT_MM_ARG_BASE;
+  else
+    regs[13] = 0;
+  regs[14] = mmap_offset;
+  eapp_args = eapp_args+1;
+
+  enclave->state = RUNNING;
+run_enclave_out:
+  release_enclave_metadata_lock();
+  return retval;
+}
+
+/**
+ * \brief Run shodow enclave with the given eid.
+ * 
+ * \param regs The host reg need to saved.
+ * \param eid The given shadow enclave id.
+ * \param enclave_run_param The parameter for run a shadow enclave.
+ * \param mm_arg_addr The relay page address for this enclave, map before enclave run.
+ * \param mm_arg_size The relay page size for this enclave, map before enclave run.  
+ */
+uintptr_t run_shadow_enclave(uintptr_t* regs, unsigned int eid, shadow_enclave_run_param_t enclave_run_param, uintptr_t mm_arg_addr, uintptr_t mm_arg_size)
+{
+  struct enclave_t* enclave = NULL;
+  struct shadow_enclave_t* shadow_enclave = NULL;
+  struct relay_page_entry_t* relay_page_entry = NULL;
+  struct pm_area_struct* pma = NULL;
+  struct vm_area_struct* vma = NULL;
+  uintptr_t retval = 0, mmap_offset = 0, free_mem = 0;
+  int need_free_secure_memory = 0, copy_page_table_ret = 0;
+
+  acquire_enclave_metadata_lock();
+
+  shadow_enclave = __get_shadow_enclave(eid);
+  enclave = __alloc_enclave();
+
+  if(!enclave)
+  {
+    sbi_bug("create enclave from shadow enclave is failed\n");
+    retval = ENCLAVE_NO_MEM;
+    goto run_enclave_out;
+  }
+
+  if(check_and_set_secure_memory(enclave_run_param.free_page, enclave_run_param.size) != 0)
+  {
+    retval = ENCLAVE_ERROR;
+    goto run_enclave_out;
+  }
+  need_free_secure_memory = 1;
+
+  enclave->free_pages = NULL;
+  enclave->free_pages_num = 0;
+  free_mem = enclave_run_param.free_page + enclave_run_param.size - 2*RISCV_PGSIZE;
+  
+  // Reserve the first two entries in the free pages
+  while(free_mem >= enclave_run_param.free_page + 2*RISCV_PGSIZE)
+  {
+    struct page_t *page = (struct page_t*)free_mem;
+    page->paddr = free_mem;
+    page->next = enclave->free_pages;
+    enclave->free_pages = page;
+    enclave->free_pages_num += 1;
+    free_mem -= RISCV_PGSIZE;
+  }
+
+  copy_page_table_ret = __copy_page_table((pte_t*) (shadow_enclave->root_page_table), &(enclave->free_pages), 2, (pte_t*)(enclave_run_param.free_page + RISCV_PGSIZE));
+  if (copy_page_table_ret < 0)
+  {
+    sbi_bug("copy_page_table fail\n");
+    retval = ENCLAVE_ERROR;
+    goto run_enclave_out;
+  }
+
+  copy_page_table_ret =  map_empty_page((uintptr_t*)(enclave_run_param.free_page + RISCV_PGSIZE), &(enclave->free_pages), ENCLAVE_DEFAULT_STACK_BASE-ENCLAVE_DEFAULT_STACK_SIZE, ENCLAVE_DEFAULT_STACK_SIZE);
+  if (copy_page_table_ret < 0)
+  {
+    sbi_bug("alloc stack for shadow enclave fail\n");
+    sbi_bug("M mode: shadow_enclave_run: ENCLAVE_DEFAULT_STACK_SIZE is larger than the free memory size \n");
+    retval = ENCLAVE_ERROR;
+    goto run_enclave_out;
+  }
+
+  SET_ENCLAVE_METADATA(shadow_enclave->entry_point, enclave, &enclave_run_param, shadow_enclave_run_param_t *, free_page);
+
+  //traverse vmas
+  pma = (struct pm_area_struct*)(enclave_run_param.free_page);
+  vma = (struct vm_area_struct*)(enclave_run_param.free_page + sizeof(struct pm_area_struct));
+  pma->paddr = enclave_run_param.free_page;
+  pma->size = enclave_run_param.size;
+  pma->free_mem = enclave_run_param.free_page + 2*RISCV_PGSIZE;
+  initilze_va_struct(pma, vma, enclave);
+
+  if(enclave_run_param.kbuffer_size < RISCV_PGSIZE || enclave_run_param.kbuffer & (RISCV_PGSIZE-1) || enclave_run_param.kbuffer_size & (RISCV_PGSIZE-1))
+  {
+    retval = ENCLAVE_ERROR;
+    goto run_enclave_out;
+  }
+  mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), ENCLAVE_DEFAULT_KBUFFER, enclave_run_param.kbuffer, enclave_run_param.kbuffer_size);
+
+  //check shm
+  if(enclave_run_param.shm_paddr && enclave_run_param.shm_size &&
+      !(enclave_run_param.shm_paddr & (RISCV_PGSIZE-1)) && !(enclave_run_param.shm_size & (RISCV_PGSIZE-1)))
+  {
+    mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), ENCLAVE_DEFAULT_SHM_BASE, enclave_run_param.shm_paddr, enclave_run_param.shm_size);
+    enclave->shm_paddr = enclave_run_param.shm_paddr;
+    enclave->shm_size = enclave_run_param.shm_size;
+  }
+  else
+  {
+    enclave->shm_paddr = 0;
+    enclave->shm_size = 0;
+  }
+
+  copy_word_to_host((unsigned int*)enclave_run_param.eid_ptr, enclave->eid);
+
+  //map the relay page
+  if((retval =map_relay_page(eid, mm_arg_addr, mm_arg_size, &mmap_offset, enclave, relay_page_entry)) < 0)
+  {
+    if (retval == ENCLAVE_NO_MEM)
+      goto failed;
+    else
+      goto run_enclave_out;
+  }
+
+  if(swap_from_host_to_enclave(regs, enclave) < 0)
+  {
+    sbi_bug("M mode: run_shadow_enclave: enclave can not be run\n");
+    retval = -1UL;
+    goto run_enclave_out;
+  }
+
+  //set return address to enclave
+  csr_write(CSR_MEPC, (uintptr_t)(enclave->entry_point));
+
+  //enable timer interrupt
+  csr_read_set(CSR_MIE, MIP_MTIP);
+  csr_read_set(CSR_MIE, MIP_MSIP);
+
+  //set default stack
+  regs[2] = ENCLAVE_DEFAULT_STACK_BASE;
+
+  //pass parameters
+  if(enclave->shm_paddr)
+    regs[10] = ENCLAVE_DEFAULT_SHM_BASE;
+  else
+    regs[10] = 0;
+  retval = regs[10];
+  regs[11] = enclave->shm_size;
+  regs[12] = (eapp_args) % 5;
+  if(enclave->mm_arg_paddr[0])
+    regs[13] = ENCLAVE_DEFAULT_MM_ARG_BASE;
+  else
+    regs[13] = 0;
+  regs[14] = mmap_offset;
+  eapp_args = eapp_args+1;
+
+  enclave->state = RUNNING;
+  sbi_printf("M mode: run shadow enclave...\n");
+
+run_enclave_out:
+  release_enclave_metadata_lock();
+  return retval;
+
+failed:
+  if(need_free_secure_memory)
+  {
+    free_secure_memory(enclave_run_param.free_page, enclave_run_param.size);
+    sbi_memset((void *)enclave_run_param.free_page, 0, enclave_run_param.size);
+  }
+  
+  if(enclave)
+    __free_enclave(enclave->eid);
+  
+  release_enclave_metadata_lock();
+  return retval;
+}
+
+
+uintptr_t attest_enclave(uintptr_t eid, uintptr_t report_ptr, uintptr_t nonce)
+{
+  struct enclave_t* enclave = NULL;
+  int attestable = 1;
+  struct report_t report;
+  enclave_state_t old_state = INVALID;
+  acquire_enclave_metadata_lock();
+  enclave = __get_enclave(eid);
+  if(!enclave || (enclave->state != FRESH && enclave->state != STOPPED)
+    || enclave->host_ptbr != csr_read(CSR_SATP))
+    attestable = 0;
+  else
+  {
+    old_state = enclave->state;
+    enclave->state = ATTESTING;
+  }
+  release_enclave_metadata_lock();
+
+  if(!attestable)
+  {
+    sbi_printf("M mode: attest_enclave: enclave%ld is not attestable\r\n", eid);
+    return -1UL;
+  }
+
+  sbi_memcpy((void*)(report.dev_pub_key), (void*)DEV_PUB_KEY, PUBLIC_KEY_SIZE);
+  sbi_memcpy((void*)(report.sm.hash), (void*)SM_HASH, HASH_SIZE);
+  sbi_memcpy((void*)(report.sm.sm_pub_key), (void*)SM_PUB_KEY, PUBLIC_KEY_SIZE);
+  sbi_memcpy((void*)(report.sm.signature), (void*)SM_SIGNATURE, SIGNATURE_SIZE);
+
+  hash_enclave(enclave, (void*)(report.enclave.hash), nonce);
+  sign_enclave((void*)(report.enclave.signature), (void*)(report.enclave.hash));
+  report.enclave.nonce = nonce;
+
+  //printHex((unsigned char*)(report.enclave.signature), 64);
+
+  copy_to_host((void*)report_ptr, (void*)(&report), sizeof(struct report_t));
+
+  acquire_enclave_metadata_lock();
+  enclave->state = old_state;
+  release_enclave_metadata_lock();
+  return 0;
+}
+
+uintptr_t attest_shadow_enclave(uintptr_t eid, uintptr_t report_ptr, uintptr_t nonce)
+{
+  struct shadow_enclave_t* shadow_enclave = NULL;
+  int attestable = 1;
+  struct report_t report;
+  acquire_enclave_metadata_lock();
+  shadow_enclave = __get_shadow_enclave(eid);
+  release_enclave_metadata_lock();
+
+  if(!attestable)
+  {
+    sbi_printf("M mode: attest_enclave: enclave%ld is not attestable\r\n", eid);
+    return -1UL;
+  }
+  update_hash_shadow_enclave(shadow_enclave, (char *)shadow_enclave->hash, nonce);
+  sbi_memcpy((char *)(report.enclave.hash), (char *)shadow_enclave->hash, HASH_SIZE);
+  sbi_memcpy((void*)(report.dev_pub_key), (void*)DEV_PUB_KEY, PUBLIC_KEY_SIZE);
+  sbi_memcpy((void*)(report.sm.hash), (void*)SM_HASH, HASH_SIZE);
+  sbi_memcpy((void*)(report.sm.sm_pub_key), (void*)SM_PUB_KEY, PUBLIC_KEY_SIZE);
+  sbi_memcpy((void*)(report.sm.signature), (void*)SM_SIGNATURE, SIGNATURE_SIZE);
+  sign_enclave((void*)(report.enclave.signature), (void*)(report.enclave.hash));
+  report.enclave.nonce = nonce;
+
+  copy_to_host((void*)report_ptr, (void*)(&report), sizeof(struct report_t));
+
+  return 0;
+}
+
+/**
+ * \brief host use this function to wake a stopped enclave.
+ * 
+ * \param regs The host reg need to saved.
+ * \param eid The given enclave id. 
+ */
+uintptr_t wake_enclave(uintptr_t* regs, unsigned int eid)
+{
+  uintptr_t retval = 0;
+  struct enclave_t* enclave = NULL;
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_real_enclave(eid);
+  if(!enclave || enclave->state != STOPPED || enclave->host_ptbr != csr_read(CSR_SATP))
+  {
+    sbi_bug("M mode: wake_enclave: enclave%d can not be accessed!\n", eid);
+    retval = -1UL;
+    goto wake_enclave_out;
+  }
+
+  enclave->state = RUNNABLE;
+
+wake_enclave_out:
+  release_enclave_metadata_lock();
+  return retval;
+}
+
+/**
+ * \brief Resume the enclave from the previous status.
+ * 
+ * \param regs The host reg need to saved.
+ * \param eid The given enclave id. 
+ */
+uintptr_t resume_enclave(uintptr_t* regs, unsigned int eid)
+{
+  uintptr_t retval = 0;
+  struct enclave_t* enclave = NULL;
+
+  acquire_enclave_metadata_lock();
+  enclave = __get_real_enclave(eid);
+  if(!enclave || enclave->state <= FRESH || enclave->host_ptbr != csr_read(CSR_SATP))
+  {
+    sbi_bug("M mode: resume_enclave: enclave%d can not be accessed\n", eid);
+    retval = -1UL;
+    goto resume_enclave_out;
+  }
+
+  if(enclave->state == STOPPED)
+  {
+    sbi_bug("M mode: resume_enclave: enclave%d is stopped\n", eid);
+    retval = ENCLAVE_TIMER_IRQ;
+    goto resume_enclave_out;
+  }
+  if(enclave->state != RUNNABLE)
+  {
+    sbi_bug("M mode: resume_enclave: enclave%d is not runnable\n", eid);
+    retval = -1UL;
+    goto resume_enclave_out;
+  }
+
+  if(swap_from_host_to_enclave(regs, enclave) < 0)
+  {
+    sbi_bug("M mode: resume_enclave: enclave can not be resume\n");
+    retval = -1UL;
+    goto resume_enclave_out;
+  }
+  enclave->state = RUNNING;
+  // regs[10] will be set to retval when mcall_trap return, so we have to
+  // set retval to be regs[10] here to succuessfully restore context
+  retval = regs[10];
+resume_enclave_out:
+  release_enclave_metadata_lock();
+  return retval;
+}
+
+/**
+ * \brief Map the memory for ocall return.
+ * 
+ * \param enclave The enclave structure.
+ * \param paddr The mapped physical address.
+ * \param size The mapped memory size.
+ */
+uintptr_t mmap_after_resume(struct enclave_t *enclave, uintptr_t paddr, uintptr_t size)
+{
+  uintptr_t retval = 0;
+  //uintptr_t vaddr = ENCLAVE_DEFAULT_MMAP_BASE;
+  uintptr_t vaddr = enclave->thread_context.prev_state.a1;
+  if(!vaddr) vaddr = ENCLAVE_DEFAULT_MMAP_BASE - (size - RISCV_PGSIZE);
+  if(check_and_set_secure_memory(paddr, size) < 0)
+  {
+    sbi_bug("M mode: mmap_after_resume: check_secure_memory(0x%lx, 0x%lx) failed\n", paddr, size);
+    retval = -1UL;
+    return retval;
+  }
+
+  //Sync and flush the remote TLB entry.
+  tlb_remote_sfence();
+
+  struct pm_area_struct *pma = (struct pm_area_struct*)paddr;
+  struct vm_area_struct *vma = (struct vm_area_struct*)(paddr + sizeof(struct pm_area_struct));
+  pma->paddr = paddr;
+  pma->size = size;
+  pma->pm_next = NULL;
+  //vma->va_start = vaddr - (size - RISCV_PGSIZE);
+  //vma->va_end = vaddr;
+  vma->va_start = vaddr;
+  vma->va_end = vaddr + size - RISCV_PGSIZE;
+  vma->vm_next = NULL;
+  vma->pma = pma;
+  if(insert_vma(&(enclave->mmap_vma), vma, ENCLAVE_DEFAULT_MMAP_BASE) < 0)
+  {
+    vma->va_end = enclave->mmap_vma->va_start;
+    vma->va_start = vma->va_end - (size - RISCV_PGSIZE);
+    vma->vm_next = enclave->mmap_vma;
+    enclave->mmap_vma = vma;
+  }
+  insert_pma(&(enclave->pma_list), pma);
+  mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), vma->va_start, paddr+RISCV_PGSIZE, size-RISCV_PGSIZE);
+  retval = vma->va_start;
+  
+  return retval;
+}
+
+/**
+ * \brief Map the sbrk memory for ocall return.
+ * 
+ * \param enclave The enclave structure.
+ * \param paddr The mapped physical address.
+ * \param size The mapped memory size. 
+ */
+uintptr_t sbrk_after_resume(struct enclave_t *enclave, uintptr_t paddr, uintptr_t size)
+{
+  uintptr_t retval = 0;
+  intptr_t req_size = (intptr_t)(enclave->thread_context.prev_state.a1);
+  if(req_size <= 0)
+  {
+    return enclave->_heap_top;
+  }
+  if(check_and_set_secure_memory(paddr, size) < 0)
+  {
+    retval = -1UL;
+    sbi_bug("M mode: sbrk_after_resume: check and set the secure memory is failed \n");
+    return retval;
+  }
+
+  //Sync and flush the remote TLB entry.
+  tlb_remote_sfence();
+  
+  struct pm_area_struct *pma = (struct pm_area_struct*)paddr;
+  struct vm_area_struct *vma = (struct vm_area_struct*)(paddr + sizeof(struct pm_area_struct));
+  pma->paddr = paddr;
+  pma->size = size;
+  pma->pm_next = NULL;
+  vma->va_start = enclave->_heap_top;
+  vma->va_end = vma->va_start + size - RISCV_PGSIZE;
+  vma->vm_next = NULL;
+  vma->pma = pma;
+  vma->vm_next = enclave->heap_vma;
+  enclave->heap_vma = vma;
+  enclave->_heap_top = vma->va_end;
+  insert_pma(&(enclave->pma_list), pma);
+  mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), vma->va_start, paddr+RISCV_PGSIZE, size-RISCV_PGSIZE);
+  retval = enclave->_heap_top;
+
+  return retval;
+}
+
+/**
+ * \brief Map the relay page for ocall return.
+ * 
+ * \param enclave The enclave structure.
+ * \param mm_arg_addr Relay page address.
+ * \param mm_arg_size Relay page size.
+ */
+uintptr_t return_relay_page_after_resume(struct enclave_t *enclave, uintptr_t mm_arg_addr, uintptr_t mm_arg_size)
+{
+  uintptr_t retval = 0, mmap_offset = 0;
+  if((retval =map_relay_page(enclave->eid, mm_arg_addr, mm_arg_size, &mmap_offset, enclave, NULL)) < 0)
+  {
+    if (retval == ENCLAVE_NO_MEM)
+      goto run_enclave_out;
+    else
+      goto run_enclave_out;
+  }
+
+run_enclave_out:
+  return retval;
+}
+
+/**
+ * \brief Host use this fucntion to re-enter enclave world.
+ * 
+ * \param regs The host register context.
+ * \param eid Resume enclave id.
+ */
+uintptr_t resume_from_ocall(uintptr_t* regs, unsigned int eid)
+{
+  uintptr_t retval = 0;
+  uintptr_t ocall_func_id = regs[12];
+  struct enclave_t* enclave = NULL;
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_real_enclave(eid);
+  if(!enclave || enclave->state != OCALLING || enclave->host_ptbr != csr_read(CSR_SATP))
+  {
+    retval = -1UL;
+    goto out;
+  }
+
+  switch(ocall_func_id)
+  {
+    case OCALL_MMAP:
+      retval = mmap_after_resume(enclave, regs[13], regs[14]);
+      if(retval == -1UL)
+        goto out;
+      break;
+    case OCALL_UNMAP:
+      retval = 0;
+      break;
+    case OCALL_SYS_WRITE:
+      retval = enclave->thread_context.prev_state.a0;
+      break;
+    case OCALL_SBRK:
+      retval = sbrk_after_resume(enclave, regs[13], regs[14]);
+      if(retval == -1UL)
+        goto out;
+      break;
+    case OCALL_READ_SECT:
+      retval = regs[13];
+      break;
+    case OCALL_WRITE_SECT:
+      retval = regs[13];
+      break;
+    case OCALL_RETURN_RELAY_PAGE:
+      retval = return_relay_page_after_resume(enclave, regs[13], regs[14]);
+      if(retval == -1UL)
+        goto out;
+      break;
+    default:
+      retval = 0;
+      break;
+  }
+
+  if(swap_from_host_to_enclave(regs, enclave) < 0)
+  {
+    retval = -1UL;
+    goto out;
+  }
+  enclave->state = RUNNING;
+
+out:
+  release_enclave_metadata_lock();
+  return retval;
+}
+
+/**
+ * \brief Host calls this function to destroy an existing enclave.
+ * 
+ * \param regs The host register context.
+ * \param eid Resume enclave id.
+ */
+uintptr_t destroy_enclave(uintptr_t* regs, unsigned int eid)
+{
+  uintptr_t retval = 0;
+  struct enclave_t *enclave = NULL;
+  uintptr_t dest_hart = 0;
+  struct pm_area_struct* pma = NULL;
+  int need_free_enclave_memory = 0;
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_enclave(eid);
+  unsigned long mm_arg_paddr[RELAY_PAGE_NUM];
+  unsigned long mm_arg_size[RELAY_PAGE_NUM];
+  for(int kk = 0; kk < RELAY_PAGE_NUM; kk++)
+  {
+    mm_arg_paddr[kk] = enclave->mm_arg_paddr[kk];
+    mm_arg_size[kk] = enclave->mm_arg_size[kk];
+  }
+  if(!enclave || enclave->state < FRESH || enclave->type == SERVER_ENCLAVE)
+  {
+    sbi_bug("M mode: destroy_enclave: enclave%d can not be accessed\r\n", eid);
+    retval = -1UL;
+    goto destroy_enclave_out;
+  }
+
+  if(enclave->state != RUNNING)
+  {
+    pma = enclave->pma_list;
+    need_free_enclave_memory = 1;
+    __free_enclave(eid);
+  }
+  else
+  {
+    //cpus' state will be protected by enclave_metadata_lock
+    for(int i = 0; i < MAX_HARTS; ++i)
+    {
+      if(cpus[i].in_enclave && cpus[i].eid == eid)
+        dest_hart = i;
+    }
+    if (dest_hart == csr_read(CSR_MHARTID))
+      ipi_destroy_enclave(regs, csr_read(CSR_SATP), eid);
+    else
+      set_ipi_destroy_enclave_and_sync(dest_hart, csr_read(CSR_SATP), eid);
+  }
+
+destroy_enclave_out:
+  release_enclave_metadata_lock();
+
+  //should wait after release enclave_metadata_lock to avoid deadlock
+  if(need_free_enclave_memory)
+  {
+    free_enclave_memory(pma);
+    free_all_relay_page(mm_arg_paddr, mm_arg_size);
+  }
+
+  return retval;
+}
+
+/**************************************************************/
+/*                   called by enclave                        */
+/**************************************************************/
+/**
+ * \brief Exit from the enclave.
+ * 
+ * \param regs The host register context.
+ * \param enclave_retval Enclave return value.
+ */
+uintptr_t exit_enclave(uintptr_t* regs, unsigned long enclave_retval)
+{
+  struct enclave_t *enclave = NULL;
+  int eid = 0;
+  uintptr_t ret = 0;
+  struct pm_area_struct *pma = NULL;
+  int need_free_enclave_memory = 0;
+  if(check_in_enclave_world() < 0)
+  {
+    sbi_bug("M mode: exit_enclave: cpu is not in enclave world now\n");
+    return -1UL;
+  }
+
+  acquire_enclave_metadata_lock();
+
+  eid = get_curr_enclave_id();
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave) != 0 || enclave->type == SERVER_ENCLAVE)
+  {
+    sbi_bug("M mode: exit_enclave: enclave%d can not be accessed!\n", eid);
+    ret = -1UL;
+    goto exit_enclave_out;
+  }
+  swap_from_enclave_to_host(regs, enclave);
+
+  pma = enclave->pma_list;
+  need_free_enclave_memory = 1;
+  unsigned long mm_arg_paddr[RELAY_PAGE_NUM];
+  unsigned long mm_arg_size[RELAY_PAGE_NUM];
+  for(int kk = 0; kk < RELAY_PAGE_NUM; kk++)
+  {
+    mm_arg_paddr[kk] = enclave->mm_arg_paddr[kk];
+    mm_arg_size[kk] = enclave->mm_arg_size[kk];
+  }
+  __free_enclave(eid);
+
+exit_enclave_out:
+
+  if(need_free_enclave_memory)
+  {
+    free_enclave_memory(pma);
+    free_all_relay_page(mm_arg_paddr, mm_arg_size);
+  }
+  release_enclave_metadata_lock();
+  return ret;
+}
+
+/**
+ * \brief Enclave needs to map a new mmap region, ocall to the host to handle.
+ * 
+ * \param regs The enclave register context.
+ * \param vaddr Mmap virtual address.
+ * \param suze Mmap virtual memory size.
+ */
+uintptr_t enclave_mmap(uintptr_t* regs, uintptr_t vaddr, uintptr_t size)
+{
+  uintptr_t ret = 0;
+  int eid = get_curr_enclave_id();
+  struct enclave_t* enclave = NULL;
+  if(check_in_enclave_world() < 0)
+    return -1;
+  if(vaddr)
+  {
+    if(vaddr & (RISCV_PGSIZE-1) || size < RISCV_PGSIZE || size & (RISCV_PGSIZE-1))
+      return -1;
+  }
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave)!=0 || enclave->state != RUNNING)
+  {
+    ret = -1UL;
+    goto out;
+  }
+
+  copy_dword_to_host((uintptr_t*)enclave->ocall_func_id, OCALL_MMAP);
+  copy_dword_to_host((uintptr_t*)enclave->ocall_arg1, size + RISCV_PGSIZE);
+
+  swap_from_enclave_to_host(regs, enclave);
+  enclave->state = OCALLING;
+  ret = ENCLAVE_OCALL;
+
+out:
+  release_enclave_metadata_lock();
+  return ret;
+}
+
+/**
+ * \brief Enclave needs to unmap a mmap region, ocall to the host to handle.
+ * 
+ * \param regs The enclave register context.
+ * \param vaddr Unmap virtual address.
+ * \param suze Unmap virtual memory size.
+ */
+uintptr_t enclave_unmap(uintptr_t* regs, uintptr_t vaddr, uintptr_t size)
+{
+  uintptr_t ret = 0;
+  int eid = get_curr_enclave_id();
+  struct enclave_t* enclave = NULL;
+  struct vm_area_struct *vma = NULL;
+  struct pm_area_struct *pma = NULL;
+  int need_free_secure_memory = 0;
+  if(check_in_enclave_world() < 0)
+    return -1;
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave)!=0 || enclave->state != RUNNING)
+  {
+    ret = -1UL;
+    goto out;
+  }
+
+  vma = find_vma(enclave->mmap_vma, vaddr, size);
+  if(!vma)
+  {
+    ret = -1UL;
+    goto out;
+  }
+  pma = vma->pma;
+  delete_vma(&(enclave->mmap_vma), vma);
+  delete_pma(&(enclave->pma_list), pma);
+  vma->vm_next = NULL;
+  pma->pm_next = NULL;
+  unmap((uintptr_t*)(enclave->root_page_table), vma->va_start, vma->va_end - vma->va_start);
+  need_free_secure_memory = 1;
+
+  copy_dword_to_host((uintptr_t*)enclave->ocall_func_id, OCALL_UNMAP);
+  copy_dword_to_host((uintptr_t*)enclave->ocall_arg0, pma->paddr);
+  copy_dword_to_host((uintptr_t*)enclave->ocall_arg1, pma->size);
+
+  swap_from_enclave_to_host(regs, enclave);
+  enclave->state = OCALLING;
+  ret = ENCLAVE_OCALL;
+
+out:
+  release_enclave_metadata_lock();
+  if(need_free_secure_memory)
+  {
+    free_enclave_memory(pma);
+  }
+  return ret;
+}
+
+/**
+ * \brief Enclave calls sbrk() in the runtime, ocall to the host to handle.
+ * 
+ * \param regs The enclave register context.
+ * \param size Stack augment memory size.
+ */
+uintptr_t enclave_sbrk(uintptr_t* regs, intptr_t size)
+{
+  uintptr_t ret = 0;
+  uintptr_t abs_size = 0;
+  int eid = get_curr_enclave_id();
+  struct enclave_t* enclave = NULL;
+  struct pm_area_struct *pma = NULL;
+  struct vm_area_struct *vma = NULL;
+  if(check_in_enclave_world() < 0)
+    return -1;
+  if(size < 0)
+  {
+    abs_size = 0 - size;
+  }
+  else
+  {
+    abs_size = size;
+  }
+  if(abs_size & (RISCV_PGSIZE-1))
+    return -1;
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave)!=0 || enclave->state != RUNNING)
+  {
+    ret = -1UL;
+    goto out;
+  }
+
+  if(size == 0)
+  {
+    ret = enclave->_heap_top;
+    goto out;
+  }
+  if(size < 0)
+  {
+    uintptr_t dest_va = enclave->_heap_top - abs_size;
+    vma = enclave->heap_vma;
+    while(vma && vma->va_start >= dest_va)
+    {
+      struct pm_area_struct *cur_pma = vma->pma;
+      delete_pma(&(enclave->pma_list), cur_pma);
+      cur_pma->pm_next = pma;
+      pma = cur_pma;
+      unmap((uintptr_t*)(enclave->root_page_table), vma->va_start, vma->va_end - vma->va_start);
+      enclave->heap_vma = vma->vm_next;
+      vma = vma->vm_next;
+    }
+    if(enclave->heap_vma)
+      enclave->_heap_top = enclave->heap_vma->va_end;
+    else
+      enclave->_heap_top = ENCLAVE_DEFAULT_HEAP_BASE;
+  }
+  copy_dword_to_host((uintptr_t*)enclave->ocall_func_id, OCALL_SBRK);
+  copy_dword_to_host((uintptr_t*)enclave->ocall_arg0, (uintptr_t)pma);
+  if(size > 0)
+    copy_dword_to_host((uintptr_t*)enclave->ocall_arg1, size + RISCV_PGSIZE);
+  else
+    copy_dword_to_host((uintptr_t*)enclave->ocall_arg1, size);
+
+  swap_from_enclave_to_host(regs, enclave);
+  enclave->state = OCALLING;
+  ret = ENCLAVE_OCALL;
+
+out:
+  release_enclave_metadata_lock();
+  if(pma)
+  {
+    free_enclave_memory(pma);
+  }
+  return ret;
+}
+
+/**
+ * \brief Enclave calls print() in the runtime, ocall to the host to handle.
+ * 
+ * \param regs The enclave register context.
+ */
+uintptr_t enclave_sys_write(uintptr_t* regs)
+{
+  uintptr_t ret = 0;
+  int eid = get_curr_enclave_id();
+  struct enclave_t* enclave = NULL;
+  if(check_in_enclave_world() < 0) 
+  {
+    sbi_bug("M mode: %s check enclave world is failed\n", __func__);
+    return -1;
+  }
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave)!=0 || enclave->state != RUNNING)
+  {
+    ret = -1UL;
+    sbi_bug("M mode: %s check enclave authentication is failed\n", __func__);
+    goto out;
+  }
+  copy_dword_to_host((uintptr_t*)enclave->ocall_func_id, OCALL_SYS_WRITE);
+
+  swap_from_enclave_to_host(regs, enclave);
+  enclave->state = OCALLING;
+  ret = ENCLAVE_OCALL;
+out:
+  release_enclave_metadata_lock();
+  return ret;
+}
+
+/**
+ * \brief Call the server enclave, and transfer the relay page ownership.
+ * 
+ * \param regs The enclave register context.
+ * \param callee_eid The callee enclave id.
+ * \param arg The passing arguments.
+ */
+uintptr_t call_enclave(uintptr_t* regs, unsigned int callee_eid, uintptr_t arg)
+{
+  struct enclave_t* top_caller_enclave = NULL;
+  struct enclave_t* caller_enclave = NULL;
+  struct enclave_t* callee_enclave = NULL;
+  struct vm_area_struct* vma = NULL;
+  struct pm_area_struct* pma = NULL;
+  uintptr_t retval = 0;
+  int caller_eid = get_curr_enclave_id();
+  if(check_in_enclave_world() < 0)
+    return -1;
+
+  acquire_enclave_metadata_lock();
+  caller_enclave = __get_enclave(caller_eid);
+  if(!caller_enclave || caller_enclave->state != RUNNING || check_enclave_authentication(caller_enclave) != 0)
+  {
+    sbi_bug("M mode: call_enclave: enclave%d can not execute call_enclave!\n", caller_eid);
+    retval = -1UL;
+    goto out;
+  }
+  if(caller_enclave->caller_eid != -1)
+    top_caller_enclave = __get_enclave(caller_enclave->top_caller_eid);
+  else
+    top_caller_enclave = caller_enclave;
+  if(!top_caller_enclave || top_caller_enclave->state != RUNNING)
+  {
+    sbi_bug("M mode: call_enclave: enclave%d can not execute call_enclave!\n", caller_eid);
+    retval = -1UL;
+    goto out;
+  }
+  callee_enclave = __get_enclave(callee_eid);
+  if(!callee_enclave || callee_enclave->type != SERVER_ENCLAVE || callee_enclave->caller_eid != -1 || callee_enclave->state != RUNNABLE)
+  {
+    sbi_bug("M mode: call_enclave: enclave%d can not be accessed!\n", callee_eid);
+    retval = -1UL;
+    goto out;
+  }
+
+  struct call_enclave_arg_t call_arg;
+  struct call_enclave_arg_t* call_arg0 = va_to_pa((uintptr_t*)(caller_enclave->root_page_table), (void*)arg);
+  if(!call_arg0)
+  {
+    sbi_bug("M mode: call_enclave: call_arg0 is not existed \n");
+    retval = -1UL;
+    goto out;
+  }
+  copy_from_host(&call_arg, call_arg0, sizeof(struct call_enclave_arg_t));
+  if(call_arg.req_vaddr != 0)
+  {
+    if(call_arg.req_vaddr & (RISCV_PGSIZE-1) || call_arg.req_size < RISCV_PGSIZE || call_arg.req_size & (RISCV_PGSIZE-1))
+    {
+      sbi_bug("M mode: call_enclave: vaddr and size is not align \n");
+      retval = -1UL;
+      goto out;
+    }
+
+    if(call_arg.req_vaddr == ENCLAVE_DEFAULT_MM_ARG_BASE)
+    {
+      callee_enclave->mm_arg_paddr[0] = caller_enclave->mm_arg_paddr[0];
+      callee_enclave->mm_arg_size[0] = caller_enclave->mm_arg_size[0];
+      caller_enclave->mm_arg_paddr[0] = 0;
+      caller_enclave->mm_arg_paddr[0] = 0;
+      unmap((uintptr_t*)(caller_enclave->root_page_table), call_arg.req_vaddr, call_arg.req_size);
+      mmap((uintptr_t*)(callee_enclave->root_page_table), &(callee_enclave->free_pages), ENCLAVE_DEFAULT_MM_ARG_BASE, callee_enclave->mm_arg_paddr[0], call_arg.req_size);
+    }
+    else
+    {
+      //Unmap for caller enclave
+      vma = find_vma(caller_enclave->mmap_vma, call_arg.req_vaddr, call_arg.req_size);
+      if(!vma)
+      {
+        sbi_bug("M mode: call_enclave:vma is not existed \n");
+        retval = -1UL;
+        goto out;
+      }
+      pma = vma->pma;
+      delete_vma(&(caller_enclave->mmap_vma), vma);
+      delete_pma(&(caller_enclave->pma_list), pma);
+      vma->vm_next = NULL;
+      pma->pm_next = NULL;
+      unmap((uintptr_t*)(caller_enclave->root_page_table), vma->va_start, vma->va_end - vma->va_start);
+      //Map for callee enclave
+      if(insert_vma(&(callee_enclave->mmap_vma), vma, ENCLAVE_DEFAULT_MMAP_BASE) < 0)
+      {
+        vma->va_end = callee_enclave->mmap_vma->va_start;
+        vma->va_start = vma->va_end - (pma->size - RISCV_PGSIZE);
+        vma->vm_next = callee_enclave->mmap_vma;
+        callee_enclave->mmap_vma = vma;
+      }
+      insert_pma(&(callee_enclave->pma_list), pma);
+      mmap((uintptr_t*)(callee_enclave->root_page_table), &(callee_enclave->free_pages), vma->va_start, pma->paddr + RISCV_PGSIZE, pma->size - RISCV_PGSIZE);
+    }
+  }
+  if(__enclave_call(regs, top_caller_enclave, caller_enclave, callee_enclave) < 0)
+  {
+    sbi_bug("M mode: call_enclave: enclave can not be run\n");
+    retval = -1UL;
+    goto out;
+  }
+  //set return address to enclave
+  csr_write(CSR_MEPC, (uintptr_t)(callee_enclave->entry_point));
+
+  //enable timer interrupt
+  csr_read_set(CSR_MIE, MIP_MTIP);
+  csr_read_set(CSR_MIE, MIP_MSIP);
+
+  //set default stack
+  regs[2] = ENCLAVE_DEFAULT_STACK_BASE;
+
+  //map kbuffer
+  mmap((uintptr_t*)(callee_enclave->root_page_table), &(callee_enclave->free_pages), ENCLAVE_DEFAULT_KBUFFER, top_caller_enclave->kbuffer, top_caller_enclave->kbuffer_size);
+  //pass parameters
+  
+  regs[10] = call_arg.req_arg;
+  if(call_arg.req_vaddr == ENCLAVE_DEFAULT_MM_ARG_BASE)
+    regs[11] = ENCLAVE_DEFAULT_MM_ARG_BASE;
+  else if(call_arg.req_vaddr)
+    regs[11] = vma->va_start;
+  else
+    regs[11] = 0;
+  regs[12] = call_arg.req_size;
+  if(callee_enclave->shm_paddr){
+    regs[13] = ENCLAVE_DEFAULT_SHM_BASE;
+  }
+  else{
+    regs[13] = 0;
+  }
+  regs[14] = callee_enclave->shm_size;
+  retval = call_arg.req_arg;
+
+  callee_enclave->state = RUNNING;
+out:
+  release_enclave_metadata_lock();
+  return retval;
+}
+
+/**
+ * \brief Server enclave return, and transfer the relay page ownership.
+ * 
+ * \param regs The enclave register context.
+ * \param arg The return arguments.
+ */
+uintptr_t enclave_return(uintptr_t* regs, uintptr_t arg)
+{
+  struct enclave_t *enclave = NULL;
+  struct enclave_t *caller_enclave = NULL;
+  struct enclave_t *top_caller_enclave = NULL;
+  int eid = 0;
+  uintptr_t ret = 0;
+  struct vm_area_struct* vma = NULL;
+  struct pm_area_struct *pma = NULL;
+
+  if(check_in_enclave_world() < 0)
+  {
+    sbi_bug("M mode: enclave_return: cpu is not in enclave world now\n");
+    return -1UL;
+  }
+
+  acquire_enclave_metadata_lock();
+
+  eid = get_curr_enclave_id();
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave) != 0 || enclave->type != SERVER_ENCLAVE)
+  {
+    sbi_bug("M mode: enclave_return: enclave%d can not return!\n", eid);
+    ret = -1UL;
+    goto out;
+  }
+  struct call_enclave_arg_t ret_arg;
+  struct call_enclave_arg_t* ret_arg0 = va_to_pa((uintptr_t*)(enclave->root_page_table), (void*)arg);
+  if(!ret_arg0)
+  {
+    sbi_bug("M mode: enclave_return: ret_arg0 is invalid \n");
+    ret = -1UL;
+    goto out;
+  }
+  copy_from_host(&ret_arg, ret_arg0, sizeof(struct call_enclave_arg_t));
+
+  caller_enclave = __get_enclave(enclave->caller_eid);
+  top_caller_enclave = __get_enclave(enclave->top_caller_eid);
+  __enclave_return(regs, enclave, caller_enclave, top_caller_enclave);
+  unmap((uintptr_t*)(enclave->root_page_table), ENCLAVE_DEFAULT_KBUFFER, top_caller_enclave->kbuffer_size);
+
+  //restore caller_enclave's req arg
+  //there is no need to check call_arg's validity again as it is already checked when executing call_enclave()
+  struct call_enclave_arg_t *call_arg = va_to_pa((uintptr_t*)(caller_enclave->root_page_table), (void*)(regs[11]));
+
+  //restore req_vaddr
+  if(!call_arg->req_vaddr || !ret_arg.req_vaddr || ret_arg.req_vaddr & (RISCV_PGSIZE-1)
+      || ret_arg.req_size < call_arg->req_size || ret_arg.req_size & (RISCV_PGSIZE-1))
+  {
+    call_arg->req_vaddr = 0;
+    sbi_printf("M MODE: enclave return: the ret argument is in-consistent with caller argument\n");
+    sbi_bug("M MODE: enclave return: call_arg->req_vaddr %lx call_arg->req_size %lx ret_arg->req_vaddr %lx ret_arg->size %lx\n", call_arg->req_vaddr, call_arg->req_size, ret_arg.req_vaddr, ret_arg.req_size);
+    goto restore_resp_addr;
+  }
+  //Remap for caller enclave
+  if(call_arg->req_vaddr == ENCLAVE_DEFAULT_MM_ARG_BASE)
+  {
+    caller_enclave->mm_arg_paddr[0] = enclave->mm_arg_paddr[0];
+    caller_enclave->mm_arg_size[0] = enclave->mm_arg_size[0];
+    enclave->mm_arg_paddr[0] = 0;
+    enclave->mm_arg_paddr[0] = 0;
+    unmap((uintptr_t*)(enclave->root_page_table), call_arg->req_vaddr, call_arg->req_size);
+    mmap((uintptr_t*)(caller_enclave->root_page_table), &(caller_enclave->free_pages), call_arg->req_vaddr, caller_enclave->mm_arg_paddr[0], call_arg->req_size);
+  }
+  else
+  {
+    vma = find_vma(enclave->mmap_vma, ret_arg.req_vaddr, ret_arg.req_size);
+    if(!vma)
+    {
+      //enclave return even when the shared mem return failed
+      call_arg->req_vaddr = 0;
+      sbi_bug("M MODE: enclave return: can not find the corresponding vma for callee enclave\n");
+      goto restore_resp_addr;
+    }
+    pma = vma->pma;
+    delete_vma(&(enclave->mmap_vma), vma);
+    delete_pma(&(enclave->pma_list), pma);
+    unmap((uintptr_t*)(enclave->root_page_table), vma->va_start, vma->va_end - vma->va_start);
+    vma->va_start = call_arg->req_vaddr;
+    vma->va_end = vma->va_start + pma->size - RISCV_PGSIZE;
+    vma->vm_next = NULL;
+    pma->pm_next = NULL;
+    if(insert_vma(&(caller_enclave->mmap_vma), vma, ENCLAVE_DEFAULT_MMAP_BASE) < 0)
+    {
+      vma->va_end = caller_enclave->mmap_vma->va_start;
+      vma->va_start = vma->va_end - (pma->size - RISCV_PGSIZE);
+      vma->vm_next = caller_enclave->mmap_vma;
+      caller_enclave->mmap_vma = vma;
+    }
+    insert_pma(&(caller_enclave->pma_list), pma);
+    mmap((uintptr_t*)(caller_enclave->root_page_table), &(caller_enclave->free_pages), vma->va_start, pma->paddr + RISCV_PGSIZE, pma->size - RISCV_PGSIZE);
+    call_arg->req_vaddr = vma->va_start;
+  }
+
+restore_resp_addr:
+  if(!ret_arg.resp_vaddr || ret_arg.resp_vaddr & (RISCV_PGSIZE-1)
+      || ret_arg.resp_size < RISCV_PGSIZE || ret_arg.resp_size & (RISCV_PGSIZE-1))
+  {
+    call_arg->resp_vaddr = 0;
+    call_arg->resp_size = 0;
+    goto restore_return_val;
+  }
+  vma = find_vma(enclave->mmap_vma, ret_arg.resp_vaddr, ret_arg.resp_size);
+  if(!vma)
+  {
+    //enclave return even when the shared mem return failed
+    call_arg->resp_vaddr = 0;
+    call_arg->resp_size = 0;
+    goto restore_return_val;
+  }
+  pma = vma->pma;
+  delete_vma(&(enclave->mmap_vma), vma);
+  delete_pma(&(enclave->pma_list), pma);
+  unmap((uintptr_t*)(enclave->root_page_table), vma->va_start, vma->va_end - vma->va_start);
+  vma->vm_next = NULL;
+  pma->pm_next = NULL;
+  if(caller_enclave->mmap_vma)
+    vma->va_end = caller_enclave->mmap_vma->va_start;
+  else
+    vma->va_end = ENCLAVE_DEFAULT_MMAP_BASE;
+  vma->va_start = vma->va_end - (pma->size - RISCV_PGSIZE);
+  vma->vm_next = caller_enclave->mmap_vma;
+  caller_enclave->mmap_vma = vma;
+  insert_pma(&(caller_enclave->pma_list), pma);
+  mmap((uintptr_t*)(caller_enclave->root_page_table), &(caller_enclave->free_pages), vma->va_start, pma->paddr + RISCV_PGSIZE, pma->size - RISCV_PGSIZE);
+  call_arg->resp_vaddr = vma->va_start;
+  call_arg->resp_size = ret_arg.resp_size;
+
+  //pass return value of server
+restore_return_val:
+  call_arg->resp_val = ret_arg.resp_val;
+  enclave->state = RUNNABLE;
+  ret = 0;
+out:
+  release_enclave_metadata_lock();
+  return ret;
+}
+
+/**************************************************************/
+/*                   called when irq                          */
+/**************************************************************/
+/**
+ * \brief Handle the time interrupt for enclave.
+ * 
+ * \param regs The enclave register context.
+ * \param mcause CSR register of mcause.
+ * \param mepc CSR register of the mepc.
+ */
+uintptr_t do_timer_irq(uintptr_t *regs, uintptr_t mcause, uintptr_t mepc)
+{
+  uintptr_t retval = 0;
+  unsigned int eid = get_curr_enclave_id();
+  struct enclave_t *enclave = NULL;
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_enclave(eid);
+  if(!enclave || enclave->state != RUNNING)
+  {
+    sbi_bug("M mode: do_timer_irq: something is wrong with enclave%d, state: %d\n", eid, enclave->state);
+    retval = -1UL;
+    goto timer_irq_out;
+  }
+   swap_from_enclave_to_host(regs, enclave);
+   enclave->state = RUNNABLE;
+
+timer_irq_out:
+  release_enclave_metadata_lock();
+
+  csr_read_clear(CSR_MIE, MIP_MTIP);
+  csr_read_set(CSR_MIP, MIP_STIP);
+  regs[10] = ENCLAVE_TIMER_IRQ;
+  retval = ENCLAVE_TIMER_IRQ;
+  return retval;
+}
+
+/**
+ * \brief Handle the yield for enclave.
+ * 
+ * \param regs The enclave register context.
+ * \param mcause CSR register of mcause.
+ * \param mepc CSR register of the mepc.
+ */
+uintptr_t do_yield(uintptr_t *regs, uintptr_t mcause, uintptr_t mepc)
+{
+  uintptr_t retval = 0;
+  unsigned int eid = get_curr_enclave_id();
+  struct enclave_t *enclave = NULL;
+
+  acquire_enclave_metadata_lock();
+
+  enclave = __get_enclave(eid);
+  if(!enclave || enclave->state != RUNNING)
+  {
+    sbi_bug("M mode: do_yield: something is wrong with enclave%d\n", eid);
+    retval = -1UL;
+    goto timer_irq_out;
+  }
+
+  swap_from_enclave_to_host(regs, enclave);
+  enclave->state = RUNNABLE;
+
+timer_irq_out:
+  release_enclave_metadata_lock();
+  retval = ENCLAVE_YIELD;
+  return retval;
+}
+
+/**
+ * \brief IPI notifaction for destroy enclave.
+ * 
+ * \param regs The enclave register context.
+ * \param host_ptbr host ptbr register.
+ * \param eid The enclave id.
+ */
+uintptr_t ipi_destroy_enclave(uintptr_t *regs, uintptr_t host_ptbr, int eid)
+{
+  uintptr_t ret = 0;
+  struct enclave_t* enclave = NULL;
+  struct pm_area_struct* pma = NULL;
+  int need_free_enclave_memory = 0;
+
+  // TODO acquire the enclave metadata lock
+  // acquire_enclave_metadata_lock();
+  // printm("M mode: ipi_destroy_enclave %d\r\n", eid);
+
+  enclave = __get_enclave(eid);
+  unsigned long mm_arg_paddr[RELAY_PAGE_NUM];
+  unsigned long mm_arg_size[RELAY_PAGE_NUM];
+  for(int kk = 0; kk < RELAY_PAGE_NUM; kk++)
+  {
+    mm_arg_paddr[kk] = enclave->mm_arg_paddr[kk];
+    mm_arg_size[kk] = enclave->mm_arg_size[kk];
+  }
+
+  //enclave may have exited or even assigned to other host
+  //after ipi sender release the enclave_metadata_lock
+  if(!enclave || enclave->state < FRESH)
+  {
+    ret = -1;
+    sbi_bug("M mode: ipi_stop_enclave: enclave is not existed!\r\n");
+    goto ipi_stop_enclave_out;
+  }
+
+  //this situation should never happen
+  if(enclave->state == RUNNING
+      && (check_in_enclave_world() < 0 || cpus[csr_read(CSR_MHARTID)].eid != eid))
+  {
+    sbi_bug("[ERROR] M mode: ipi_stop_enclave: this situation should never happen!\r\n");
+    ret = -1;
+    goto ipi_stop_enclave_out;
+  }
+
+  if(enclave->state == RUNNING)
+  {
+    swap_from_enclave_to_host(regs, enclave);
+    //regs[10] = ENCLAVE_DESTROYED;
+    regs[10] = 0;
+  }
+  pma = enclave->pma_list;
+  need_free_enclave_memory = 1;
+  __free_enclave(eid);
+
+ipi_stop_enclave_out:
+  // release_enclave_metadata_lock();
+
+  if(need_free_enclave_memory)
+  {
+    free_enclave_memory(pma);
+    free_all_relay_page(mm_arg_paddr, mm_arg_size);
+  }
+  regs[10] = 0;
+	regs[11] = 0;
+  return ret;
+}
+
+/**
+ * \brief Enclave call read in the runtime, ocall to the host to handle.
+ * 
+ * \param regs The enclave register context.
+ */
+uintptr_t enclave_read_sec(uintptr_t *regs, uintptr_t sec){
+  uintptr_t ret = 0;
+  int eid = get_curr_enclave_id();
+  struct enclave_t *enclave = NULL;
+  if(check_in_enclave_world() < 0){
+    return -1;
+  }
+  acquire_enclave_metadata_lock();
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave) != 0 || enclave->state != RUNNING){
+    ret = -1;
+    goto out;
+  }
+  copy_dword_to_host((uintptr_t*)enclave->ocall_func_id, OCALL_READ_SECT);
+  copy_dword_to_host((uintptr_t*)enclave->ocall_arg0, sec);
+  swap_from_enclave_to_host(regs, enclave);
+  enclave->state = OCALLING;
+  ret = ENCLAVE_OCALL;
+
+out:
+  release_enclave_metadata_lock();
+  return ret;
+
+}
+
+/**
+ * \brief Enclave call write() in the runtime, ocall to the host to handle.
+ * 
+ * \param regs The enclave register context.
+ */
+uintptr_t enclave_write_sec(uintptr_t *regs, uintptr_t sec){
+  uintptr_t ret = 0;
+  int eid = get_curr_enclave_id();
+  struct enclave_t *enclave = NULL;
+  if(check_in_enclave_world() < 0){
+    return -1;
+  }
+  acquire_enclave_metadata_lock();
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave) != 0|| enclave->state != RUNNING){
+    ret = -1;
+    goto out;
+  }
+  copy_dword_to_host((uintptr_t*)enclave->ocall_func_id, OCALL_WRITE_SECT);
+  copy_dword_to_host((uintptr_t*)enclave->ocall_arg0,sec);
+  swap_from_enclave_to_host(regs, enclave);
+  enclave->state = OCALLING;
+  ret = ENCLAVE_OCALL;
+
+out:
+  release_enclave_metadata_lock();
+  return ret;
+}
+
+/**
+ * \brief Enclave return the relay page in the runtime, ocall to the host to handle.
+ * 
+ * \param regs The enclave register context.
+ */
+uintptr_t enclave_return_relay_page(uintptr_t *regs)
+{
+  uintptr_t ret = 0;
+  int eid = get_curr_enclave_id();
+  struct enclave_t *enclave = NULL;
+  if(check_in_enclave_world() < 0){
+    return -1;
+  }
+  acquire_enclave_metadata_lock();
+  enclave = __get_enclave(eid);
+  if(!enclave || check_enclave_authentication(enclave) != 0|| enclave->state != RUNNING){
+    ret = -1;
+    sbi_bug("M mode: enclave_return_relay_page: check enclave is failed\n");
+    goto out;
+  }
+
+  copy_dword_to_host((uintptr_t*)enclave->ocall_func_id, OCALL_RETURN_RELAY_PAGE);
+  copy_dword_to_host((uintptr_t*)enclave->ocall_arg0,enclave->mm_arg_paddr[0]);
+  copy_dword_to_host((uintptr_t*)enclave->ocall_arg1,enclave->mm_arg_size[0]);
+  //remap the relay page for host
+  for(int kk = 0; kk < RELAY_PAGE_NUM; kk++)
+  {
+    if (enclave->mm_arg_paddr[kk])
+    {
+      __free_secure_memory(enclave->mm_arg_paddr[kk], enclave->mm_arg_size[kk]);
+      __free_relay_page_entry(enclave->mm_arg_paddr[kk], enclave->mm_arg_size[kk]);
+      unmap((uintptr_t*)(enclave->root_page_table), ENCLAVE_DEFAULT_MM_ARG_BASE, enclave->mm_arg_size[kk]);
+    }
+  }
+  swap_from_enclave_to_host(regs, enclave);
+  enclave->state = OCALLING;
+  ret = ENCLAVE_OCALL;
+
+out:
+  release_enclave_metadata_lock();
+  return ret;
+}
diff --git a/lib/sbi/sm/enclave_mm.c b/lib/sbi/sm/enclave_mm.c
new file mode 100644
index 0000000..801fb70
--- /dev/null
+++ b/lib/sbi/sm/enclave_mm.c
@@ -0,0 +1,195 @@
+#include "sm/sm.h"
+#include "sm/enclave.h"
+#include "sm/enclave_vm.h"
+#include "sm/enclave_mm.h"
+#include "sbi/riscv_atomic.h"
+#include "sbi/sbi_math.h"
+
+// mm_region_list maintains the (free?) secure pages in monitor
+static struct mm_region_list_t *mm_region_list;
+static spinlock_t mm_regions_lock = SPINLOCK_INIT;
+extern spinlock_t mbitmap_lock;
+
+
+/**
+ * \brief This function will turn a set of untrusted pages to secure pages.
+ * Frist, it will valiated the range is valid.
+ * Then, it ensures the pages are untrusted/public now.
+ * Afterthat, it updates the metadata of the pages into secure (or private).
+ * Last, it unmaps the pages from the host PTEs.
+ *
+ * FIXME: we should re-consider the order of the last two steps.
+ * 
+ * \param paddr the check physical address. 
+ * \param size the check physical size
+ */
+int check_and_set_secure_memory(unsigned long paddr, unsigned long size)
+{
+  int ret = 0;
+  if(paddr & (RISCV_PGSIZE-1) || size < RISCV_PGSIZE || size & (RISCV_PGSIZE-1))
+  {
+    ret = -1;
+    return ret;
+  }
+
+  spin_lock(&mbitmap_lock);
+
+  if(test_public_range(PADDR_TO_PFN(paddr), size >> RISCV_PGSHIFT) != 0)
+  {
+    ret = -1;
+    goto out;
+  }
+  set_private_range(PADDR_TO_PFN(paddr), size >> RISCV_PGSHIFT);
+  unmap_mm_region(paddr, size);
+
+out:
+  spin_unlock(&mbitmap_lock);
+  return ret;
+}
+
+/**
+ * \brief Free a set of secure pages.
+ * It turn the secure pgaes into unsecure (or public)
+ * and remap all the pages back to host's PTEs.
+ * 
+ * \param paddr The free physical address.
+ * \param size The free memory size. 
+ */
+int __free_secure_memory(unsigned long paddr, unsigned long size)
+{
+  int ret = 0;
+
+  set_public_range(PADDR_TO_PFN(paddr), size >> RISCV_PGSHIFT);
+  remap_mm_region(paddr, size);
+  return ret;
+}
+
+/**
+ * \brief Free a set of secure pages.
+ * It turn the secure pgaes into unsecure (or public)
+ * and remap all the pages back to host's PTEs.
+ * 
+ * \param paddr The free physical address.
+ * \param size The free memory size. 
+ */
+int free_secure_memory(unsigned long paddr, unsigned long size)
+{
+  int ret = 0;
+  spin_lock(&mbitmap_lock);
+
+  set_public_range(PADDR_TO_PFN(paddr), size >> RISCV_PGSHIFT);
+  remap_mm_region(paddr, size);
+
+  spin_unlock(&mbitmap_lock);
+  return ret;
+}
+
+/**
+ * \brief mm_init adds a new range into mm_region_list for monitor/enclaves to use.
+ * 
+ * \param paddr The init physical address.
+ * \param size The init memory size. 
+ */
+uintptr_t mm_init(uintptr_t paddr, unsigned long size)
+{
+  uintptr_t ret = 0;
+  spin_lock(&mm_regions_lock);
+
+  if(size < RISCV_PGSIZE || (paddr & (RISCV_PGSIZE-1)) || (size & (RISCV_PGSIZE-1)))
+  {
+    ret = -1;
+    goto out;
+  }
+
+  if(check_and_set_secure_memory(paddr, size) != 0)
+  {
+    ret = -1;
+    goto out;
+  }
+
+  struct mm_region_list_t* list = (struct mm_region_list_t*)paddr;
+  list->paddr = paddr;
+  list->size = size;
+  list->next = mm_region_list;
+  mm_region_list = list;
+
+out:
+  spin_unlock(&mm_regions_lock);
+  return ret;
+}
+
+/**
+ * \brief mm_alloc returns a memory region
+ * The returned memory size is put into resp_size, and the addr in return value.
+ * 
+ * \param req_size The request memory size.
+ * \param resp_size The response memory size. 
+ */
+void* mm_alloc(unsigned long req_size, unsigned long *resp_size)
+{
+  void* ret = NULL;
+  spin_lock(&mm_regions_lock);
+
+  if(!mm_region_list)
+  {
+    ret = NULL;
+    goto out;
+  }
+
+  ret = (void*)(mm_region_list->paddr);
+  *resp_size = mm_region_list->size;
+  mm_region_list = mm_region_list->next;
+
+out:
+  spin_unlock(&mm_regions_lock);
+  return ret;
+}
+
+/**
+ * \brief mm_free frees a memory region back to mm_region_list.
+ * 
+ * \param paddr The physical address need to be reclaimed.
+ * \param size The reclaimed memory size. 
+ */
+int mm_free(void* paddr, unsigned long size)
+{
+  int ret = 0;
+  spin_lock(&mm_regions_lock);
+
+  if(size < RISCV_PGSIZE || ((uintptr_t)paddr & (RISCV_PGSIZE-1)) != 0)
+  {
+    ret = -1;
+    goto out;
+  }
+
+  struct mm_region_list_t* list = (struct mm_region_list_t*)paddr;
+  list->paddr = (uintptr_t)paddr;
+  list->size = size;
+  list->next = mm_region_list;
+  mm_region_list = list;
+
+out:
+  spin_unlock(&mm_regions_lock);
+  return ret;
+}
+
+/**
+ * \brief grant enclave access to enclave's memory, it's an empty function now.
+ * 
+ * \param paddr The physical address need to be reclaimed.
+ * \param size The reclaimed memory size. 
+ */
+int grant_enclave_access(struct enclave_t* enclave)
+{
+  return 0;
+}
+
+/**
+ * \brief It's an empty function now.
+ * 
+ * \param enclave The current enclave. 
+ */
+int retrieve_enclave_access(struct enclave_t *enclave)
+{
+  return 0;
+}
diff --git a/lib/sbi/sm/enclave_vm.c b/lib/sbi/sm/enclave_vm.c
new file mode 100644
index 0000000..8069af0
--- /dev/null
+++ b/lib/sbi/sm/enclave_vm.c
@@ -0,0 +1,596 @@
+#include "sm/vm.h"
+// #include "mtrap.h"
+#include "sm/enclave_vm.h"
+#include "sbi/riscv_encoding.h"
+#include "sbi/sbi_console.h"
+
+//get the ppn from an pte entry
+static uintptr_t pte_ppn(pte_t pte)
+{
+  return pte >> PTE_PPN_SHIFT;
+}
+
+/**
+ * \brief internal functions of check_enclave_layout, it will recursively check the region
+ *
+ * \param page_table is an PT page (physical addr), it could be non-root PT page
+ * \param vaddr is the start virtual addr of the PTE in page_table
+ * \param level is the PT page level of page_table
+ */
+static int __check_enclave_layout(uintptr_t page_table, uintptr_t va_start, uintptr_t va_end, uintptr_t pa_start, uintptr_t pa_end, uintptr_t vaddr, int level)
+{
+  if(level < 0)
+  {
+    return -1;
+  }
+
+  uintptr_t* pte = (uintptr_t*)page_table;
+  uintptr_t region_size  = RISCV_PGSIZE * (1 << (level*RISCV_PGLEVEL_BITS));
+  for(int i=0; i < (RISCV_PGSIZE/sizeof(uintptr_t)); ++i)
+  {
+    uintptr_t addr0 = vaddr + i*region_size;
+    uintptr_t addr1 = addr0 + region_size;
+    if(addr1 <= va_start || addr0 >= va_end)
+    {
+      continue;
+    }
+
+    if(PTE_VALID(pte[i]))
+    {
+      if(PTE_ILLEGAL(pte[i]))
+      {
+        return -1;
+      }
+
+      addr0 = PTE_TO_PFN(pte[i]) << RISCV_PGSHIFT;
+      addr1 = addr0 + region_size;
+      if(IS_LEAF_PTE(pte[i]))
+      {
+        if(!(addr0 >= pa_start && addr1 <= pa_end))
+        {
+          return -1;
+        }
+      }
+      else if(__check_enclave_layout(PTE_TO_PFN(pte[i]) << RISCV_PGSHIFT, va_start, va_end,
+            pa_start, pa_end, addr0, level-1))
+      {
+        return -1;
+      }
+    }
+  }
+  return 0;
+}
+
+/**
+ * \brief check whether a VM region is mapped (only) to a PM region
+ *
+ * \param root_page_table is the root of the pgae table
+ * \param va_start is the start of the VM region
+ * \param va_end is the end of the VM region
+ * \param pa_start is the start of the PM region
+ * \param pa_end is the end of the PM region
+ */
+int check_enclave_layout(uintptr_t root_page_table, uintptr_t va_start, uintptr_t va_end, uintptr_t pa_start, uintptr_t pa_end)
+{
+  return __check_enclave_layout(root_page_table, va_start, va_end, pa_start, pa_end, 0, RISCV_PGLEVELS-1);
+}
+
+/**
+ * \brief The auxiliary function for traverse_vams().
+ * 
+ * \param page_table The traversed page table. 
+ * \param vma_list The vma list for the enclave.
+ * \param vma_num Pointer, the vma number in the vma list.
+ * \param va_start Pointer, the given / return value for the start of the virtual address.
+ * \param va_end Pointer, the given / return value for the end of the virtual address.
+ * \param vaddr The temporary / init virtual address.
+ */
+static void __traverse_vmas(uintptr_t page_table, struct vm_area_struct *vma_list, int *vma_num, uintptr_t *va_start, uintptr_t *va_end, uintptr_t vaddr, int level)
+{
+  if(level < 0)
+  {
+    return;
+  }
+
+  uintptr_t *pte= (uintptr_t*)page_table;
+  uintptr_t region_size = RISCV_PGSIZE * (1 << (level*RISCV_PGLEVEL_BITS));
+  for(int i = 0; i < (RISCV_PGSIZE / sizeof(uintptr_t)); ++i)
+  {
+    if(!PTE_VALID(pte[i]))
+    {
+      if((*va_start) && (*va_end))
+      {
+        vma_list[*vma_num].va_start = *va_start;
+        vma_list[*vma_num].va_end = *va_end;
+        vma_list[*vma_num].vm_next = (struct vm_area_struct*)(&vma_list[*vma_num + 1]);
+        *va_start = 0;
+        *va_end = 0;
+        *vma_num += 1;
+      }
+      continue;
+    }
+
+    if(IS_LEAF_PTE(pte[i]))
+    {
+      if(!(*va_start))
+      {
+        *va_start = vaddr + i*region_size;
+      }
+      *va_end = vaddr + (i+1)*region_size;
+    }
+    else
+    {
+      __traverse_vmas(PTE_TO_PFN(pte[i]) << RISCV_PGSHIFT, vma_list, vma_num,
+          va_start, va_end, vaddr + i*region_size, level-1);
+    }
+  }
+
+  if(level == (RISCV_PGLEVELS-1) && (*va_start) && (*va_end))
+  {
+    vma_list[*vma_num].va_start = *va_start;
+    vma_list[*vma_num].va_end = *va_end;
+    vma_list[*vma_num].vm_next = 0;
+    *va_start = 0;
+    *va_end = 0;
+    *vma_num += 1;
+  }
+  else if(level == (RISCV_PGLEVELS-1) && *vma_num)
+  {
+    vma_list[*vma_num - 1].vm_next = 0;
+  }
+}
+
+/**
+ * \brief This traverse the vma list, check and set a new vma list. should only be called during create_enclave as two vma may be mistakely regarded as one
+ * after monitor map new pages for enclave.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param vma_list The vma list for the enclave.
+ */
+void traverse_vmas(uintptr_t root_page_table, struct vm_area_struct *vma_list)
+{
+  uintptr_t va_start = 0;
+  uintptr_t va_end = 0;
+  int vma_num = 0;
+  __traverse_vmas(root_page_table, vma_list, &vma_num, &va_start, &va_end, 0, RISCV_PGLEVELS-1);
+}
+
+/**
+ * \brief The auxiliary function for the va_to_pa().
+ * 
+ * \param page_table The traversed page table.
+ * \param va Pointer, given virtual address. 
+ * \param level The page table level.
+ */
+void* __va_to_pa(uintptr_t* page_table, uintptr_t *va, int level)
+{
+  if(!page_table || level<0)
+    return NULL;
+
+  uintptr_t page_size_bits = RISCV_PGSHIFT + level*RISCV_PGLEVEL_BITS;
+  uintptr_t pos = (((uintptr_t)va) >> page_size_bits) & ((1<<RISCV_PGLEVEL_BITS)-1);
+  uintptr_t pte = page_table[pos];
+  uintptr_t next_page_table = PTE_TO_PFN(pte) << RISCV_PGSHIFT;
+  if(PTE_VALID(pte))
+  {
+    if(IS_LEAF_PTE(pte))
+    {
+      uintptr_t pa = next_page_table + (((uintptr_t)va) & ((1 << page_size_bits) - 1));
+      return (void*)pa;
+    }
+    else
+    {
+      return __va_to_pa((uintptr_t*)next_page_table, va, level-1);
+    }
+  }
+  else
+  {
+    return NULL;
+  }
+}
+
+/**
+ * \brief Retrieve the corresponding physical address with the given virtual address.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param va Pointer, the given virtual address.
+ */
+void* va_to_pa(uintptr_t* root_page_table, void* va)
+{
+  void* result = NULL;
+  result = __va_to_pa(root_page_table, va, RISCV_PGLEVELS-1);
+  return result;
+}
+
+/**
+ * \brief Insert a vma structure in the vma list.
+ * 
+ * \param vma_list Pointer of the vma_list (pointer), the enclave vma list. 
+ * \param vma Pointer, the given vma structure.
+ * \param up_bound The up bound, error checking.
+ */
+int insert_vma(struct vm_area_struct **vma_list, struct vm_area_struct *vma, uintptr_t up_bound)
+{
+  if(vma->va_end > up_bound)
+    return -1;
+
+  struct vm_area_struct* first_vma = *vma_list;
+  if(!first_vma || (first_vma->va_start >= vma->va_end))
+  {
+    vma->vm_next = first_vma;
+    *vma_list = vma;
+    return 0;
+  }
+
+  int found = 0;
+  struct vm_area_struct* second_vma = first_vma->vm_next;
+  while(second_vma)
+  {
+    if((first_vma->va_end <= vma->va_start) && (second_vma->va_start >= vma->va_end))
+    {
+      vma->vm_next = second_vma;
+      first_vma->vm_next = vma;
+      found = 1;
+      break;
+    }
+    first_vma = second_vma;
+    second_vma = second_vma->vm_next;
+  }
+  if(!found)
+  {
+    if(first_vma && (first_vma->va_end <= vma->va_start))
+    {
+      first_vma->vm_next = vma;
+      vma->vm_next = NULL;
+      return 0;
+    }
+    return -1;
+  }
+
+  return 0;
+}
+
+/**
+ * \brief Delete a vma structure in the vma list.
+ * 
+ * \param vma_list Pointer of the vma_list (pointer), the enclave vma list. 
+ * \param vma Pointer, the given vma structure.
+ */
+int delete_vma(struct vm_area_struct **vma_list, struct vm_area_struct *vma)
+{
+  struct vm_area_struct *last_vma = (struct vm_area_struct*)(*vma_list);
+  if(last_vma->va_start <= vma->va_start && last_vma->va_end >= vma->va_end)
+  {
+    *vma_list = last_vma->vm_next;
+    vma->vm_next = NULL;
+    last_vma->vm_next = NULL;
+    return 0;
+  }
+
+  struct vm_area_struct *cur_vma = last_vma->vm_next;
+  while(cur_vma)
+  {
+    if(cur_vma->va_start <= vma->va_start && cur_vma->va_end >= vma->va_end)
+    {
+      last_vma->vm_next = cur_vma->vm_next;
+      vma->vm_next = NULL;
+      cur_vma->vm_next = NULL;
+      return 0;
+    }
+    last_vma = cur_vma;
+    cur_vma = cur_vma->vm_next;
+  }
+
+  return -1;
+}
+
+/**
+ * \brief Find a vma structure in the vma list.
+ * 
+ * \param vma_list Pointer, the enclave vma list. 
+ * \param vaddr The given virtual address.
+ * \param size The vma size.
+ */
+struct vm_area_struct* find_vma(struct vm_area_struct *vma_list, uintptr_t vaddr, uintptr_t size)
+{
+  uintptr_t va_start = vaddr;
+  uintptr_t va_end = vaddr + size;
+  struct vm_area_struct *vma = vma_list;
+  while(vma)
+  {
+    if(vma->va_start <= va_start && vma->va_end >= va_end)
+    {
+      return vma;
+    }
+    vma = vma->vm_next;
+  }
+  return NULL;
+}
+
+/**
+ * \brief Insert a pma structure in the pma list.
+ * 
+ * \param pma_list Pointer of the pma_list (pointer), the enclave pma list. 
+ * \param pma The given pma structure.
+ */
+int insert_pma(struct pm_area_struct **pma_list, struct pm_area_struct *pma)
+{
+  pma->pm_next = *pma_list;
+  *pma_list = pma;
+  return 0;
+}
+
+/**
+ * \brief Delete a pma structure in the pma list.
+ * 
+ * \param pma_list Pointer of the pma_list (pointer), the enclave pma list. 
+ * \param pma The given pma structure.
+ */
+int delete_pma(struct pm_area_struct **pma_list, struct pm_area_struct *pma)
+{
+  struct pm_area_struct *last_pma = *pma_list;
+  if(last_pma->paddr == pma->paddr && last_pma->size == pma->size)
+  {
+    *pma_list = last_pma->pm_next;
+    pma->pm_next = NULL;
+    last_pma->pm_next = NULL;
+    return 0;
+  }
+
+  struct pm_area_struct *cur_pma = last_pma->pm_next;
+  while(cur_pma)
+  {
+    if(cur_pma->paddr == pma->paddr && cur_pma->size == pma->size)
+    {
+      last_pma->pm_next = cur_pma->pm_next;
+      pma->pm_next = NULL;
+      cur_pma->pm_next = NULL;
+      return 0;
+    }
+    last_pma = cur_pma;
+    cur_pma = cur_pma->pm_next;
+  }
+
+  return -1;
+}
+
+/**
+ * \brief The auxiliary function of the pte_walk_create.
+ * 
+ * \param page_table The given page table. 
+ * \param free_pages Pointer of the free page page structure(pointer).
+ * \param va The given virtual address.
+ * \param level The page table level.
+ */
+static uintptr_t *__pte_walk_create(uintptr_t *page_table, struct page_t **free_pages, uintptr_t va, int level)
+{
+  uintptr_t pos = (va >> (RISCV_PGSHIFT + level*RISCV_PGLEVEL_BITS)) & ((1<<RISCV_PGLEVEL_BITS)-1);
+  if(level == 0)
+  {
+    return &(page_table[pos]);
+  }
+
+  if(!(page_table[pos] & PTE_V))
+  {
+    if(!(*free_pages))
+    {
+      sbi_bug("M mode: __pte_walk_create: free_pages is empty\n");
+      return NULL;
+    }
+    uintptr_t paddr = (*free_pages)->paddr;
+    *free_pages = (*free_pages)->next;
+    page_table[pos] = pte_create(paddr>>RISCV_PGSHIFT, PTE_V);
+  }
+  return __pte_walk_create((uintptr_t*)(PTE_TO_PFN(page_table[pos]) << RISCV_PGSHIFT),
+      free_pages, va, level-1);
+}
+
+/**
+ * \brief Walk the page table and create the pte entry.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param free_pages Pointer of the free page page structure(pointer).
+ * \param va The given virtual address.
+ */
+static uintptr_t *pte_walk_create(uintptr_t *root_page_table, struct page_t **free_pages, uintptr_t va)
+{
+  return __pte_walk_create(root_page_table, free_pages, va, RISCV_PGLEVELS-1);
+}
+
+/**
+ * \brief The auxiliary function of the pte_walk.
+ * 
+ * \param page_table The given page table. 
+ * \param va The given virtual address.
+ * \param level The page table level.
+ */
+static uintptr_t *__pte_walk(uintptr_t *page_table, uintptr_t va, int level)
+{
+  uintptr_t pos = (va >> (RISCV_PGSHIFT + level*RISCV_PGLEVEL_BITS)) & ((1<<RISCV_PGLEVEL_BITS)-1);
+  if(level == 0)
+  {
+    return &(page_table[pos]);
+  }
+
+  if(!(page_table[pos] & PTE_V))
+  {
+    return NULL;
+  }
+
+  //we do not support 2M page now
+  /*
+  if((page_table[pos] & PTE_R) || (page_table[pos] & PTE_W))
+  {
+    return &(page_table[pos]);
+  }
+  */
+
+  return __pte_walk((uintptr_t*)(PTE_TO_PFN(page_table[pos]) << RISCV_PGSHIFT), va, level-1);
+}
+
+/**
+ * \brief Walk the page table and return the pte entry address.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param va The given virtual address.
+ */
+uintptr_t *pte_walk(uintptr_t *root_page_table, uintptr_t va)
+{
+  return __pte_walk(root_page_table, va, RISCV_PGLEVELS-1);
+}
+
+/**
+ * \brief Map on page for enclave.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param free_pages Pointer of the free page page structure(pointer).
+ * \param va The mapped virtual address
+ * \param pa The mapped physical address
+ * \param flag The mapping flag.
+ */
+static int map_one_page(uintptr_t *root_page_table, struct page_t **free_pages, uintptr_t va, uintptr_t pa, uintptr_t flag)
+{
+  uintptr_t *pte = pte_walk_create(root_page_table, free_pages, va);
+  if(!pte)
+  {
+    return -1;
+  }
+  if(PTE_VALID(*pte))
+  {
+    sbi_bug("M mode: map_one_page: va 0x%lx is already mmaped: pte: 0x%lx\n", va, *pte);
+  }
+
+  *pte = pte_create(pa>>RISCV_PGSHIFT, flag | PTE_V);
+  return 0;
+}
+
+/**
+ * \brief Unmap on page for enclave.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param va The mapped virtual address.
+ */
+static int unmap_one_page(uintptr_t *root_page_table, uintptr_t va)
+{
+  uintptr_t *pte = pte_walk(root_page_table, va);
+  if(!pte)
+    return -1;
+  *pte = 0;
+  return 0;
+}
+
+/**
+ * \brief Map a range of virtual address to the corresponding physical address.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param free_pages Pointer of the free page page structure(pointer).
+ * \param vaddr The mapped virtual address.
+ * \param paddr The mapped physical address.
+ * \param size The mapped range size.
+ */
+int mmap(uintptr_t* root_page_table, struct page_t **free_pages, uintptr_t vaddr, uintptr_t paddr, uintptr_t size)
+{
+  uintptr_t va = vaddr;
+  uintptr_t pa = paddr;
+  uintptr_t va_end = vaddr + size;
+  while(va < va_end)
+  {
+    if(map_one_page(root_page_table, free_pages, va, pa, PTE_D | PTE_A | PTE_R | PTE_W | PTE_U | PTE_V) != 0)
+    {
+      sbi_bug("M mode: mmap: map one page is failed\n");
+      return -1;
+    }
+    va += RISCV_PGSIZE;
+    pa += RISCV_PGSIZE;
+  }
+  return 0;
+}
+
+/**
+ * \brief Unap a range of virtual address to the corresponding physical address.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param vaddr The unmapped virtual address.
+ * \param size The unmapped range size.
+ */
+int unmap(uintptr_t* root_page_table, uintptr_t vaddr, uintptr_t size)
+{
+  uintptr_t va = vaddr;
+  uintptr_t va_end = vaddr + size;
+  while(va < va_end)
+  {
+    unmap_one_page(root_page_table, va);
+    va += RISCV_PGSIZE;
+  }
+  return 0;
+}
+
+/**
+ * \brief Copy the page table entry.
+ * 
+ * \param page_table The given page table. 
+ * \param free_pages Pointer of the free page page structure(pointer).
+ * \param level The page table level.
+ * \param copy_page The copied page table page.
+ */
+int __copy_page_table(pte_t* page_table, struct page_t ** free_page, int level, pte_t* copy_page)
+{
+  pte_t* t = page_table;
+  pte_t* c_t = copy_page;
+  int i,ret;
+  if(level >= 0)
+  {
+    for (i = 0; i < (1<<RISCV_PGLEVEL_BITS); i++) {
+      if(((level > 0) && (t[i] & PTE_V)) || 
+          ((level == 0) && (t[i] & PTE_V) && (t[i] & PTE_W)))
+      {
+        pte_t* next_copy_page_table;
+        pte_t* next_page_table;
+        if ((*free_page) == NULL)
+          return -1;
+        uintptr_t free_ppn = ((*free_page)->paddr) >> RISCV_PGSHIFT;
+        *free_page = (*free_page)->next;
+        c_t[i] = ptd_create(free_ppn);
+        c_t[i] = c_t[i] | (t[i] & 0x3ff);
+        next_copy_page_table = (pte_t*) (pte_ppn(c_t[i]) << RISCV_PGSHIFT);
+        next_page_table = (pte_t*) (pte_ppn(t[i]) << RISCV_PGSHIFT);
+        ret = __copy_page_table(next_page_table, free_page, level-1, next_copy_page_table);
+        if (ret < 0)
+          return -1;
+      }
+      else if((level == 0) && (t[i] & PTE_V) && (!(t[i] & PTE_W)))
+      {
+        c_t[i] = t[i];
+      }
+    }
+  }
+  else
+  {
+    sbi_memcpy(c_t , t, RISCV_PGSIZE);
+  }
+  return 0;
+}
+
+/**
+ * \brief Map an empty page table.
+ * 
+ * \param root_page_table The enclave root page table. 
+ * \param free_pages Pointer of the free page page structure(pointer).
+ * \param vaddr The virtual address for the empty page.
+ * \param size The empty page size.
+ */
+int map_empty_page(uintptr_t* root_page_table, struct page_t **free_pages, uintptr_t vaddr, uintptr_t size)
+{
+  uintptr_t va = vaddr;
+  uintptr_t va_end = vaddr + size;
+  while(va < va_end)
+  {
+    if ((*free_pages) == NULL)
+      return -1;
+    uintptr_t free_ppn = (*free_pages)->paddr;
+    *free_pages = (*free_pages)->next;
+    map_one_page(root_page_table, free_pages, va, free_ppn, PTE_R | PTE_W | PTE_U | PTE_V);
+    va += RISCV_PGSIZE;
+  }
+  return 0;
+}
diff --git a/lib/sbi/sm/gm/big.c b/lib/sbi/sm/gm/big.c
new file mode 100644
index 0000000..b542ffe
--- /dev/null
+++ b/lib/sbi/sm/gm/big.c
@@ -0,0 +1,853 @@
+#include "sm/gm/big.h"
+
+typedef struct
+{
+  u64 m_low;
+  u64 m_high;
+} uint128_t;
+
+void vli_clear(u64 *vli, u8 ndigits)
+{
+  int i;
+
+  for(i = 0; i < ndigits; ++i){
+    vli[i] = 0;
+  }
+}
+
+/* Returns true if vli == 0, false otherwise. */
+int vli_is_zero(u64 *vli, u8 ndigits)
+{
+  int i;
+
+  for(i = 0; i < ndigits; ++i){
+    if (vli[i])
+      return 0;
+  }
+
+  return 1;
+}
+
+/* Returns nonzero if bit bit of vli is set. */
+u64 vli_test_bit(u64 *vli, u8 bit, u8 ndigits)
+{
+  return (vli[bit/64] & ((u64)1 << (bit % 64)));
+}
+
+/* Counts the number of 64-bit "digits" in vli. */
+u32 vli_num_digits(u64 *vli, u8 ndigits)
+{
+  int i;
+  /* Search from the end until we find a non-zero digit.
+   * We do it in reverse because we expect that most digits will
+   * be nonzero.
+   */
+  for(i = ndigits - 1; i >= 0 && vli[i] == 0; --i);
+
+  return (i + 1);
+}
+
+/* Counts the number of bits required for vli. */
+u32 vli_num_bits(u64 *vli, u8 ndigits)
+{
+  u32 i, num_digits;
+  u64 digit;
+
+  num_digits = vli_num_digits(vli, ndigits);
+  if(num_digits == 0)
+    return 0;
+
+  digit = vli[num_digits - 1];
+  for(i = 0; digit; ++i)
+  digit >>= 1;
+
+  return ((num_digits - 1) * 64 + i);
+}
+
+/* Sets dest = src. */
+void vli_set(u64 *dest, u64 *src, u8 ndigits)
+{
+  u32 i;
+
+  for(i = 0; i < ndigits; ++i)
+    dest[i] = src[i];
+}
+
+/* Returns sign of left - right. */
+int vli_cmp(u64 *left, u64 *right, u8 ndigits)
+{
+  int i;
+
+  for(i = ndigits - 1; i >= 0; --i){
+    if(left[i] > right[i])
+      return 1;
+    else if (left[i] < right[i])
+      return -1;
+  }
+  return 0;
+}
+
+/* Computes result = in << c, returning carry. Can modify in place
+ * (if result == in). 0 < shift < 64.
+ */
+u64 vli_lshift(u64 *result, u64 *in, u32 shift, u8 ndigits)
+{
+  u64 carry = 0;
+  int i;
+
+  for(i = 0; i < ndigits; ++i){
+    u64 temp = in[i];
+    result[i] = (temp << shift) | carry;
+    carry = shift ? temp >> (64 - shift) : 0;
+  }
+
+  return carry;
+}
+
+/* Computes result = in >> c, returning carry. Can modify in place
+ * (if result == in). 0 < shift < 64.
+ */
+u64 vli_rshift(u64 *result, u64 *in, u32 shift, u8 ndigits)
+{
+  u64 carry = 0;
+  int i;
+
+  for(i = ndigits -1; i >= 0; --i){
+    u64 temp = in[i];
+    result[i] = (temp >> shift) | carry;
+    carry = shift ? temp << (64 - shift) : 0;
+  }
+
+  return carry;
+}
+
+/* Computes result = left + right, returning carry. Can modify in place. */
+u64 vli_add(u64 *result, u64 *left, u64 *right, u8 ndigits)
+{
+  u64 carry = 0;
+  u32 i;
+
+  for(i = 0; i < ndigits; ++i){
+    u64 sum;
+
+    sum = left[i] + right[i] + carry;
+    if(sum != left[i]){
+      carry = (sum < left[i]);
+    }
+    result[i] = sum;
+  }
+
+  return carry;
+}
+
+/* Computes result = left - right, returning borrow. Can modify in place. */
+u64 vli_sub(u64 *result, u64 *left, u64 *right, u8 ndigits)
+{
+  u64 borrow = 0;
+  int i;
+
+  for(i = 0; i < ndigits; ++i){
+    u64 diff;
+
+    diff = left[i] - right[i] - borrow;
+    if (diff != left[i])
+      borrow = (diff > left[i]);
+
+    result[i] = diff;
+  }
+
+  return borrow;
+}
+
+static uint128_t mul_64_64(u64 left, u64 right)
+{
+  u64 a0 = left & 0xffffffffull;
+  u64 a1 = left >> 32;
+  u64 b0 = right & 0xffffffffull;
+  u64 b1 = right >> 32;
+  u64 m0 = a0 * b0;
+  u64 m1 = a0 * b1;
+  u64 m2 = a1 * b0;
+  u64 m3 = a1 * b1;
+  uint128_t result;
+
+  m2 += (m0 >> 32);
+  m2 += m1;
+
+  /* Overflow */
+  if (m2 < m1)
+  m3 += 0x100000000ull;
+
+  result.m_low = (m0 & 0xffffffffull) | (m2 << 32);
+  result.m_high = m3 + (m2 >> 32);
+
+  return result;
+}
+
+static uint128_t add_128_128(uint128_t a, uint128_t b)
+{
+  uint128_t result;
+
+  result.m_low = a.m_low + b.m_low;
+  result.m_high = a.m_high + b.m_high + (result.m_low < a.m_low);
+
+  return result;
+}
+
+static u64 vli_add_digit_mul(u64 *result, u64 *b, u64 c, u64 *d, u8 digits)
+{
+  uint128_t mul;
+  u64 carry;
+  u32 i;
+
+  if(c == 0)
+    return 0;
+
+  carry = 0;
+  for (i = 0; i < digits; i++) {
+    mul = mul_64_64(c, d[i]);
+    if((result[i] = b[i] + carry) < carry){
+      carry = 1;
+    }
+    else{
+      carry = 0;
+    }
+    if((result[i] += mul.m_low) < mul.m_low){
+      carry++;
+    }
+    carry += mul.m_high;
+  }
+
+  return carry;
+}
+
+void bn_mult(u64 *result, u64 *left, u64 *right, u8 ndigits)
+{
+  u64 t[2*ndigits];
+  u32 bdigits, cdigits, i;
+
+  vli_clear(t, 2*ndigits);
+
+  bdigits = vli_num_digits(left, ndigits);
+  cdigits = vli_num_digits(right, ndigits);
+
+  for(i=0; i<bdigits; i++){
+    t[i+cdigits] += vli_add_digit_mul(&t[i], &t[i], left[i], right, cdigits);
+  }
+
+  vli_set(result, t, 2*ndigits);
+}
+
+#define BN_DIGIT_BITS  32
+#define BN_MAX_DIGIT   0xFFFFFFFF
+static u32 vli_sub_digit_mult(u32 *a, u32 *b, u32 c, u32 *d, u32 digits)
+{
+  u64 result;
+  u32 borrow, rh, rl;
+  u32 i;
+
+  if(c == 0)
+  return 0;
+
+  borrow = 0;
+  for(i=0; i<digits; i++) {
+    result = (u64)c * d[i];
+    rl = result & BN_MAX_DIGIT;
+    rh = (result >> BN_DIGIT_BITS) & BN_MAX_DIGIT;
+    if((a[i] = b[i] - borrow) > (BN_MAX_DIGIT - borrow)){
+      borrow = 1;
+    }else{
+      borrow = 0;
+    }
+    if((a[i] -= rl) > (BN_MAX_DIGIT - rl)){
+      borrow++;
+    }
+    borrow += rh;
+  }
+
+  return borrow;
+}
+
+static u32 bn_digit_bits(u32 a)
+{
+  u32 i;
+
+  for(i = 0; i< sizeof(a) * 8; i++){
+    if(a == 0)
+      break;
+    a >>= 1;
+  }
+
+  return i;
+}
+
+void bn_div(u32 *a, u32 *b, u32 *c, u32 cdigits, u32 *d, u32 ddigits)
+{
+  u32 ai, t, cc[cdigits+1], dd[cdigits/2];
+  u32 dddigits, shift;
+  u64 tmp;
+  int i;
+
+  dddigits = ddigits;
+
+  shift = BN_DIGIT_BITS - bn_digit_bits(d[dddigits-1]);
+  vli_clear((u64*)cc, dddigits/2);
+  cc[cdigits] = vli_lshift((u64*)cc, (u64*)c, shift, cdigits/2);
+  vli_lshift((u64*)dd, (u64*)d, shift, dddigits/2);
+  t = dd[dddigits-1];
+
+  vli_clear((u64*)a, cdigits/2);
+  i = cdigits - dddigits;
+  for(; i>=0; i--){
+    if(t == BN_MAX_DIGIT){
+      ai = cc[i+dddigits];
+    }else{
+      tmp = cc[i+dddigits-1];
+      tmp += (u64)cc[i+dddigits] << BN_DIGIT_BITS;
+      ai = tmp / (t + 1);
+    }
+
+    cc[i+dddigits] -= vli_sub_digit_mult(&cc[i], &cc[i], ai, dd, dddigits);
+    while(cc[i+dddigits] || (vli_cmp((u64*)&cc[i], (u64*)dd, dddigits/2) >= 0)){
+      ai++;
+      cc[i+dddigits] -= vli_sub((u64*)&cc[i], (u64*)&cc[i], (u64*)dd, dddigits/2);
+    }
+    a[i] = ai;
+  }
+
+  vli_rshift((u64*)b, (u64*)cc, shift, dddigits/2);
+}
+
+void vli_div(u64 *result, u64 *remainder, u64 *left, u64 cdigits, u64 *right, u8 ddigits)
+{
+  bn_div((u32*)result, (u32*)remainder, (u32*)left, cdigits*2, (u32*)right, ddigits*2);
+}
+
+void bn_mod(u64 *result, u64 *left, u64 *right, u8 ndigits)
+{
+  u64 t[2*ndigits];
+
+  vli_div(t, result, left, ndigits*2, right, ndigits);
+}
+
+void _vli_mult(u64 *result, u64 *left, u64 *right, u8 ndigits)
+{
+  uint128_t r01 = { 0, 0 };
+  u64 r2 = 0;
+  unsigned int i, k;
+
+  /* Compute each digit of result in sequence, maintaining the
+   * carries.
+   */
+  for(k = 0; k < ndigits * 2 - 1; k++){
+    unsigned int min;
+
+    if(k < ndigits)
+      min = 0;
+    else
+      min = (k + 1) - ndigits;
+
+    for(i = min; i <= k && i < ndigits; i++){
+      uint128_t product;
+
+      product = mul_64_64(left[i], right[k - i]);
+
+      r01 = add_128_128(r01, product);
+      r2 += (r01.m_high < product.m_high);
+    }
+
+    result[k] = r01.m_low;
+    r01.m_low = r01.m_high;
+    r01.m_high = r2;
+    r2 = 0;
+  }
+
+  result[ndigits * 2 - 1] = r01.m_low;
+}
+
+void vli_mult(u64 *result, u64 *left, u64 *right, u8 ndigits)
+{
+#if 1
+  bn_mult(result, left, right, ndigits);
+#else
+  _vli_mult(result, left, right, ndigits);
+#endif
+}
+
+void vli_square(u64 *result, u64 *left, u8 ndigits)
+{
+  uint128_t r01 = { 0, 0 };
+  u64 r2 = 0;
+  int i, k;
+
+  for(k = 0; k < ndigits * 2 - 1; k++){
+    unsigned int min;
+
+    if(k < ndigits)
+      min = 0;
+    else
+      min = (k + 1) - ndigits;
+
+    for(i = min; i <= k && i <= k - i; i++){
+      uint128_t product;
+
+      product = mul_64_64(left[i], left[k - i]);
+
+      if(i < k - i){
+        r2 += product.m_high >> 63;
+        product.m_high = (product.m_high << 1) |
+          (product.m_low >> 63);
+        product.m_low <<= 1;
+      }
+
+      r01 = add_128_128(r01, product);
+      r2 += (r01.m_high < product.m_high);
+    }
+
+    result[k] = r01.m_low;
+    r01.m_low = r01.m_high;
+    r01.m_high = r2;
+    r2 = 0;
+  }
+
+  result[ndigits * 2 - 1] = r01.m_low;
+}
+
+/* Computes result = (left + right) % mod.
+   Assumes that left < mod and right < mod, result != mod. */
+void vli_mod_add(u64 *result, u64 *left, u64 *right, u64 *mod, u8 ndigits)
+{
+  u64 carry;
+
+  carry = vli_add(result, left, right, ndigits);
+  /* result > mod (result = mod + remainder), so subtract mod to
+   * get remainder.
+   */
+
+  if(carry || vli_cmp(result, mod, ndigits) >= 0){
+    /* result > mod (result = mod + remainder), so subtract mod to get remainder. */
+    vli_sub(result, result, mod, ndigits);
+  }
+}
+
+/* Computes result = (left - right) % mod.
+ * Assumes that left < mod and right < mod, result != mod.
+ */
+void vli_mod_sub(u64 *result, u64 *left, u64 *right, u64 *mod, u8 ndigits)
+{
+  u64 borrow;
+
+  borrow = vli_sub(result, left, right, ndigits);
+  /* In this case, result == -diff == (max int) - diff.
+   * Since -x % d == d - x, we can get the correct result from
+   * result + mod (with overflow).
+   */
+  if(borrow)
+    vli_add(result, result, mod, ndigits);
+}
+
+/* Computes result = product % curve_prime
+ * from http://www.nsa.gov/ia/_files/nist-routines.pdf
+ */
+void vli_mmod_fast_nist_256(u64 *result, u64 *product, u64 *curve_prime, u8 ndigits)
+{
+  u64 tmp[2 * ndigits];
+  int carry;
+
+  /* t */
+  vli_set(result, product, ndigits);
+
+  /* s1 */
+  tmp[0] = 0;
+  tmp[1] = product[5] & 0xffffffff00000000ull;
+  tmp[2] = product[6];
+  tmp[3] = product[7];
+  carry = vli_lshift(tmp, tmp, 1, ndigits);
+  carry += vli_add(result, result, tmp, ndigits);
+
+  /* s2 */
+  tmp[1] = product[6] << 32;
+  tmp[2] = (product[6] >> 32) | (product[7] << 32);
+  tmp[3] = product[7] >> 32;
+  carry += vli_lshift(tmp, tmp, 1, ndigits);
+  carry += vli_add(result, result, tmp, ndigits);
+
+  /* s3 */
+  tmp[0] = product[4];
+  tmp[1] = product[5] & 0xffffffff;
+  tmp[2] = 0;
+  tmp[3] = product[7];
+  carry += vli_add(result, result, tmp, ndigits);
+
+  /* s4 */
+  tmp[0] = (product[4] >> 32) | (product[5] << 32);
+  tmp[1] = (product[5] >> 32) | (product[6] & 0xffffffff00000000ull);
+  tmp[2] = product[7];
+  tmp[3] = (product[6] >> 32) | (product[4] << 32);
+  carry += vli_add(result, result, tmp, ndigits);
+
+  /* d1 */
+  tmp[0] = (product[5] >> 32) | (product[6] << 32);
+  tmp[1] = (product[6] >> 32);
+  tmp[2] = 0;
+  tmp[3] = (product[4] & 0xffffffff) | (product[5] << 32);
+  carry -= vli_sub(result, result, tmp, ndigits);
+
+  /* d2 */
+  tmp[0] = product[6];
+  tmp[1] = product[7];
+  tmp[2] = 0;
+  tmp[3] = (product[4] >> 32) | (product[5] & 0xffffffff00000000ull);
+  carry -= vli_sub(result, result, tmp, ndigits);
+
+  /* d3 */
+  tmp[0] = (product[6] >> 32) | (product[7] << 32);
+  tmp[1] = (product[7] >> 32) | (product[4] << 32);
+  tmp[2] = (product[4] >> 32) | (product[5] << 32);
+  tmp[3] = (product[6] << 32);
+  carry -= vli_sub(result, result, tmp, ndigits);
+
+  /* d4 */
+  tmp[0] = product[7];
+  tmp[1] = product[4] & 0xffffffff00000000ull;
+  tmp[2] = product[5];
+  tmp[3] = product[6] & 0xffffffff00000000ull;
+  carry -= vli_sub(result, result, tmp, ndigits);
+
+  if (carry < 0) {
+    do{
+      carry += vli_add(result, result, curve_prime, ndigits);
+    }while(carry < 0);
+  }
+  else{
+    while(carry || vli_cmp(curve_prime, result, ndigits) != 1){
+      carry -= vli_sub(result, result, curve_prime, ndigits);
+    }
+  }
+}
+
+void vli_mmod_fast_sm2_256(u64 *result, u64 *_product, u64 *mod, u8 ndigits)
+{
+  u32 tmp1[8];
+  u32 tmp2[8];
+  u32 tmp3[8];
+  u32 *product = (u32 *)_product;
+  int carry = 0;
+
+  vli_set(result, (u64 *)product, ndigits);
+  vli_clear((u64 *)tmp1, ndigits);
+  vli_clear((u64 *)tmp2, ndigits);
+  vli_clear((u64 *)tmp3, ndigits);
+
+  /* Y0 */
+  tmp1[0] = tmp1[3] = tmp1[7] = product[8];
+  tmp2[2] = product[8];
+  carry += vli_add(result, result, (u64 *)tmp1, ndigits);
+  carry -= vli_sub(result, result, (u64 *)tmp2, ndigits);
+
+  /* Y1 */
+  tmp1[0] = tmp1[1] = tmp1[4] = tmp1[7] = product[9];
+  tmp1[3] = 0;
+  tmp2[2] = product[9];
+  carry += vli_add(result, result, (u64 *)tmp1, ndigits);
+  carry -= vli_sub(result, result, (u64 *)tmp2, ndigits);
+
+  /* Y2 */
+  tmp1[0] = tmp1[1] = tmp1[5] = tmp1[7] = product[10];
+  tmp1[4] = 0;
+  carry += vli_add(result, result, (u64 *)tmp1, ndigits);
+
+  /* Y3 */
+  tmp1[0] = tmp1[1] = tmp1[3] = tmp1[6] = tmp1[7] = product[11];
+  tmp1[5] =  0;
+  carry += vli_add(result, result, (u64 *)tmp1, ndigits);
+
+  /* Y4 */
+  tmp1[0] = tmp1[1] = tmp1[3] = tmp1[4] = tmp1[7] = tmp3[7] = product[12];
+  tmp1[6] = 0;
+  carry += vli_add(result, result, (u64 *)tmp1, ndigits);
+  carry += vli_add(result, result, (u64 *)tmp3, ndigits);
+
+  /* Y5 */
+  tmp1[0] = tmp1[1] = tmp1[3] = tmp1[4] = tmp1[5] = tmp1[7] = product[13];
+  tmp2[2] = product[13];
+  tmp3[0] = tmp3[3] = tmp3[7] = product[13];
+  carry += vli_add(result, result, (u64 *)tmp1, ndigits);
+  carry += vli_add(result, result, (u64 *)tmp3, ndigits);
+  carry -= vli_sub(result, result, (u64 *)tmp2, ndigits);
+
+  /* Y6 */
+  tmp1[0] = tmp1[1] = tmp1[3] = tmp1[4] = tmp1[5] = tmp1[6] = tmp1[7] = product[14];
+  tmp2[2] = product[14];
+  tmp3[0] = tmp3[1] = tmp3[4] = tmp3[7] = product[14];
+  tmp3[3] = 0;
+  carry += vli_add(result, result, (u64 *)tmp1, ndigits);
+  carry += vli_add(result, result, (u64 *)tmp3, ndigits);
+  carry -= vli_sub(result, result, (u64 *)tmp2, ndigits);
+
+  /* Y7 */
+  tmp1[0] = tmp1[1] = tmp1[3] = tmp1[4] = tmp1[5] = tmp1[6] = tmp1[7] = product[15];
+  tmp3[0] = tmp3[1] = tmp3[5]  = product[15];
+  tmp3[4] = 0;
+  tmp3[7] = 0;
+  tmp2[7] = product[15];
+  tmp2[2] = 0;
+  carry += vli_lshift((u64 *)tmp2, (u64 *)tmp2, 1, ndigits);
+  carry += vli_add(result, result, (u64 *)tmp1, ndigits);
+  carry += vli_add(result, result, (u64 *)tmp3, ndigits);
+  carry += vli_add(result, result, (u64 *)tmp2, ndigits);
+  if(carry < 0){
+    do{
+      carry += vli_add(result, result, mod, ndigits);
+    }while(carry < 0);
+  }
+  else{
+    while(carry || vli_cmp(mod, result, ndigits) != 1)
+    {
+      carry -= vli_sub(result, result, mod, ndigits);
+    }
+  }
+}
+
+/* Computes result = (product) % mod. */
+void _vli_mod(u64 *result, u64 *product, u64 *mod, u8 ndigits)
+{
+  u64 modMultiple[2 * ndigits];
+  uint digitShift, bitShift;
+  uint productBits;
+  uint modBits = vli_num_bits(mod, ndigits);
+
+  productBits = vli_num_bits(product + ndigits, ndigits);
+  if(productBits){
+    productBits += ndigits * 64;
+  }
+  else{
+    productBits = vli_num_bits(product, ndigits);
+  }
+
+  if(productBits < modBits){
+    /* product < mod. */
+    vli_set(result, product, ndigits);
+    return;
+  }
+
+  /* Shift mod by (leftBits - modBits). This multiplies mod by the largest
+   power of two possible while still resulting in a number less than left. */
+  vli_clear(modMultiple, ndigits);
+  vli_clear(modMultiple + ndigits, ndigits);
+  digitShift = (productBits - modBits) / 64;
+  bitShift = (productBits - modBits) % 64;
+  if(bitShift){
+    modMultiple[digitShift + ndigits] = vli_lshift(modMultiple + digitShift, mod, bitShift, ndigits);
+  }
+  else{
+    vli_set(modMultiple + digitShift, mod, ndigits);
+  }
+
+  /* Subtract all multiples of mod to get the remainder. */
+  vli_clear(result, ndigits);
+  result[0] = 1; /* Use result as a temp var to store 1 (for subtraction) */
+  while(productBits > ndigits * 64 || vli_cmp(modMultiple, mod, ndigits) >= 0)
+  {
+    int cmp = vli_cmp(modMultiple + ndigits, product + ndigits, ndigits);
+    if(cmp < 0 || (cmp == 0 && vli_cmp(modMultiple, product, ndigits) <= 0)){
+      if (vli_sub(product, product, modMultiple, ndigits))
+      {
+      /* borrow */
+      vli_sub(product + ndigits, product + ndigits, result, ndigits);
+      }
+      vli_sub(product + ndigits, product + ndigits, modMultiple + ndigits, ndigits);
+    }
+    u64 carry = (modMultiple[ndigits] & 0x01) << 63;
+    vli_rshift(modMultiple + ndigits, modMultiple + ndigits, 1, ndigits);
+    vli_rshift(modMultiple, modMultiple, 1, ndigits);
+    modMultiple[ndigits-1] |= carry;
+
+    --productBits;
+  }
+  vli_set(result, product, ndigits);
+}
+
+/* Computes result = (product) % mod. */
+void vli_mod(u64 *result, u64 *product, u64 *mod, u8 ndigits)
+{
+#if 1
+  bn_mod(result, product, mod, ndigits);
+#else
+  _vli_mod(result, product, mod, ndigits);
+#endif
+}
+
+/* Computes result = (left * right) % curve->p. */
+void vli_mod_mult_fast(u64 *result, u64 *left, u64 *right, u64 *mod, u8 ndigits)
+{
+  u64 product[2 * ndigits];
+
+  vli_mult(product, left, right, ndigits);
+#if 1
+  vli_mod(result, product, mod, ndigits);
+#else
+  if ( mod[1] == 0xFFFFFFFF00000000ull)
+  vli_mmod_fast_sm2_256(result, product, mod, ndigits);
+  else
+  vli_mmod_fast_nist_256(result, product, mod, ndigits);
+#endif
+}
+
+/* Computes result = left^2 % curve->p. */
+void vli_mod_square_fast(u64 *result, u64 *left, u64 *mod, u8 ndigits)
+{
+  u64 product[2 * ndigits];
+
+  vli_square(product, left, ndigits);
+#if 1
+  vli_mod(result, product, mod, ndigits);
+
+#else
+  if ( mod[1] == 0xFFFFFFFF00000000ull)
+  vli_mmod_fast_sm2_256(result, product, mod, ndigits);
+  else
+  vli_mmod_fast_nist_256(result, product, mod, ndigits);
+#endif
+}
+
+/* Computes result = (left * right) % mod. */
+void vli_mod_mult(u64 *result, u64 *left, u64 *right, u64 *mod, u8 ndigits)
+{
+  u64 product[2 * ndigits];
+
+  vli_mult(product, left, right, ndigits);
+  vli_mod(result, product, mod, ndigits);
+}
+
+/* Computes result = left^2 % mod. */
+void vli_mod_square(u64 *result, u64 *left, u64 *mod, u8 ndigits)
+{
+  u64 product[2 * ndigits];
+
+  vli_square(product, left, ndigits);
+  vli_mod(result, product, mod, ndigits);
+}
+
+#define DIGIT_2MSB(x)  (u64)(((x) >> (VLI_DIGIT_BITS - 2)) & 0x03)
+/* Computes result = left^p % mod. */
+void vli_mod_exp(u64 *result, u64 *left, u64 *p, u64 *mod, u8 ndigits)
+{
+  u64 bpower[3][ndigits], t[ndigits];
+  u64 ci_bits, ci;
+  u32 j, s;
+  u32 digits;
+  int i;
+
+  vli_set(bpower[0], left, ndigits);
+  vli_mod_mult(bpower[1], bpower[0], left, mod, ndigits);
+  vli_mod_mult(bpower[2], bpower[1], left, mod, ndigits);
+  vli_clear(t, ndigits);
+  t[0] = 1;
+
+  digits = vli_num_digits(p , ndigits);
+
+  i = digits - 1;
+  for( ; i >= 0; i--){
+    ci = p[i];
+    ci_bits = VLI_DIGIT_BITS;
+
+    if(i == (digits - 1)){
+      while(!DIGIT_2MSB(ci)){
+        ci <<= 2;
+        ci_bits -= 2;
+      }
+    }
+
+    for( j = 0; j < ci_bits; j += 2) {
+      vli_mod_mult(t, t, t, mod, ndigits);
+      vli_mod_mult(t, t, t, mod, ndigits);
+      if((s = DIGIT_2MSB(ci)) != 0){
+        vli_mod_mult(t, t, bpower[s-1], mod, ndigits);
+      }
+      ci <<= 2;
+    }
+  }
+
+  vli_set(result, t, ndigits);
+}
+
+#define EVEN(vli) (!(vli[0] & 1))
+/* Computes result = (1 / p_input) % mod. All VLIs are the same size.
+ * See "From Euclid's GCD to Montgomery Multiplication to the Great Divide"
+ * https://labs.oracle.com/techrep/2001/smli_tr-2001-95.pdf
+ */
+void vli_mod_inv(u64 *result, u64 *input, u64 *mod, u8 ndigits)
+{
+  u64 a[ndigits], b[ndigits];
+  u64 u[ndigits], v[ndigits];
+  u64 carry;
+  int cmp_result;
+
+  if(vli_is_zero(input, ndigits)){
+    vli_clear(result, ndigits);
+    return;
+  }
+
+  vli_set(a, input, ndigits);
+  vli_set(b, mod, ndigits);
+  vli_clear(u, ndigits);
+  u[0] = 1;
+  vli_clear(v, ndigits);
+
+  while((cmp_result = vli_cmp(a, b, ndigits)) != 0){
+    carry = 0;
+
+    if(EVEN(a)){
+      vli_rshift(a, a, 1, ndigits);
+
+      if(!EVEN(u))
+        carry = vli_add(u, u, mod, ndigits);
+
+      vli_rshift(u, u, 1, ndigits);
+      if (carry)
+        u[ndigits - 1] |= 0x8000000000000000ull;
+    }
+    else if(EVEN(b)){
+      vli_rshift(b, b, 1, ndigits);
+
+      if(!EVEN(v))
+        carry = vli_add(v, v, mod, ndigits);
+
+      vli_rshift(v, v, 1, ndigits);
+      if(carry)
+        v[ndigits - 1] |= 0x8000000000000000ull;
+    }else if(cmp_result > 0){
+      vli_sub(a, a, b, ndigits);
+      vli_rshift(a, a, 1, ndigits);
+
+      if(vli_cmp(u, v, ndigits) < 0)
+        vli_add(u, u, mod, ndigits);
+
+      vli_sub(u, u, v, ndigits);
+      if(!EVEN(u))
+        carry = vli_add(u, u, mod, ndigits);
+
+      vli_rshift(u, u, 1, ndigits);
+      if(carry)
+        u[ndigits - 1] |= 0x8000000000000000ull;
+    }
+    else{
+      vli_sub(b, b, a, ndigits);
+      vli_rshift(b, b, 1, ndigits);
+
+      if(vli_cmp(v, u, ndigits) < 0)
+        vli_add(v, v, mod, ndigits);
+
+      vli_sub(v, v, u, ndigits);
+      if(!EVEN(v))
+        carry = vli_add(v, v, mod, ndigits);
+
+      vli_rshift(v, v, 1, ndigits);
+      if(carry)
+        v[ndigits - 1] |= 0x8000000000000000ull;
+    }
+  }
+
+  vli_set(result, u, ndigits);
+}
diff --git a/lib/sbi/sm/gm/ecc.c b/lib/sbi/sm/gm/ecc.c
new file mode 100644
index 0000000..d34e7b9
--- /dev/null
+++ b/lib/sbi/sm/gm/ecc.c
@@ -0,0 +1,356 @@
+#include "sm/gm/ecc.h"
+#include "sm/gm/big.h"
+
+/* Returns 1 if point is the point at infinity, 0 otherwise. */
+int ecc_point_is_zero(struct ecc_curve *curve, ecc_point *point)
+{
+  return (vli_is_zero(point->x, curve->ndigits)
+      && vli_is_zero(point->y, curve->ndigits));
+}
+
+/* Double in place */
+void ecc_point_double_jacobian(struct ecc_curve *curve, u64 *X1, u64 *Y1, u64 *Z1)
+{
+  /* t1 = X, t2 = Y, t3 = Z */
+  u64 t4[ECC_MAX_DIGITS];
+  u64 t5[ECC_MAX_DIGITS];
+
+  if(vli_is_zero(Z1, curve->ndigits))
+    return;
+
+  vli_mod_square_fast(t4, Y1, curve->p, curve->ndigits);   /* t4 = y1^2 */
+  vli_mod_mult_fast(t5, X1, t4, curve->p, curve->ndigits); /* t5 = x1*y1^2 = A */
+  vli_mod_square_fast(t4, t4, curve->p, curve->ndigits);   /* t4 = y1^4 */
+  vli_mod_mult_fast(Y1, Y1, Z1, curve->p, curve->ndigits); /* t2 = y1*z1 = z3 */
+  vli_mod_square_fast(Z1, Z1, curve->p, curve->ndigits);   /* t3 = z1^2 */
+
+  vli_mod_add(X1, X1, Z1, curve->p, curve->ndigits); /* t1 = x1 + z1^2 */
+  vli_mod_add(Z1, Z1, Z1, curve->p, curve->ndigits); /* t3 = 2*z1^2 */
+  vli_mod_sub(Z1, X1, Z1, curve->p, curve->ndigits); /* t3 = x1 - z1^2 */
+  vli_mod_mult_fast(X1, X1, Z1, curve->p, curve->ndigits);    /* t1 = x1^2 - z1^4 */
+
+  vli_mod_add(Z1, X1, X1, curve->p, curve->ndigits); /* t3 = 2*(x1^2 - z1^4) */
+  vli_mod_add(X1, X1, Z1, curve->p, curve->ndigits); /* t1 = 3*(x1^2 - z1^4) */
+  if(vli_test_bit(X1, 0, curve->ndigits)){
+    u64 carry = vli_add(X1, X1, curve->p, curve->ndigits);
+    vli_rshift(X1, X1, 1, curve->ndigits);
+    X1[ECC_MAX_DIGITS-1] |= carry << 63;
+  }
+  else{
+    vli_rshift(X1, X1, 1, curve->ndigits);
+  }
+
+  /* t1 = 3/2*(x1^2 - z1^4) = B */
+  vli_mod_square_fast(Z1, X1, curve->p, curve->ndigits);      /* t3 = B^2 */
+  vli_mod_sub(Z1, Z1, t5, curve->p, curve->ndigits); /* t3 = B^2 - A */
+  vli_mod_sub(Z1, Z1, t5, curve->p, curve->ndigits); /* t3 = B^2 - 2A = x3 */
+  vli_mod_sub(t5, t5, Z1, curve->p, curve->ndigits); /* t5 = A - x3 */
+  vli_mod_mult_fast(X1, X1, t5, curve->p, curve->ndigits);    /* t1 = B * (A - x3) */
+  vli_mod_sub(t4, X1, t4, curve->p, curve->ndigits); /* t4 = B * (A - x3) - y1^4 = y3 */
+
+  vli_set(X1, Z1, curve->ndigits);
+  vli_set(Z1, Y1, curve->ndigits);
+  vli_set(Y1, t4, curve->ndigits);
+}
+
+/* Modify (x1, y1) => (x1 * z^2, y1 * z^3) */
+void apply_z(struct ecc_curve *curve, u64 *X1, u64 *Y1, u64 *Z)
+{
+  u64 t1[ECC_MAX_DIGITS];
+
+  vli_mod_square_fast(t1, Z, curve->p, curve->ndigits);    /* z^2 */
+  vli_mod_mult_fast(X1, X1, t1, curve->p, curve->ndigits); /* x1 * z^2 */
+  vli_mod_mult_fast(t1, t1, Z, curve->p, curve->ndigits);  /* z^3 */
+  vli_mod_mult_fast(Y1, Y1, t1, curve->p, curve->ndigits); /* y1 * z^3 */
+}
+
+/* P = (x1, y1) => 2P, (x2, y2) => P' */
+void XYcZ_initial_double(struct ecc_curve *curve, u64 *X1, u64 *Y1, u64 *X2, u64 *Y2, u64 *initialZ)
+{
+  u64 z[ECC_MAX_DIGITS];
+
+  vli_set(X2, X1, curve->ndigits);
+  vli_set(Y2, Y1, curve->ndigits);
+
+  if(initialZ){
+    vli_set(z, initialZ, curve->ndigits);
+  }
+  else{
+    vli_clear(z, curve->ndigits);
+    z[0] = 1;
+  }
+  apply_z(curve, X1, Y1, z);
+
+  ecc_point_double_jacobian(curve, X1, Y1, z);
+
+  apply_z(curve, X2, Y2, z);
+}
+
+/* Input P = (x1, y1, Z), Q = (x2, y2, Z)
+   Output P' = (x1', y1', Z3), P + Q = (x3, y3, Z3)
+   or P => P', Q => P + Q
+   */
+void XYcZ_add(struct ecc_curve *curve, u64 *X1, u64 *Y1, u64 *X2, u64 *Y2)
+{
+  /* t1 = X1, t2 = Y1, t3 = X2, t4 = Y2 */
+  u64 t5[ECC_MAX_DIGITS];
+
+  vli_mod_sub(t5, X2, X1, curve->p, curve->ndigits); /* t5 = x2 - x1 */
+  vli_mod_square_fast(t5, t5, curve->p, curve->ndigits);      /* t5 = (x2 - x1)^2 = A */
+  vli_mod_mult_fast(X1, X1, t5, curve->p, curve->ndigits);    /* t1 = x1*A = B */
+  vli_mod_mult_fast(X2, X2, t5, curve->p, curve->ndigits);    /* t3 = x2*A = C */
+  vli_mod_sub(Y2, Y2, Y1, curve->p, curve->ndigits); /* t4 = y2 - y1 */
+  vli_mod_square_fast(t5, Y2, curve->p, curve->ndigits);      /* t5 = (y2 - y1)^2 = D */
+
+  vli_mod_sub(t5, t5, X1, curve->p, curve->ndigits); /* t5 = D - B */
+  vli_mod_sub(t5, t5, X2, curve->p, curve->ndigits); /* t5 = D - B - C = x3 */
+  vli_mod_sub(X2, X2, X1, curve->p, curve->ndigits); /* t3 = C - B */
+  vli_mod_mult_fast(Y1, Y1, X2, curve->p, curve->ndigits);    /* t2 = y1*(C - B) */
+  vli_mod_sub(X2, X1, t5, curve->p, curve->ndigits); /* t3 = B - x3 */
+  vli_mod_mult_fast(Y2, Y2, X2, curve->p, curve->ndigits);    /* t4 = (y2 - y1)*(B - x3) */
+  vli_mod_sub(Y2, Y2, Y1, curve->p, curve->ndigits); /* t4 = y3 */
+
+  vli_set(X2, t5, curve->ndigits);
+}
+
+/* Input P = (x1, y1, Z), Q = (x2, y2, Z)
+ * Output P + Q = (x3, y3, Z3), P - Q = (x3', y3', Z3)
+ * or P => P - Q, Q => P + Q
+ */
+void XYcZ_addC(struct ecc_curve *curve, u64 *X1, u64 *Y1, u64 *X2, u64 *Y2)
+{
+  /* t1 = X1, t2 = Y1, t3 = X2, t4 = Y2 */
+  u64 t5[ECC_MAX_DIGITS];
+  u64 t6[ECC_MAX_DIGITS];
+  u64 t7[ECC_MAX_DIGITS];
+
+  vli_mod_sub(t5, X2, X1, curve->p, curve->ndigits); /* t5 = x2 - x1 */
+  vli_mod_square_fast(t5, t5, curve->p, curve->ndigits);      /* t5 = (x2 - x1)^2 = A */
+  vli_mod_mult_fast(X1, X1, t5, curve->p, curve->ndigits);    /* t1 = x1*A = B */
+  vli_mod_mult_fast(X2, X2, t5, curve->p, curve->ndigits);    /* t3 = x2*A = C */
+  vli_mod_add(t5, Y2, Y1, curve->p, curve->ndigits); /* t4 = y2 + y1 */
+  vli_mod_sub(Y2, Y2, Y1, curve->p, curve->ndigits); /* t4 = y2 - y1 */
+
+  vli_mod_sub(t6, X2, X1, curve->p, curve->ndigits); /* t6 = C - B */
+  vli_mod_mult_fast(Y1, Y1, t6, curve->p, curve->ndigits);    /* t2 = y1 * (C - B) */
+  vli_mod_add(t6, X1, X2, curve->p, curve->ndigits); /* t6 = B + C */
+  vli_mod_square_fast(X2, Y2, curve->p, curve->ndigits);      /* t3 = (y2 - y1)^2 */
+  vli_mod_sub(X2, X2, t6, curve->p, curve->ndigits); /* t3 = x3 */
+
+  vli_mod_sub(t7, X1, X2, curve->p, curve->ndigits); /* t7 = B - x3 */
+  vli_mod_mult_fast(Y2, Y2, t7, curve->p, curve->ndigits);    /* t4 = (y2 - y1)*(B - x3) */
+  vli_mod_sub(Y2, Y2, Y1, curve->p, curve->ndigits); /* t4 = y3 */
+
+  vli_mod_square_fast(t7, t5, curve->p, curve->ndigits);      /* t7 = (y2 + y1)^2 = F */
+  vli_mod_sub(t7, t7, t6, curve->p, curve->ndigits); /* t7 = x3' */
+  vli_mod_sub(t6, t7, X1, curve->p, curve->ndigits); /* t6 = x3' - B */
+  vli_mod_mult_fast(t6, t6, t5, curve->p, curve->ndigits);    /* t6 = (y2 + y1)*(x3' - B) */
+  vli_mod_sub(Y1, t6, Y1, curve->p, curve->ndigits); /* t2 = y3' */
+
+  vli_set(X1, t7, curve->ndigits);
+}
+
+void ecc_point_mult(struct ecc_curve *curve, ecc_point *result, ecc_point *point, u64 *scalar, u64 *initialZ)
+{
+  /* R0 and R1 */
+  u64 Rx[2][ECC_MAX_DIGITS];
+  u64 Ry[2][ECC_MAX_DIGITS];
+  u64 z[ECC_MAX_DIGITS];
+  int i, nb;
+
+  vli_set(Rx[1], point->x, curve->ndigits);
+  vli_set(Ry[1], point->y, curve->ndigits);
+
+  XYcZ_initial_double(curve, Rx[1], Ry[1], Rx[0], Ry[0], initialZ);
+
+  for(i = vli_num_bits(scalar, curve->ndigits) - 2; i > 0; --i){
+    nb = !vli_test_bit(scalar, i, curve->ndigits);
+    XYcZ_addC(curve, Rx[1-nb], Ry[1-nb], Rx[nb], Ry[nb]);
+    XYcZ_add(curve, Rx[nb], Ry[nb], Rx[1-nb], Ry[1-nb]);
+  }
+
+  nb = !vli_test_bit(scalar, 0, curve->ndigits);
+  XYcZ_addC(curve, Rx[1-nb], Ry[1-nb], Rx[nb], Ry[nb]);
+
+  /* Find final 1/Z value. */
+  vli_mod_sub(z, Rx[1], Rx[0], curve->p, curve->ndigits); /* X1 - X0 */
+  vli_mod_mult_fast(z, z, Ry[1-nb], curve->p, curve->ndigits);     /* Yb * (X1 - X0) */
+  vli_mod_mult_fast(z, z, point->x, curve->p, curve->ndigits);   /* xP * Yb * (X1 - X0) */
+  vli_mod_inv(z, z, curve->p, curve->ndigits);            /* 1 / (xP * Yb * (X1 - X0)) */
+  vli_mod_mult_fast(z, z, point->y, curve->p, curve->ndigits);   /* yP / (xP * Yb * (X1 - X0)) */
+  vli_mod_mult_fast(z, z, Rx[1-nb], curve->p, curve->ndigits);     /* Xb * yP / (xP * Yb * (X1 - X0)) */
+  /* End 1/Z calculation */
+
+  XYcZ_add(curve, Rx[nb], Ry[nb], Rx[1-nb], Ry[1-nb]);
+
+  apply_z(curve, Rx[0], Ry[0], z);
+
+  vli_set(result->x, Rx[0], curve->ndigits);
+  vli_set(result->y, Ry[0], curve->ndigits);
+}
+
+static u32 max(u32 a, u32 b)
+{
+  return (a > b ? a : b);
+}
+
+void ecc_point_mult2(struct ecc_curve *curve, ecc_point *result, ecc_point *g, ecc_point *p, u64 *s, u64 *t)
+{
+  u64 tx[ECC_MAX_DIGITS];
+  u64 ty[ECC_MAX_DIGITS];
+  u64 tz[ECC_MAX_DIGITS];
+  u64 z[ECC_MAX_DIGITS];
+  ecc_point sum;
+  u64 *rx;
+  u64 *ry;
+  int i;
+
+  rx = result->x;
+  ry = result->y;
+
+  /* Calculate sum = G + Q. */
+  vli_set(sum.x, p->x, curve->ndigits);
+  vli_set(sum.y, p->y, curve->ndigits);
+  vli_set(tx, g->x, curve->ndigits);
+  vli_set(ty, g->y, curve->ndigits);
+
+  vli_mod_sub(z, sum.x, tx, curve->p, curve->ndigits); /* Z = x2 - x1 */
+  XYcZ_add(curve, tx, ty, sum.x, sum.y);
+  vli_mod_inv(z, z, curve->p, curve->ndigits); /* Z = 1/Z */
+  apply_z(curve, sum.x, sum.y, z);
+
+  /* Use Shamir's trick to calculate u1*G + u2*Q */
+  ecc_point *points[4] = {NULL, g, p, &sum};
+  u32 numBits = max(vli_num_bits(s, curve->ndigits), vli_num_bits(t, curve->ndigits));
+
+  ecc_point *point = points[(!!vli_test_bit(s, numBits-1, curve->ndigits))
+        | ((!!vli_test_bit(t, numBits-1, curve->ndigits)) << 1)];
+  vli_set(rx, point->x, curve->ndigits);
+  vli_set(ry, point->y, curve->ndigits);
+  vli_clear(z, curve->ndigits);
+  z[0] = 1;
+
+  for(i = numBits - 2; i >= 0; --i){
+    ecc_point_double_jacobian(curve, rx, ry, z);
+
+    int index = (!!vli_test_bit(s, i, curve->ndigits))
+      | ((!!vli_test_bit(t, i, curve->ndigits)) << 1);
+    ecc_point *point = points[index];
+    if(point){
+      vli_set(tx, point->x, curve->ndigits);
+      vli_set(ty, point->y, curve->ndigits);
+      apply_z(curve, tx, ty, z);
+      vli_mod_sub(tz, rx, tx, curve->p, curve->ndigits); /* Z = x2 - x1 */
+      XYcZ_add(curve, tx, ty, rx, ry);
+      vli_mod_mult_fast(z, z, tz, curve->p, curve->ndigits);
+    }
+  }
+
+  vli_mod_inv(z, z, curve->p, curve->ndigits); /* Z = 1/Z */
+  apply_z(curve, rx, ry, z);
+}
+
+void ecc_point_add(struct ecc_curve *curve, ecc_point *result, ecc_point *left, ecc_point *right)
+{
+  u64 x1[ECC_MAX_DIGITS];
+  u64 y1[ECC_MAX_DIGITS];
+  u64 x2[ECC_MAX_DIGITS];
+  u64 y2[ECC_MAX_DIGITS];
+  u64 z[ECC_MAX_DIGITS];
+
+  vli_set(x1, left->x, curve->ndigits);
+  vli_set(y1, left->y, curve->ndigits);
+  vli_set(x2, right->x, curve->ndigits);
+  vli_set(y2, right->y, curve->ndigits);
+
+  vli_mod_sub(z, x2, x1, curve->p, curve->ndigits); /* Z = x2 - x1 */
+
+  XYcZ_add(curve, x1, y1, x2, y2);
+  vli_mod_inv(z, z, curve->p, curve->ndigits); /* Z = 1/Z */
+  apply_z(curve, x2,y2, z);
+
+  vli_set(result->x, x2, curve->ndigits);
+  vli_set(result->y, y2, curve->ndigits);
+}
+
+void ecc_bytes2native(u64 *native, void *bytes, u8 ndigits)
+{
+  u64 *_bytes = (u64*)bytes;
+  unsigned int i;
+  unsigned int le_int = 1;
+  unsigned char* le_ch = (unsigned char*)(&le_int);
+
+  //little endian
+  if(*le_ch)
+  {
+    for(i = 0; i < ndigits/2; ++i){
+      if(native == _bytes){
+        u64 temp;
+        temp = be64_to_le64(native[i]);
+        native[i] = be64_to_le64(_bytes[ndigits - i - 1]);
+        _bytes[ndigits - i - 1] = temp;
+      }
+      else{
+        native[i] = be64_to_le64(_bytes[ndigits - i - 1]);
+        native[ndigits - i - 1] = be64_to_le64(_bytes[i]);
+      }
+    }
+  }
+  //big endian
+  else
+  {
+    for(i = 0; i < ndigits/2; ++i){
+      if(native == _bytes){
+        u64 temp;
+        temp = native[i];
+        native[i] = _bytes[ndigits - i - 1];
+        _bytes[ndigits - i - 1] = temp;
+      }
+      else{
+        native[i] = _bytes[ndigits - i - 1];
+        native[ndigits - i - 1] = _bytes[i];
+      }
+    }
+  }
+}
+
+void ecc_native2bytes(void *bytes, u64 *native, u8 ndigits)
+{
+  u64 *_bytes = (u64*)bytes;
+  unsigned int i;
+  unsigned int le_int = 1;
+  unsigned char* le_ch = (unsigned char*)(&le_int);
+
+  //little endian
+  if(*le_ch)
+  {
+    for(i = 0; i < ndigits/2; ++i){
+      if(_bytes == native){
+        u64 temp;
+        temp = le64_to_be64(_bytes[ndigits - i - 1]);
+        _bytes[ndigits - i - 1] = le64_to_be64(native[i]);
+        native[i] = temp;
+      }
+      else{
+        _bytes[i] = le64_to_be64(native[ndigits - i - 1]);
+        _bytes[ndigits - i - 1] = le64_to_be64(native[i]);
+      }
+    }
+  }
+  else
+  //big endian
+  {
+    for(i = 0; i < ndigits/2; ++i){
+      if(_bytes == native){
+        u64 temp;
+        temp = _bytes[ndigits - i - 1];
+        _bytes[ndigits - i - 1] = native[i];
+        native[i] = temp;
+      }
+      else{
+        _bytes[i] = native[ndigits - i - 1];
+        _bytes[ndigits - i - 1] = native[i];
+      }
+    }
+  }
+}
diff --git a/lib/sbi/sm/gm/random.c b/lib/sbi/sm/gm/random.c
new file mode 100644
index 0000000..1e48eeb
--- /dev/null
+++ b/lib/sbi/sm/gm/random.c
@@ -0,0 +1,18 @@
+#include "sm/gm/random.h"
+
+int vli_get_random(u8 *data, u32 len)
+{
+  int ret = 0;
+
+  //TODO: optimize it with real entropy machine
+  /*srand(0x11223344);
+  int i=0;
+  for(i=0; i < sizeof(u32)/sizeof(u8); ++i)
+  {
+    *data = (u8)rand();
+    data += 1;
+  }*/
+  *(u32*)data = 0x11223344;
+
+  return ret;
+}
diff --git a/lib/sbi/sm/gm/sm2.c b/lib/sbi/sm/gm/sm2.c
new file mode 100644
index 0000000..5a22dd7
--- /dev/null
+++ b/lib/sbi/sm/gm/sm2.c
@@ -0,0 +1,603 @@
+#include "sm/gm/random.h"
+#include "sm/gm/big.h"
+#include "sm/gm/ecc.h"
+#include "sm/gm/sm2.h"
+#include "sm/gm/sm3.h"
+#include "sbi/sbi_string.h"
+
+void *memset(void *s, int c, size_t count)
+{
+  return sbi_memset(s, c, count);
+}
+
+static int mem_cmp(char* s1, char* s2, int count)
+{
+  int i = 0;
+
+  if(!s1 || !s2)
+    return -1;
+
+  for(; i< count; ++i)
+  {
+    if(*(s1 + i) != *(s2 + i))
+      return -1;
+  }
+
+  return 0;
+}
+
+struct ecc_curve sm2_curve = {
+  .ndigits = ECC_MAX_DIGITS,
+  .g = {
+    .x = {
+      0x715A4589334C74C7ull, 0x8FE30BBFF2660BE1ull,
+      0x5F9904466A39C994ull, 0x32C4AE2C1F198119ull
+    },
+    .y = {
+      0x02DF32E52139F0A0ull, 0xD0A9877CC62A4740ull,
+      0x59BDCEE36B692153ull, 0xBC3736A2F4F6779Cull
+    },
+  },
+  .p = {
+    0xFFFFFFFFFFFFFFFFull, 0xFFFFFFFF00000000ull,
+    0xFFFFFFFFFFFFFFFFull, 0xFFFFFFFEFFFFFFFFull
+  },
+  .n = {
+    0x53BBF40939D54123ull, 0x7203DF6B21C6052Bull,
+    0xFFFFFFFFFFFFFFFFull, 0xFFFFFFFEFFFFFFFFull
+  },
+  .h = {
+    0x0000000000000001ull, 0x0000000000000000ull,
+    0x0000000000000000ull, 0x0000000000000000ull,
+  },
+  .a = {
+    0xFFFFFFFFFFFFFFFCull, 0xFFFFFFFF00000000ull,
+    0xFFFFFFFFFFFFFFFFull, 0xFFFFFFFEFFFFFFFFull
+  },
+  .b = {
+    0xDDBCBD414D940E93ull, 0xF39789F515AB8F92ull,
+    0x4D5A9E4BCF6509A7ull, 0x28E9FA9E9D9F5E34ull
+  },
+};
+
+/*x2 = 2w + (x2&(2w  1))*/
+void sm2_w(u64 *result, u64 *x)
+{
+  result[0] = x[0];
+  result[1] = x[1];
+  result[1] |= 0x80;
+  result[2] = 0;
+  result[3] = 0;
+}
+
+void sm3_kdf(u8 *Z, u32 zlen, u8 *K, u32 klen)
+{
+  u32 ct = 0x00000001;
+  u8 ct_char[32];
+  u8 *hash = K;
+  u32 i, t;
+  struct sm3_context md[1];
+
+  t = klen/ECC_NUMWORD;
+  //s4: K=Ha1||Ha2||...
+  for(i = 0; i < t; i++){
+    //s2: Hai=Hv(Z||ct)
+    sm3_init(md);
+    sm3_update(md, Z, zlen);
+    put_unaligned_be32(ct, ct_char);
+    sm3_update(md, ct_char, 4);
+    sm3_final(md, hash);
+    hash += 32;
+    ct++;
+  }
+
+  t = klen % ECC_NUMBITS;
+  if(t){
+    sm3_init(md);
+    sm3_update(md, Z, zlen);
+    put_unaligned_be32(ct, ct_char);
+    sm3_update(md, ct_char, 4);
+    sm3_final(md, ct_char);
+    sbi_memcpy(hash, ct_char, t);
+  }
+}
+
+void sm3_z(u8 *id, u32 idlen, ecc_point *pub, u8 *hash)
+{
+  u8 a[ECC_NUMWORD];
+  u8 b[ECC_NUMWORD];
+  u8 x[ECC_NUMWORD];
+  u8 y[ECC_NUMWORD];
+  u8 idlen_char[2];
+  struct sm3_context md[1];
+
+  put_unaligned_be16(idlen<<3, idlen_char);
+
+  ecc_bytes2native((u64*)a, sm2_curve.a, sm2_curve.ndigits);
+  ecc_bytes2native((u64*)b, sm2_curve.b, sm2_curve.ndigits);
+  ecc_bytes2native((u64*)x, sm2_curve.g.x, sm2_curve.ndigits);
+  ecc_bytes2native((u64*)y, sm2_curve.g.y, sm2_curve.ndigits);
+
+  sm3_init(md);
+  sm3_update(md, idlen_char, 2);
+  sm3_update(md, id, idlen);
+  sm3_update(md, a, ECC_NUMWORD);
+  sm3_update(md, b, ECC_NUMWORD);
+  sm3_update(md, x, ECC_NUMWORD);
+  sm3_update(md, y, ECC_NUMWORD);
+  sm3_update(md, (u8*)pub->x, ECC_NUMWORD);
+  sm3_update(md, (u8*)pub->y, ECC_NUMWORD);
+  sm3_final(md, hash);
+
+  return;
+}
+
+int sm2_valid_public_key(ecc_point *publicKey)
+{
+  u64 na[ECC_MAX_DIGITS] = {3}; /* a mod p = (-3) mod p */
+  u64 tmp1[ECC_MAX_DIGITS];
+  u64 tmp2[ECC_MAX_DIGITS];
+
+  if(ecc_point_is_zero(&sm2_curve, publicKey))
+    return 1;
+
+  if(vli_cmp(sm2_curve.p, publicKey->x, sm2_curve.ndigits) != 1 
+      || vli_cmp(sm2_curve.p, publicKey->y, sm2_curve.ndigits) != 1)
+    return 1;
+
+  /* tmp1 = y^2 */
+  vli_mod_square_fast(tmp1, publicKey->y, sm2_curve.p, sm2_curve.ndigits);
+  /* tmp2 = x^2 */
+  vli_mod_square_fast(tmp2, publicKey->x, sm2_curve.p, sm2_curve.ndigits);
+  /* tmp2 = x^2 + a = x^2 - 3 */
+  vli_mod_sub(tmp2, tmp2, na, sm2_curve.p, sm2_curve.ndigits);
+  /* tmp2 = x^3 + ax */
+  vli_mod_mult_fast(tmp2, tmp2, publicKey->x, sm2_curve.p, sm2_curve.ndigits);
+  /* tmp2 = x^3 + ax + b */
+  vli_mod_add(tmp2, tmp2, sm2_curve.b, sm2_curve.p, sm2_curve.ndigits);
+
+  /* Make sure that y^2 == x^3 + ax + b */
+  if(vli_cmp(tmp1, tmp2, sm2_curve.ndigits) != 0)
+    return 1;
+
+  return 0;
+}
+
+int sm2_make_prikey(u8 *prikey)
+{
+  u64 pri[ECC_MAX_DIGITS];
+  int i = 10;
+
+  do{
+    vli_get_random((u8*)pri, ECC_NUMWORD);
+    if(vli_cmp(sm2_curve.n, pri, sm2_curve.ndigits) != 1){
+      vli_sub(pri, pri, sm2_curve.n, sm2_curve.ndigits);
+    }
+
+    /* The private key cannot be 0 (mod p). */
+    if(!vli_is_zero(pri, sm2_curve.ndigits)){
+      ecc_native2bytes(prikey, pri, sm2_curve.ndigits);
+      return 0;
+    }
+  }while(i--);
+
+  return -1;
+}
+
+int sm2_make_pubkey(u8 *prikey, ecc_point *pubkey)
+{
+  ecc_point pub[1];
+  u64 pri[ECC_MAX_DIGITS];
+
+  ecc_bytes2native(pri, prikey, sm2_curve.ndigits);
+  ecc_point_mult(&sm2_curve, pub, &sm2_curve.g, pri, NULL);
+  ecc_native2bytes(pubkey->x, pub->x, sm2_curve.ndigits);
+  ecc_native2bytes(pubkey->y, pub->y, sm2_curve.ndigits);
+
+  return 0;
+}
+
+int sm2_make_keypair(u8 *prikey, ecc_point *pubkey)
+{
+  sm2_make_prikey(prikey);
+  sm2_make_pubkey(prikey, pubkey);
+  return 0;
+}
+
+int sm2_point_mult(ecc_point *G, u8 *k, ecc_point *P)
+{
+  int rc = 0;
+
+  ecc_point G_[1];
+  ecc_point P_[1];
+  u64 k_[ECC_MAX_DIGITS];
+
+  ecc_bytes2native(k_, k, sm2_curve.ndigits);
+  ecc_bytes2native(G_->x, G->x, sm2_curve.ndigits);
+  ecc_bytes2native(G_->y, G->y, sm2_curve.ndigits);
+
+  ecc_point_mult(&sm2_curve, P_, G_, k_, NULL);
+
+  ecc_native2bytes(P->x, P_->x, sm2_curve.ndigits);
+  ecc_native2bytes(P->y, P_->y, sm2_curve.ndigits);
+
+  return rc;
+}
+
+int sm2_sign(u8 *r_, u8 *s_, u8 *prikey, u8 *hash_)
+{
+  u64 k[ECC_MAX_DIGITS];
+  u64 one[ECC_MAX_DIGITS] = {1};
+  u64 random[ECC_MAX_DIGITS];
+  u64 pri[ECC_MAX_DIGITS];
+  u64 hash[ECC_MAX_DIGITS];
+  u64 r[ECC_MAX_DIGITS];
+  u64 s[ECC_MAX_DIGITS];
+
+  ecc_point p;
+
+  ecc_bytes2native(pri, prikey, sm2_curve.ndigits);
+  ecc_bytes2native(hash, hash_, sm2_curve.ndigits);
+
+  vli_get_random((u8*)random, ECC_NUMWORD);
+  if(vli_is_zero(random, sm2_curve.ndigits)){
+    /* The random number must not be 0. */
+    return 0;
+  }
+
+  vli_set(k, random, sm2_curve.ndigits);
+  if(vli_cmp(sm2_curve.n, k, sm2_curve.ndigits) != 1){
+    vli_sub(k, k, sm2_curve.n, sm2_curve.ndigits);
+  }
+
+  /* tmp = k * G */
+  ecc_point_mult(&sm2_curve, &p, &sm2_curve.g, k, NULL);
+
+  /* r = x1 + e (mod n) */
+  vli_mod_add(r, p.x, hash, sm2_curve.n, sm2_curve.ndigits);
+  if(vli_cmp(sm2_curve.n, r, sm2_curve.ndigits) != 1){
+    vli_sub(r, r, sm2_curve.n, sm2_curve.ndigits);
+  }
+
+  if(vli_is_zero(r, sm2_curve.ndigits)){
+    /* If r == 0, fail (need a different random number). */
+    return 0;
+  }
+
+  /* s = r*d */
+  vli_mod_mult(s, r, pri, sm2_curve.n, sm2_curve.ndigits);
+  /* k-r*d */
+  vli_mod_sub(s, k, s, sm2_curve.n, sm2_curve.ndigits);
+  /* 1+d */
+  vli_mod_add(pri, pri, one, sm2_curve.n, sm2_curve.ndigits);
+  /* (1+d)' */
+  vli_mod_inv(pri, pri, sm2_curve.n, sm2_curve.ndigits);
+  /* (1+d)'*(k-r*d) */
+  vli_mod_mult(s, pri, s, sm2_curve.n, sm2_curve.ndigits);
+
+  ecc_native2bytes(r_, r, sm2_curve.ndigits);
+  ecc_native2bytes(s_, s, sm2_curve.ndigits);
+
+  return 1;
+}
+
+int sm2_verify(ecc_point *pubkey, u8 *hash_, u8 *r_, u8 *s_)
+{
+  ecc_point result;
+  ecc_point pub[1];
+  u64 t[ECC_MAX_DIGITS];
+  u64 r[ECC_MAX_DIGITS];
+  u64 s[ECC_MAX_DIGITS];
+  u64 hash[ECC_MAX_DIGITS];
+
+  ecc_bytes2native(pub->x, pubkey->x, sm2_curve.ndigits);
+  ecc_bytes2native(pub->y, pubkey->y, sm2_curve.ndigits);
+  ecc_bytes2native(r, r_, sm2_curve.ndigits);
+  ecc_bytes2native(s, s_, sm2_curve.ndigits);
+  ecc_bytes2native(hash, hash_, sm2_curve.ndigits);
+
+  if(vli_is_zero(r, sm2_curve.ndigits) || vli_is_zero(s, sm2_curve.ndigits)){
+    /* r, s must not be 0. */
+    return -1;
+  }
+
+  if(vli_cmp(sm2_curve.n, r, sm2_curve.ndigits) != 1
+      || vli_cmp(sm2_curve.n, s, sm2_curve.ndigits) != 1){
+    /* r, s must be < n. */
+    return -1;
+  }
+
+  vli_mod_add(t, r, s, sm2_curve.n, sm2_curve.ndigits); // r + s
+  if(t == 0)
+    return -1;
+
+  ecc_point_mult2(&sm2_curve, &result, &sm2_curve.g, pub, s, t);
+
+  /* v = x1 + e (mod n) */
+  vli_mod_add(result.x, result.x, hash, sm2_curve.n, sm2_curve.ndigits);
+
+  if(vli_cmp(sm2_curve.n, result.x, sm2_curve.ndigits) != 1){
+    vli_sub(result.x, result.x, sm2_curve.n, sm2_curve.ndigits);
+  }
+
+  /* Accept only if v == r. */
+  return vli_cmp(result.x, r, sm2_curve.ndigits);
+}
+
+int sm2_encrypt(ecc_point *pubKey, u8 *M, u32 Mlen, u8 *C, u32 *Clen)
+{
+  u64 k[ECC_MAX_DIGITS];
+  u8 t[SM3_DATA_LEN];
+  ecc_point pub[1];
+  ecc_point *C1 = (ecc_point *)C;
+  u8 *C2 = C + ECC_NUMWORD*2;
+  u8 *C3 = C + ECC_NUMWORD*2 + Mlen;
+
+  ecc_point kP;
+  u8 *x2 = (u8*)kP.x;
+  u8 *y2 = (u8*)kP.y;
+  u8 *x2y2 = (u8*)kP.x;
+  struct sm3_context md[1];
+  int i=0;
+
+  ecc_bytes2native(pub->x, pubKey->x, sm2_curve.ndigits);
+  ecc_bytes2native(pub->y, pubKey->y, sm2_curve.ndigits);
+
+  vli_get_random((u8*)k, ECC_NUMWORD);
+
+  /* C1 = k * G */
+  ecc_point_mult(&sm2_curve, C1, &sm2_curve.g, k, NULL);
+  ecc_native2bytes(C1->x, C1->x, sm2_curve.ndigits);
+  ecc_native2bytes(C1->y, C1->y, sm2_curve.ndigits);
+
+  /* S = h * Pb */
+  ecc_point S;
+  ecc_point_mult(&sm2_curve, &S, pub, sm2_curve.h, NULL);
+  if(sm2_valid_public_key(&S) != 0)
+    return -1;
+
+  /* kP = k * Pb */
+  ecc_point_mult(&sm2_curve, &kP, pub, k, NULL);
+  if(vli_is_zero(kP.x, sm2_curve.ndigits)
+      | vli_is_zero(kP.y, sm2_curve.ndigits)){
+    return 0;
+  }
+  ecc_native2bytes(kP.x, kP.x, sm2_curve.ndigits);
+  ecc_native2bytes(kP.y, kP.y, sm2_curve.ndigits);
+
+  /* t=KDF(x2  y2, klen) */
+  sm3_kdf(x2y2, ECC_NUMWORD*2, t, Mlen);
+
+  /* C2 = M  t*/
+  for(i = 0; i < Mlen; i++){
+    C2[i] = M[i]^t[+i];
+  }
+
+  /*C3 = Hash(x2  M  y2)*/
+  sm3_init(md);
+  sm3_update(md, x2, ECC_NUMWORD);
+  sm3_update(md, M, Mlen);
+  sm3_update(md, y2, ECC_NUMWORD);
+  sm3_final(md, C3);
+
+  if(Clen)
+    *Clen = Mlen + ECC_NUMWORD*2 + SM3_DATA_LEN;
+
+  return 0;
+}
+
+int sm2_decrypt(u8 *prikey, u8 *C, u32 Clen, u8 *M, u32 *Mlen)
+{
+  u8 hash[SM3_DATA_LEN];
+  u64 pri[ECC_MAX_DIGITS];
+  ecc_point *C1 = (ecc_point *)C;
+  u8 *C2 = C + ECC_NUMWORD*2;
+  u8 *C3 = C + Clen - SM3_DATA_LEN;
+  ecc_point dB;
+  u64 *x2 = dB.x;
+  u64 *y2 = dB.y;
+  u64 *x2y2 = x2;
+  struct sm3_context md[1];
+  int outlen = Clen - ECC_NUMWORD*2 - SM3_DATA_LEN;
+  int i=0;
+
+  ecc_bytes2native(pri, prikey, sm2_curve.ndigits);
+  ecc_bytes2native(C1->x, C1->x, sm2_curve.ndigits);
+  ecc_bytes2native(C1->y, C1->y, sm2_curve.ndigits);
+
+  if(sm2_valid_public_key(C1) != 0)
+    return -1;
+
+  ecc_point S;
+  ecc_point_mult(&sm2_curve, &S, C1, sm2_curve.h, NULL);
+  if(sm2_valid_public_key(&S) != 0)
+    return -1;
+
+  ecc_point_mult(&sm2_curve, &dB, C1, pri, NULL);
+  ecc_native2bytes(x2, x2, sm2_curve.ndigits);
+  ecc_native2bytes(y2, y2, sm2_curve.ndigits);
+
+  sm3_kdf((u8*)x2y2, ECC_NUMWORD*2, M, outlen);
+  if(vli_is_zero(x2, sm2_curve.ndigits)
+      | vli_is_zero(y2, sm2_curve.ndigits)){
+    return 0;
+  }
+
+  for(i = 0; i < outlen; i++)
+    M[i]=M[i]^C2[i];
+
+  sm3_init(md);
+  sm3_update(md, (u8*)x2, ECC_NUMWORD);
+  sm3_update(md, M, outlen);
+  sm3_update(md, (u8*)y2, ECC_NUMWORD);
+  sm3_final(md, hash);
+
+  *Mlen = outlen;
+  if(mem_cmp((void*)hash , (void*)C3, SM3_DATA_LEN) != 0)
+    return -1;
+  else
+    return 0;
+}
+
+int sm2_shared_point(u8* selfPriKey,  u8* selfTempPriKey, ecc_point* selfTempPubKey,
+    ecc_point *otherPubKey, ecc_point* otherTempPubKey, ecc_point *key)
+{
+  ecc_point selfTempPub;
+  ecc_point otherTempPub;
+  ecc_point otherPub;
+  ecc_point U[1];
+
+  u64 selfTempPri[ECC_MAX_DIGITS];
+  u64 selfPri[ECC_MAX_DIGITS];
+  u64 temp1[ECC_MAX_DIGITS];
+  u64 temp2[ECC_MAX_DIGITS];
+  u64 tA[ECC_MAX_DIGITS];
+
+  ecc_bytes2native(selfTempPri, selfTempPriKey, sm2_curve.ndigits);
+  ecc_bytes2native(selfPri, selfPriKey, sm2_curve.ndigits);
+  ecc_bytes2native(selfTempPub.x, selfTempPubKey->x, sm2_curve.ndigits);
+  ecc_bytes2native(selfTempPub.y, selfTempPubKey->y, sm2_curve.ndigits);
+  ecc_bytes2native(otherTempPub.x, otherTempPubKey->x, sm2_curve.ndigits);
+  ecc_bytes2native(otherTempPub.y, otherTempPubKey->y, sm2_curve.ndigits);
+  ecc_bytes2native(otherPub.x, otherPubKey->x, sm2_curve.ndigits);
+  ecc_bytes2native(otherPub.y, otherPubKey->y, sm2_curve.ndigits);
+
+  /***********x1_=2^w+x2 & (2^w-1)*************/
+  sm2_w(temp1, selfTempPub.x);
+  /***********tA=(dA+x1_*rA)mod n *************/
+  vli_mod_mult(temp1, selfTempPri, temp1, sm2_curve.n, sm2_curve.ndigits);
+  vli_mod_add(tA, selfPri, temp1, sm2_curve.n, sm2_curve.ndigits);
+  /***********x2_=2^w+x2 & (2^w-1)*************/
+  if(sm2_valid_public_key(&otherTempPub) != 0)
+    return -1;
+  sm2_w(temp2, otherTempPub.x);
+  /**************U=[h*tA](PB+[x2_]RB)**********/
+  /* U=[x2_]RB */
+  ecc_point_mult(&sm2_curve, U, &otherTempPub, temp2, NULL);
+  /*U=PB+U*/
+  ecc_point_add(&sm2_curve, U, &otherPub, U);
+  /*tA=tA*h */
+  vli_mod_mult(tA, tA, sm2_curve.h, sm2_curve.n, sm2_curve.ndigits);
+  ecc_point_mult(&sm2_curve, U, U,tA, NULL);
+
+  ecc_native2bytes(key->x, U->x, sm2_curve.ndigits);
+  ecc_native2bytes(key->y, U->y, sm2_curve.ndigits);
+
+  return 0;
+}
+
+int sm2_shared_key(ecc_point *point, u8 *ZA, u8 *ZB, u32 keyLen, u8 *key)
+{
+  u8 Z[ECC_NUMWORD*4];
+  sbi_memcpy(Z, point->x, ECC_NUMWORD);
+  sbi_memcpy(Z + ECC_NUMWORD, point->y, ECC_NUMWORD);
+  sbi_memcpy(Z + ECC_NUMWORD*2, ZA, ECC_NUMWORD);
+  sbi_memcpy(Z + ECC_NUMWORD*3, ZB, ECC_NUMWORD);
+  sm3_kdf(Z, ECC_NUMWORD*4, key, keyLen);
+  
+  return 0;
+}
+
+/****hash = Hash(Ux||ZA||ZB||x1||y1||x2||y2)****/
+int ECC_Key_ex_hash1(u8* x, ecc_point *RA, ecc_point* RB, u8 ZA[],u8 ZB[],u8 *hash)
+{
+  struct sm3_context md[1];
+
+  sm3_init(md);
+  sm3_update(md, x, ECC_NUMWORD);
+  sm3_update(md, ZA, ECC_NUMWORD);
+  sm3_update(md, ZB, ECC_NUMWORD);
+  sm3_update(md, (u8*)RA->x, ECC_NUMWORD);
+  sm3_update(md, (u8*)RA->y, ECC_NUMWORD);
+  sm3_update(md, (u8*)RB->x, ECC_NUMWORD);
+  sm3_update(md, (u8*)RB->y, ECC_NUMWORD);
+  sm3_final(md, (u8*)hash);
+
+  return 0;
+}
+
+/****SA = Hash(temp||Uy||Hash)****/
+int ECC_Key_ex_hash2(u8 temp, u8* y,u8 *hash, u8* SA)
+{
+  struct sm3_context md[1];
+
+  sm3_init(md);
+  sm3_update(md, &temp,1);
+  sm3_update(md, y,ECC_NUMWORD);
+  sm3_update(md, hash,ECC_NUMWORD);
+  sm3_final(md, SA);
+
+  return 0;
+}
+
+int ECC_KeyEx_Init_I(u8 *pri, ecc_point *pub)
+{
+  return sm2_make_pubkey(pri, pub);
+}
+
+int ECC_KeyEx_Re_I(u8 *rb, u8 *dB, ecc_point *RA, ecc_point *PA, u8* ZA, u8 *ZB, u8 *K, u32 klen, ecc_point *RB, ecc_point *V, u8* SB)
+{
+  u8 Z[ECC_NUMWORD*2 + ECC_NUMBITS/4]={0};
+  u8 hash[ECC_NUMWORD];
+  u8 temp=0x02;
+
+  //--------B2: RB=[rb]G=(x2,y2)--------
+  sm2_make_pubkey(rb, RB);
+  /********************************************/
+  sm2_shared_point(dB,  rb, RB, PA, RA, V);
+  //------------B7:KB=KDF(VX,VY,ZA,ZB,KLEN)----------
+  sbi_memcpy(Z, V->x, ECC_NUMWORD);
+  sbi_memcpy(Z+ECC_NUMWORD, (u8*)V->y, ECC_NUMWORD);
+  sbi_memcpy(Z+ECC_NUMWORD*2, ZA,ECC_NUMWORD);
+  sbi_memcpy(Z+ECC_NUMWORD*3, ZB,ECC_NUMWORD);
+  sm3_kdf(Z,ECC_NUMWORD*4, K, klen);
+  //---------------B8:(optional) SB=hash(0x02||Vy||HASH(Vx||ZA||ZB||x1||y1||x2||y2)-------------
+  ECC_Key_ex_hash1((u8*)V->x,  RA, RB, ZA, ZB, hash);
+  ECC_Key_ex_hash2(temp, (u8*)V->y, hash, SB);
+
+  return 0;
+}
+
+int ECC_KeyEx_Init_II(u8* ra, u8* dA, ecc_point* RA, ecc_point* RB, ecc_point* PB, u8
+    ZA[],u8 ZB[],u8 SB[],u8 K[], u32 klen,u8 SA[])
+{
+  u8 Z[ECC_NUMWORD*2 + ECC_NUMWORD*2]={0};
+  u8 hash[ECC_NUMWORD],S1[ECC_NUMWORD];
+  u8 temp[2]={0x02,0x03};
+  ecc_point U[1];
+
+  /********************************************/
+  sm2_shared_point(dA, ra, RA, PB, RB, U);
+  /************KA=KDF(UX,UY,ZA,ZB,KLEN)**********/
+  sbi_memcpy(Z, U->x,ECC_NUMWORD);
+  sbi_memcpy(Z+ECC_NUMWORD, U->y,ECC_NUMWORD);
+  sbi_memcpy(Z+ECC_NUMWORD*2,ZA,ECC_NUMWORD);
+  sbi_memcpy(Z+ECC_NUMWORD*2 +ECC_NUMWORD ,ZB,ECC_NUMWORD);
+  sm3_kdf(Z,ECC_NUMWORD*2+ECC_NUMWORD*2, K, klen);
+  /****S1 = Hash(0x02||Uy||Hash(Ux||ZA||ZB||x1||y1||x2||y2))****/
+  ECC_Key_ex_hash1((u8*)U->x,  RA, RB, ZA, ZB, hash);
+  ECC_Key_ex_hash2(temp[0], (u8*)U->y, hash, S1);
+  /*test S1=SB?*/
+  if(mem_cmp((void*)S1, (void*)SB, ECC_NUMWORD)!=0)
+    return -1;
+  /*SA = Hash(0x03||yU||Hash(xU||ZA||ZB||x1||y1||x2||y2)) */
+  ECC_Key_ex_hash2(temp[1], (u8*)U->y, hash, SA);
+
+  return 0;
+}
+
+int ECC_KeyEx_Re_II(ecc_point *V, ecc_point *RA, ecc_point *RB, u8 ZA[], u8 ZB[], u8 SA[])
+{
+  u8 hash[ECC_NUMWORD];
+  u8 S2[ECC_NUMWORD];
+  u8 temp=0x03;
+
+  /*S2 = Hash(0x03||Vy||Hash(Vx||ZA||ZB||x1||y1||x2||y2))*/
+  ECC_Key_ex_hash1((u8*)V->x,  RA, RB, ZA, ZB, hash);
+  ECC_Key_ex_hash2(temp, (u8*)V->y, hash, S2);
+
+  if(mem_cmp((void*)S2, (void*)SA, ECC_NUMWORD)!=0)
+    return -1;
+
+  return 0;
+}
diff --git a/lib/sbi/sm/gm/sm3.c b/lib/sbi/sm/gm/sm3.c
new file mode 100644
index 0000000..f0bb75c
--- /dev/null
+++ b/lib/sbi/sm/gm/sm3.c
@@ -0,0 +1,325 @@
+#include "sm/gm/sm3.h"
+#include "sbi/sbi_string.h"
+
+/*
+ * 32-bit integer manipulation macros (big endian)
+ */
+#ifndef GET_ULONG_BE
+#define GET_ULONG_BE(n, b, i)                     \
+{                                                 \
+  (n) = ( (unsigned long)(b)[(i)] << 24 )         \
+  | ( (unsigned long)(b)[(i) + 1] << 16 )         \
+  | ( (unsigned long)(b)[(i) + 2] << 8  )         \
+  | ( (unsigned long)(b)[(i) + 3]       );        \
+}
+#endif
+
+#ifndef PUT_ULONG_BE
+#define PUT_ULONG_BE(n, b, i)                     \
+{                                                 \
+  (b)[(i)] = (unsigned char)((n) >> 24);          \
+  (b)[(i) + 1] = (unsigned char)((n) >> 16);      \
+  (b)[(i) + 2] = (unsigned char)((n) >>  8);      \
+  (b)[(i) + 3] = (unsigned char)((n));            \
+}
+#endif
+
+/*
+ * SM3 context setup
+ */
+void sm3_init(struct sm3_context *ctx)
+{
+  ctx->total[0] = 0;
+  ctx->total[1] = 0;
+
+  ctx->state[0] = 0x7380166F;
+  ctx->state[1] = 0x4914B2B9;
+  ctx->state[2] = 0x172442D7;
+  ctx->state[3] = 0xDA8A0600;
+  ctx->state[4] = 0xA96F30BC;
+  ctx->state[5] = 0x163138AA;
+  ctx->state[6] = 0xE38DEE4D;
+  ctx->state[7] = 0xB0FB0E4E;
+}
+
+static void sm3_process(struct sm3_context *ctx, unsigned char data[64])
+{
+  unsigned long SS1, SS2, TT1, TT2, W[68], W1[64];
+  unsigned long A, B, C, D, E, F, G, H;
+  unsigned long T[64];
+  unsigned long Temp1, Temp2, Temp3, Temp4, Temp5;
+  int j;
+
+  for(j = 0; j < 16; j++)
+    T[j] = 0x79CC4519;
+  for(j = 16; j < 64; j++)
+    T[j] = 0x7A879D8A;
+
+  GET_ULONG_BE(W[ 0], data,  0);
+  GET_ULONG_BE(W[ 1], data,  4);
+  GET_ULONG_BE(W[ 2], data,  8);
+  GET_ULONG_BE(W[ 3], data, 12);
+  GET_ULONG_BE(W[ 4], data, 16);
+  GET_ULONG_BE(W[ 5], data, 20);
+  GET_ULONG_BE(W[ 6], data, 24);
+  GET_ULONG_BE(W[ 7], data, 28);
+  GET_ULONG_BE(W[ 8], data, 32);
+  GET_ULONG_BE(W[ 9], data, 36);
+  GET_ULONG_BE(W[10], data, 40);
+  GET_ULONG_BE(W[11], data, 44);
+  GET_ULONG_BE(W[12], data, 48);
+  GET_ULONG_BE(W[13], data, 52);
+  GET_ULONG_BE(W[14], data, 56);
+  GET_ULONG_BE(W[15], data, 60);
+
+#define FF0(x,y,z) ((x) ^ (y) ^ (z))
+#define FF1(x,y,z) (((x) & (y)) | ( (x) & (z)) | ( (y) & (z)))
+
+#define GG0(x,y,z) ( (x) ^ (y) ^ (z))
+#define GG1(x,y,z) (((x) & (y)) | ( (~(x)) & (z)) )
+
+#define SHL(x,n) (((x) & 0xFFFFFFFF) << n)
+#define ROTL(x,n) (SHL((x),n) | ((x) >> (32 - n)))
+
+#define P0(x) ((x) ^  ROTL((x),9) ^ ROTL((x),17))
+#define P1(x) ((x) ^  ROTL((x),15) ^ ROTL((x),23))
+
+  for(j = 16; j < 68; j++ )
+  {
+    Temp1 = W[j - 16] ^ W[j - 9];
+    Temp2 = ROTL(W[j - 3], 15);
+    Temp3 = Temp1 ^ Temp2;
+    Temp4 = P1(Temp3);
+    Temp5 =  ROTL(W[j - 13], 7 ) ^ W[j - 6];
+    W[j] = Temp4 ^ Temp5;
+  }
+
+  for(j = 0; j < 64; j++)
+  {
+    W1[j] = W[j] ^ W[j + 4];
+  }
+
+  A = ctx->state[0];
+  B = ctx->state[1];
+  C = ctx->state[2];
+  D = ctx->state[3];
+  E = ctx->state[4];
+  F = ctx->state[5];
+  G = ctx->state[6];
+  H = ctx->state[7];
+
+  for(j = 0; j < 16; j++)
+  {
+    SS1 = ROTL((ROTL(A, 12) + E + ROTL(T[j], j)), 7);
+    SS2 = SS1 ^ ROTL(A, 12);
+    TT1 = FF0(A, B, C) + D + SS2 + W1[j];
+    TT2 = GG0(E, F, G) + H + SS1 + W[j];
+    D = C;
+    C = ROTL(B, 9);
+    B = A;
+    A = TT1;
+    H = G;
+    G = ROTL(F, 19);
+    F = E;
+    E = P0(TT2);
+  }
+
+  for(j = 16; j < 64; j++)
+  {
+    SS1 = ROTL((ROTL(A, 12) + E + ROTL(T[j], j)), 7);
+    SS2 = SS1 ^ ROTL(A, 12);
+    TT1 = FF1(A, B, C) + D + SS2 + W1[j];
+    TT2 = GG1(E, F, G) + H + SS1 + W[j];
+    D = C;
+    C = ROTL(B, 9);
+    B = A;
+    A = TT1;
+    H = G;
+    G = ROTL(F, 19);
+    F = E;
+    E = P0(TT2);
+  }
+
+  ctx->state[0] ^= A;
+  ctx->state[1] ^= B;
+  ctx->state[2] ^= C;
+  ctx->state[3] ^= D;
+  ctx->state[4] ^= E;
+  ctx->state[5] ^= F;
+  ctx->state[6] ^= G;
+  ctx->state[7] ^= H;
+}
+
+/*
+ * SM3 process buffer
+ */
+void sm3_update(struct sm3_context *ctx, unsigned char *input, int ilen)
+{
+  int fill;
+  unsigned long left;
+
+  if(ilen <= 0)
+    return;
+
+  left = ctx->total[0] & 0x3F;
+  fill = 64 - left;
+
+  ctx->total[0] += ilen;
+  ctx->total[0] &= 0xFFFFFFFF;
+
+  if(ctx->total[0] < (unsigned long)ilen)
+    ctx->total[1]++;
+
+  if(left && ilen >= fill)
+  {
+    sbi_memcpy((void *)(ctx->buffer + left),
+      (void *)input, fill);
+    sm3_process(ctx, ctx->buffer);
+    input += fill;
+    ilen -= fill;
+    left = 0;
+  }
+
+  while(ilen >= 64)
+  {
+    sm3_process( ctx, input );
+    input += 64;
+    ilen  -= 64;
+  }
+
+  if(ilen > 0)
+  {
+    sbi_memcpy((void*)(ctx->buffer + left),
+      (void*)input, ilen);
+  }
+}
+
+static const unsigned char sm3_padding[64] =
+{
+  0x80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+};
+
+/*
+ * SM3 final digest
+ */
+void sm3_final(struct sm3_context *ctx, unsigned char output[32])
+{
+  unsigned long last, padn;
+  unsigned long high, low;
+  unsigned char msglen[8];
+
+  high = (ctx->total[0] >> 29)
+    | (ctx->total[1] << 3);
+  low = (ctx->total[0] << 3);
+
+  PUT_ULONG_BE( high, msglen, 0 );
+  PUT_ULONG_BE( low,  msglen, 4 );
+
+  last = ctx->total[0] & 0x3F;
+  padn = (last < 56) ? (56 - last) : (120 - last);
+
+  sm3_update( ctx, (unsigned char *) sm3_padding, padn );
+  sm3_update( ctx, msglen, 8 );
+
+  PUT_ULONG_BE( ctx->state[0], output,  0 );
+  PUT_ULONG_BE( ctx->state[1], output,  4 );
+  PUT_ULONG_BE( ctx->state[2], output,  8 );
+  PUT_ULONG_BE( ctx->state[3], output, 12 );
+  PUT_ULONG_BE( ctx->state[4], output, 16 );
+  PUT_ULONG_BE( ctx->state[5], output, 20 );
+  PUT_ULONG_BE( ctx->state[6], output, 24 );
+  PUT_ULONG_BE( ctx->state[7], output, 28 );
+}
+
+/*
+ * output = SM3( input buffer )
+ */
+void sm3(unsigned char *input, int ilen,
+          unsigned char output[32])
+{
+  struct sm3_context ctx;
+
+  sm3_init(&ctx);
+  sm3_update(&ctx, input, ilen);
+  sm3_final(&ctx, output);
+
+  sbi_memset(&ctx, 0, sizeof(struct sm3_context));
+}
+
+/*
+ * SM3 HMAC context setup
+ */
+void sm3_hmac_init(struct sm3_context *ctx, unsigned char *key, int keylen)
+{
+  int i;
+  unsigned char sum[32];
+
+  if(keylen > 64)
+  {
+    sm3(key, keylen, sum);
+    keylen = 32;
+    //keylen = ( is224 ) ? 28 : 32;
+    key = sum;
+  }
+
+  sbi_memset(ctx->ipad, 0x36, 64);
+  sbi_memset(ctx->opad, 0x5C, 64);
+
+  for(i = 0; i < keylen; i++)
+  {
+    ctx->ipad[i] = (unsigned char)(ctx->ipad[i] ^ key[i]);
+    ctx->opad[i] = (unsigned char)(ctx->opad[i] ^ key[i]);
+  }
+
+  sm3_init(ctx);
+  sm3_update(ctx, ctx->ipad, 64);
+
+  sbi_memset(sum, 0, sizeof(sum));
+}
+
+/*
+ * SM3 HMAC process buffer
+ */
+void sm3_hmac_update(struct sm3_context *ctx, unsigned char *input, int ilen)
+{
+  sm3_update(ctx, input, ilen);
+}
+
+/*
+ * SM3 HMAC final digest
+ */
+void sm3_hmac_final(struct sm3_context *ctx, unsigned char output[32])
+{
+  int hlen;
+  unsigned char tmpbuf[32];
+
+  //is224 = ctx->is224;
+  hlen =  32;
+
+  sm3_final(ctx, tmpbuf);
+  sm3_init(ctx);
+  sm3_update(ctx, ctx->opad, 64);
+  sm3_update(ctx, tmpbuf, hlen);
+  sm3_final(ctx, output);
+
+  sbi_memset(tmpbuf, 0, sizeof(tmpbuf));
+}
+
+/*
+ * output = HMAC-SM#( hmac key, input buffer )
+ */
+void sm3_hmac(unsigned char *key, int keylen,
+    unsigned char *input, int ilen,
+    unsigned char output[32])
+{
+  struct sm3_context ctx;
+
+  sm3_hmac_init(&ctx, key, keylen);
+  sm3_hmac_update(&ctx, input, ilen);
+  sm3_hmac_final(&ctx, output);
+
+  sbi_memset(&ctx, 0, sizeof(struct sm3_context));
+}
diff --git a/lib/sbi/sm/platform/pt_area/platform.c b/lib/sbi/sm/platform/pt_area/platform.c
new file mode 100644
index 0000000..a07b0d8
--- /dev/null
+++ b/lib/sbi/sm/platform/pt_area/platform.c
@@ -0,0 +1,31 @@
+#include "sm/platform/pt_area/platform_thread.h"
+#include "sm/pmp.h"
+#include "sm/sm.h"
+
+/**
+ * \brief It uses two PMP regions.
+ * Region-0 is for protecting secure monitor's memory
+ * Region-last is for allowing host kernel to access any other mem.
+ *
+ */
+int platform_init()
+{
+  clear_pmp(0);
+
+  ///config the PMP 0 to protect security monitor
+  struct pmp_config_t pmp_config;
+  pmp_config.paddr = (uintptr_t)SM_BASE;
+  pmp_config.size = (unsigned long)SM_SIZE;
+  pmp_config.mode = PMP_A_NAPOT;
+  pmp_config.perm = PMP_NO_PERM;
+  set_pmp(0, pmp_config);
+
+  ///config the last PMP to allow kernel to access memory
+  pmp_config.paddr = 0;
+  pmp_config.size = -1UL;
+  pmp_config.mode = PMP_A_NAPOT;
+  pmp_config.perm = PMP_R | PMP_W | PMP_X;
+  set_pmp(NPMP-1, pmp_config);
+
+  return 0;
+}
diff --git a/lib/sbi/sm/platform/pt_area/platform_thread.c b/lib/sbi/sm/platform/pt_area/platform_thread.c
new file mode 100644
index 0000000..43161b3
--- /dev/null
+++ b/lib/sbi/sm/platform/pt_area/platform_thread.c
@@ -0,0 +1,56 @@
+#include "sm/platform/pt_area/platform_thread.h"
+#include "sm/enclave.h"
+#include "sbi/riscv_encoding.h"
+
+void platform_enter_enclave_world()
+{
+  ///TODO: add register to indicate whether in encalve world or not
+  return;
+}
+
+void platform_exit_enclave_world()
+{
+  ///TODO: add register to indicate whether in encalve world or not
+  return;
+}
+
+int platform_check_in_enclave_world()
+{
+  ///TODO: add register to indicate whether in encalve world or not
+  return 0;
+}
+
+/**
+ * \brief Compare the used satp and the enclave ptbr (encl_ptr)
+ * It's supposed to be equal.
+ *
+ * \param enclave the check enclave
+ */
+int platform_check_enclave_authentication(struct enclave_t* enclave)
+{
+  if(enclave->thread_context.encl_ptbr != csr_read(CSR_SATP))
+    return -1;
+  return 0;
+}
+
+/**
+ * \brief Switch to enclave's ptbr (enclave_ptbr).
+ *
+ * \param thread the current enclave thread.
+ * \param enclave_ptbr the  enclave ptbr value.
+ */
+void platform_switch_to_enclave_ptbr(struct thread_state_t* thread, uintptr_t enclave_ptbr)
+{
+  csr_write(CSR_SATP, enclave_ptbr);
+}
+
+/**
+ * \brief Switch to host's ptbr (host_ptbr).
+ *
+ * \param thread the current enclave thread.
+ * \param host_ptbr the  host ptbr value.
+ */
+void platform_switch_to_host_ptbr(struct thread_state_t* thread, uintptr_t host_ptbr)
+{
+  csr_write(CSR_SATP, host_ptbr);
+}
diff --git a/lib/sbi/sm/pmp.c b/lib/sbi/sm/pmp.c
new file mode 100644
index 0000000..66cc02c
--- /dev/null
+++ b/lib/sbi/sm/pmp.c
@@ -0,0 +1,240 @@
+#include "sm/pmp.h"
+#include "sm/ipi.h"
+#include "sbi/riscv_asm.h"
+#include "sbi/sbi_pmp.h"
+#include "sbi/sbi_console.h"
+
+/**
+ * \brief Set pmp and sync all harts.
+ * 
+ * \param pmp_idx_arg The pmp index.
+ * \param pmp_config_arg The pmp config.
+ */
+void set_pmp_and_sync(int pmp_idx_arg, struct pmp_config_t pmp_config_arg)
+{
+  struct pmp_data_t pmp_data;
+  u32 source_hart = current_hartid();
+
+  //set current hart's pmp
+  set_pmp(pmp_idx_arg, pmp_config_arg);
+  //sync all other harts
+  SBI_PMP_DATA_INIT(&pmp_data, pmp_config_arg, pmp_idx_arg, source_hart);
+  sbi_send_pmp(0xFFFFFFFF&(~(1<<source_hart)), 0, &pmp_data);
+  return;
+}
+
+/**
+ * \brief Clear pmp and sync all harts.
+ * 
+ * \param pmp_idx_arg The pmp index.
+ */
+void clear_pmp_and_sync(int pmp_idx)
+{
+  struct pmp_config_t pmp_config = {0,};
+  
+  pmp_config.mode = PMP_OFF;
+  set_pmp_and_sync(pmp_idx, pmp_config);
+
+  return;
+}
+
+//TODO Only handle for the __riscv_64
+void set_pmp_reg(int pmp_idx, uintptr_t* pmp_address, uintptr_t* pmp_config)
+{
+  uintptr_t tmp_pmp_address, tmp_pmp_config;
+  tmp_pmp_address = *pmp_address;
+  tmp_pmp_config = *pmp_config;
+  switch(pmp_idx)
+  {
+    case 0: 
+      PMP_SET(0, 0, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 1: 
+      PMP_SET(1, 0, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 2: 
+      PMP_SET(2, 0, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 3: 
+      PMP_SET(3, 0, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 4: 
+      PMP_SET(4, 0, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 5: 
+      PMP_SET(5, 0, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 6: 
+      PMP_SET(6, 0, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 7: 
+      PMP_SET(7, 0, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 8: 
+      PMP_SET(8, 2, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 9: 
+      PMP_SET(9, 2, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 10: 
+      PMP_SET(10, 2, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 11: 
+      PMP_SET(11, 2, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 12: 
+      PMP_SET(12, 2, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 13: 
+      PMP_SET(13, 2, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 14: 
+      PMP_SET(14, 2, tmp_pmp_address, tmp_pmp_config);
+      break;
+    case 15: 
+      PMP_SET(15, 2, tmp_pmp_address, tmp_pmp_config);
+      break;
+    default:
+      break;
+  }
+  *pmp_address = tmp_pmp_address;
+  *pmp_config = tmp_pmp_config;
+}
+
+/**
+ * \brief set current hart's pmp
+ *
+ * \param pmp_idx the index of target PMP register
+ * \param pmp_cfg the configuration of the PMP register
+ */
+
+#define PMP_CONFIG_OFFSET(pmp_idx) ((uintptr_t)PMPCFG_BIT_NUM * (pmp_idx % PMP_PER_CFG_REG))
+void set_pmp(int pmp_idx, struct pmp_config_t pmp_cfg)
+{
+  uintptr_t pmp_address = 0;
+  uintptr_t pmp_config = ((pmp_cfg.mode & PMP_A) | (pmp_cfg.perm & (PMP_R|PMP_W|PMP_X))) << PMP_CONFIG_OFFSET(pmp_idx);
+
+  switch(pmp_cfg.mode)
+  {
+    case PMP_A_TOR:
+      pmp_address = pmp_cfg.paddr;
+      break;
+    case PMP_A_NA4:
+      pmp_address = pmp_cfg.paddr;
+    case PMP_A_NAPOT:
+      if(pmp_cfg.paddr == 0 && pmp_cfg.size == -1UL)
+        pmp_address = -1UL;
+      else
+        pmp_address = (pmp_cfg.paddr | ((pmp_cfg.size>>1)-1)) >> 2;
+      break;
+    case PMP_OFF:
+      pmp_address = 0;
+      break;
+    default:
+      pmp_address = 0;
+      break;
+  }
+  set_pmp_reg(pmp_idx, &pmp_address, &pmp_config);
+
+  return;
+}
+
+/**
+ * \brief clear the configuration of a PMP register
+ *
+ * \param pmp_idx the index of target PMP register
+ */
+void clear_pmp(int pmp_idx)
+{
+  struct pmp_config_t pmp_cfg;
+
+  pmp_cfg.mode = PMP_OFF;
+  pmp_cfg.perm = PMP_NO_PERM;
+  pmp_cfg.paddr = 0;
+  pmp_cfg.size = 0;
+  set_pmp(pmp_idx, pmp_cfg);
+
+  return;
+}
+
+/**
+ * \brief Get the configuration of a pmp register (pmp_idx)
+ *
+ * \param pmp_idx the index of target PMP register
+ */
+struct pmp_config_t get_pmp(int pmp_idx)
+{
+  struct pmp_config_t pmp = {0,};
+  uintptr_t pmp_address = 0;
+  uintptr_t pmp_config = 0;
+  unsigned long order = 0;
+  unsigned long size = 0;
+
+  set_pmp_reg(pmp_idx, &pmp_address, &pmp_config);
+
+  pmp_config >>= (uintptr_t)PMPCFG_BIT_NUM * (pmp_idx % PMP_PER_CFG_REG);
+  pmp_config &= PMPCFG_BITS;
+  switch(pmp_config & PMP_A)
+  {
+    case PMP_A_TOR:
+      break;
+    case PMP_A_NA4:
+      size = 4;
+      break;
+    case PMP_A_NAPOT:
+      while(pmp_address & 1)
+      {
+        order += 1;
+        pmp_address >>= 1;
+      }
+      order += 3;
+      size = 1 << order;
+      pmp_address <<= (order-1);
+      break;
+    case PMP_OFF:
+      pmp_address = 0;
+      size = 0;
+      break;
+  }
+
+  pmp.mode = pmp_config & PMP_A;
+  pmp.perm = pmp_config & (PMP_R | PMP_W | PMP_X);
+  pmp.paddr = pmp_address;
+  pmp.size = size;
+
+  return pmp;
+}
+
+/**
+ * \brief Check the validness of a range to be PMP config
+ *  	  e.g., the size should be powers of 2
+ *
+ * \param paddr the start address of the PMP region
+ * \param size the size of the PMP region
+ */
+int illegal_pmp_addr(uintptr_t paddr, uintptr_t size)
+{
+  if(paddr & (size - 1))
+    return -1;
+  
+  if((size == 0) || (size & (size - 1)))
+    return -1;
+
+  if(size < RISCV_PGSIZE)
+    return -1;
+
+  return 0;
+}
+
+//check whether two regions are overlapped
+int region_overlap(uintptr_t pa_0, uintptr_t size_0, uintptr_t pa_1, uintptr_t size_1)
+{
+  return (pa_0 <= pa_1 && (pa_0 + size_0) > pa_1) || (pa_1 <= pa_0 && (pa_1 + size_1) > pa_0);
+}
+
+//check whether two regions are included
+int region_contain(uintptr_t pa_0, uintptr_t size_0, uintptr_t pa_1, uintptr_t size_1)
+{
+  return (pa_0 <= pa_1 && (pa_0 + size_0) >= (pa_1 + size_1))
+    || (pa_1 <= pa_0 && (pa_1 + size_1) >= (pa_0 + size_0));
+}
diff --git a/lib/sbi/sm/relay_page.c b/lib/sbi/sm/relay_page.c
new file mode 100644
index 0000000..1791b98
--- /dev/null
+++ b/lib/sbi/sm/relay_page.c
@@ -0,0 +1,221 @@
+#include "sbi/sbi_console.h"
+#include "sm/sm.h"
+#include "sm/enclave.h"
+#include "sm/enclave_vm.h"
+#include "sm/server_enclave.h"
+#include "sm/ipi.h"
+#include "sm/relay_page.h"
+#include "sm/enclave_mm.h"
+
+/**************************************************************/
+/*                                 called by enclave                                                        */
+/**************************************************************/
+
+/**
+ * \brief Monitor is responsible for change the relay page ownership:
+ * It can be divide into two phases: First  umap the relay page for caller
+ * enclave, and map the relay page for subsequent enclave asynchronously.
+ * Second, change the relay page ownership entry in the relay page linke memory. 
+ *
+ * note: the relay_page_addr_u is the virtual address of relay page. However in the relay page entry,
+ * it binds the enclave name with the physical address.
+ * 
+ * The first enclave in the enclave call chain can only hold single relay page region (now version),
+ * but can split to atmost N piece ans transfer to different enclaves. The following enclave can receive\
+ * multiple relay page entry.
+ * 
+ * \param enclave The enclave structure.
+ * \param relay_page_addr_u The relay page address.
+ * \param relay_page_size The relay page size.
+ * \param enclave_name_u The given enclave name.
+ */
+uintptr_t transfer_relay_page(struct enclave_t *enclave, unsigned long relay_page_addr_u, unsigned long relay_page_size, char *enclave_name_u)
+{
+  uintptr_t ret = 0;
+  char *enclave_name = NULL;
+  unsigned long relay_page_addr = 0;
+
+  enclave_name = va_to_pa((uintptr_t*)(enclave->root_page_table), enclave_name_u);
+  relay_page_addr = (unsigned long)va_to_pa((uintptr_t*)(enclave->root_page_table), (char *)relay_page_addr_u);
+  if(!enclave_name)
+  {
+    ret = -1UL;
+    goto failed;
+  }
+  //unmap the relay page for call enclave
+  unmap((uintptr_t*)(enclave->root_page_table), relay_page_addr_u, relay_page_size);  
+  for (int kk = 0; kk < 5; kk++)
+  {
+    if(enclave->mm_arg_paddr[kk] == relay_page_addr)
+    {
+      enclave->mm_arg_paddr[kk] = 0;
+      enclave->mm_arg_size[kk] = 0;
+    }
+  }
+
+  //change the relay page ownership
+  if (change_relay_page_ownership((unsigned long)relay_page_addr, relay_page_size, enclave_name) < 0)
+  {
+    ret = -1UL;
+    sbi_bug("M mode: transfer_relay_page: change relay page ownership failed\n");
+  }
+  release_enclave_metadata_lock();
+  return ret;
+failed:
+  release_enclave_metadata_lock();
+  sbi_bug("M MODE: transfer_relay_page: failed\n");
+  return ret;
+}
+
+/**
+ * \brief Handle the asyn enclave call. Obtain the corresponding relay page virtual address and size, and
+ *  invoke the transfer_relay_page.
+ *
+ * \param enclave_name The callee enclave name  
+ */
+uintptr_t asyn_enclave_call(uintptr_t* regs, uintptr_t enclave_name, uintptr_t arg)
+{
+  uintptr_t ret = 0;
+  struct enclave_t *enclave = NULL;
+  int eid = 0;
+  if(check_in_enclave_world() < 0)
+  {
+    sbi_bug("M mode: asyn_enclave_call: CPU not in the enclave mode\n");
+    return -1UL;
+  }
+
+  acquire_enclave_metadata_lock();
+
+  eid = get_curr_enclave_id();
+  enclave = __get_enclave(eid);
+  if(!enclave)
+  {
+    ret = -1UL;
+    goto failed;
+  }
+  struct call_enclave_arg_t call_arg;
+  struct call_enclave_arg_t* call_arg0 = va_to_pa((uintptr_t*)(enclave->root_page_table), (void*)arg);
+  if(!call_arg0)
+  {
+    ret = -1UL;
+    goto failed;
+  }
+  copy_from_host(&call_arg, call_arg0, sizeof(struct call_enclave_arg_t));
+  if (transfer_relay_page(enclave, call_arg.req_vaddr, call_arg.req_size, (char *)enclave_name) < 0)
+  {
+    sbi_bug("M mode: asyn_enclave_call: transfer relay page is failed\n");
+    goto failed;
+  }
+  release_enclave_metadata_lock();
+  return ret;
+failed:
+  release_enclave_metadata_lock();
+  sbi_bug("M MODE: asyn_enclave_call: failed\n");
+  return ret;
+}
+
+/**
+ * \brief Split relay page into two pieces:
+ * it will update the relay page entry in the global link memory,
+ * and add a new splitted entry. Also, it will update the enclave->mm_arg_paddr
+ * and enclave->mm_arg_size. If the relay page owned by single enclave is upper
+ * than RELAY_PAGE_NUM, an error will be reported.
+ *
+ * \param mem_addr_u The split memory address.
+ * \param mem_size The split memory size.
+ * \param split_addr_u Thesplit point in the memory region.
+ */
+uintptr_t split_mem_region(uintptr_t *regs, uintptr_t mem_addr_u, uintptr_t mem_size, uintptr_t split_addr_u)
+{
+  uintptr_t ret = 0;
+  struct enclave_t *enclave = NULL;
+  uintptr_t mem_addr = 0, split_addr = 0;
+  int eid = 0;
+  if(check_in_enclave_world() < 0)
+  {
+    sbi_bug("M mode: split_mem_region: CPU not in the enclave mode\n");
+    return -1UL;
+  }
+
+  acquire_enclave_metadata_lock();
+
+  eid = get_curr_enclave_id();
+  enclave = __get_enclave(eid);
+  if(!enclave)
+  {
+    ret = -1UL;
+    goto failed;
+  }
+  if((split_addr_u < mem_addr_u) || (split_addr_u > (mem_addr_u + mem_size)))
+  {
+    sbi_bug("M mode: split_mem_region: split address is not in the relay page region \n");
+    ret = -1UL;
+    goto failed;
+  }
+  mem_addr = (unsigned long)va_to_pa((uintptr_t*)(enclave->root_page_table), (char *)mem_addr_u);
+  split_addr = (unsigned long)va_to_pa((uintptr_t*)(enclave->root_page_table), (char *)split_addr_u);
+  int found_corres_entry = 0;
+  for(int kk = 0; kk < RELAY_PAGE_NUM; kk++)
+  {
+    if ((enclave->mm_arg_paddr[kk] == mem_addr) && (enclave->mm_arg_size[kk] == mem_size))
+    {
+      unsigned long split_size = enclave->mm_arg_paddr[kk] + enclave->mm_arg_size[kk] - split_addr;
+      int found_empty_entry = 0;
+      //free the old relay page entry in the global link memory
+      __free_relay_page_entry(enclave->mm_arg_paddr[kk], enclave->mm_arg_size[kk]);
+      //adjust the relay page region for enclave metadata
+      enclave->mm_arg_size[kk] = split_addr - enclave->mm_arg_paddr[kk];
+      //add the adjusted relay page entry in the global link memory
+      __alloc_relay_page_entry(enclave->enclave_name, enclave->mm_arg_paddr[kk], enclave->mm_arg_size[kk]);
+      //find the empty relay page entry for this enclave 
+      sbi_bug("M mode: split_mem_region1: split addr %lx split size %lx \n", enclave->mm_arg_paddr[kk], enclave->mm_arg_size[kk]);
+      for(int jj = kk; jj < RELAY_PAGE_NUM; jj++)
+      {
+        if ((enclave->mm_arg_paddr[jj] == 0) && (enclave->mm_arg_size[jj] == 0))
+        {
+          //add the new splitted relay page entry in the enclave metadata
+          enclave->mm_arg_paddr[jj] = split_addr;
+          enclave->mm_arg_size[jj] = split_size;
+          sbi_printf("M mode: split_mem_region2: split addr %lx split size %lx \n", enclave->mm_arg_paddr[jj], enclave->mm_arg_size[jj]);
+          __alloc_relay_page_entry(enclave->enclave_name, enclave->mm_arg_paddr[jj], enclave->mm_arg_size[jj]);
+          found_empty_entry = 1;
+          break;
+        }
+      }
+      if (!found_empty_entry)
+      {
+        sbi_bug("M mode: split mem region: can not find the empty entry for splitted relay page \n");
+        ret = -1UL;
+        goto failed;
+      }
+      found_corres_entry = 1;
+      break;
+    }
+  }
+  if (!found_corres_entry)
+  {
+    sbi_bug("M mode: split mem region: can not find the correspongind relay page region\n");
+    ret = -1UL;
+    goto failed;
+  }
+  release_enclave_metadata_lock();
+  return ret;
+failed:
+  release_enclave_metadata_lock();
+  sbi_bug("M MODE: split_mem_region: failed\n");
+  return ret;
+}
+
+int free_all_relay_page(unsigned long *mm_arg_paddr, unsigned long *mm_arg_size)
+{
+  int ret = 0;
+  for(int kk = 0; kk < RELAY_PAGE_NUM; kk++)
+  {
+    if (mm_arg_paddr[kk])
+    {
+      ret = __free_secure_memory(mm_arg_paddr[kk], mm_arg_size[kk]);
+      ret = __free_relay_page_entry(mm_arg_paddr[kk], mm_arg_size[kk]);
+    }
+  }
+  return ret;
+}
\ No newline at end of file
diff --git a/lib/sbi/sm/server_enclave.c b/lib/sbi/sm/server_enclave.c
new file mode 100644
index 0000000..2107188
--- /dev/null
+++ b/lib/sbi/sm/server_enclave.c
@@ -0,0 +1,469 @@
+#include "sm/sm.h"
+#include "sm/enclave.h"
+#include "sm/enclave_vm.h"
+#include "sm/enclave_mm.h"
+#include "sm/server_enclave.h"
+#include "sm/ipi.h"
+#include "sbi/sbi_string.h"
+#include "sbi/sbi_console.h"
+
+struct link_mem_t* server_enclave_head = NULL;
+struct link_mem_t* server_enclave_tail = NULL;
+
+// Compare the server enclave name.
+static int server_name_cmp(char* name1, char* name2)
+{
+  for(int i=0; i<NAME_LEN; ++i)
+  {
+    if(name1[i] != name2[i])
+    {
+      return 1;
+    }
+    if(name1[i] == 0)
+    {
+      return 0;
+    }
+  }
+  return 0;
+}
+
+/**
+ * \brief Allocate a server enclave with the given name from the server enclave list.
+ *
+ * \param server_name The given server enclave name.
+ */
+static struct server_enclave_t* __alloc_server_enclave(char *server_name)
+{
+  struct enclave_t* enclave = __alloc_enclave();
+  if(!enclave)
+    return NULL;
+
+  struct link_mem_t *cur, *next;
+  struct server_enclave_t* server_enclave = NULL;
+  int found = 0;
+
+  //server_enclave metadata list hasn't be initialized yet
+  if(server_enclave_head == NULL)
+  {
+    server_enclave_head = init_mem_link(sizeof(struct server_enclave_t)*SERVERS_PER_METADATA_REGION, sizeof(struct server_enclave_t));
+    if(!server_enclave_head)
+    {
+      goto failed;
+    }
+    server_enclave_tail = server_enclave_head;
+  }
+
+  //check whether server name already existed
+  for(cur = server_enclave_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(int i = 0; i < (cur->slab_num); i++)
+    {
+      server_enclave = (struct server_enclave_t*)(cur->addr) + i;
+      if(server_enclave->entity && server_name_cmp(server_name, server_enclave->server_name)==0)
+      {
+        sbi_bug("M mode: __alloc_server_enclave: server already existed!\n");
+        server_enclave = (void*)(-1UL);
+        goto failed;
+      }
+    }
+  }
+
+  found = 0;
+  for(cur = server_enclave_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(int i = 0; i < (cur->slab_num); i++)
+    {
+      server_enclave = (struct server_enclave_t*)(cur->addr) + i;
+      if(!(server_enclave->entity))
+      {
+        sbi_memcpy(server_enclave->server_name, server_name, NAME_LEN);
+        server_enclave->entity = enclave;
+        found = 1;
+        break;
+      }
+    }
+    if(found)
+      break;
+  }
+
+  //don't have enough enclave metadata
+  if(!found)
+  {
+    next = add_link_mem(&server_enclave_tail);
+    if(next == NULL)
+    {
+      sbi_bug("M mode: __alloc_server_enclave: don't have enough mem\n");
+      server_enclave = NULL;
+      goto failed;
+    }
+    server_enclave = (struct server_enclave_t*)(next->addr);
+    sbi_memcpy(server_enclave->server_name, server_name, NAME_LEN);
+    server_enclave->entity = enclave;
+  }
+
+  return server_enclave;
+
+failed:
+  if(enclave)
+    __free_enclave(enclave->eid);
+  if(server_enclave == (void *)-1UL)
+    return (void *)-1UL;
+  if(server_enclave)
+    sbi_memset((void*)server_enclave, 0, sizeof(struct server_enclave_t));
+
+  return NULL;
+}
+
+/**
+ * \brief Get the server enclav by eid.
+ *
+ * \param eid The server enclave id.
+ */
+static struct server_enclave_t* __get_server_enclave(int eid)
+{
+  struct link_mem_t *cur;
+  struct server_enclave_t *server_enclave;
+  int found;
+
+  found = 0;
+  for(cur = server_enclave_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(int i=0; i < (cur->slab_num); ++i)
+    {
+      server_enclave = (struct server_enclave_t*)(cur->addr) + i;
+      if(server_enclave->entity && server_enclave->entity->eid == eid)
+      {
+        found = 1;
+        break;
+      }
+    }
+  }
+
+  //haven't alloc this eid 
+  if(!found)
+  {
+    sbi_bug("M mode: __get_server_enclave: haven't alloc this enclave:%d\r\n", eid);
+    server_enclave = NULL;
+  }
+
+  return server_enclave;
+}
+
+/**
+ * \brief Get the server enclave by the given name.
+ *
+ * \param server_name The given server enclave name.
+ */
+static struct server_enclave_t* __get_server_enclave_by_name(char* server_name)
+{
+  struct link_mem_t *cur;
+  struct server_enclave_t *server_enclave;
+  int i, found;
+
+  found = 0;
+  for(cur = server_enclave_head; cur != NULL; cur = cur->next_link_mem)
+  {
+    for(i=0; i < (cur->slab_num); ++i)
+    {
+      server_enclave = (struct server_enclave_t*)(cur->addr) + i;
+      if(server_enclave->entity && server_name_cmp(server_enclave->server_name, server_name)==0)
+      {
+        found = 1;
+        break;
+      }
+    }
+  }
+
+  //haven't alloc this eid 
+  if(!found)
+  {
+    sbi_bug("M mode: __get_server_enclave_by_name: haven't alloc this enclave:%s\n", server_name);
+    server_enclave = NULL;
+  }
+
+  return server_enclave;
+}
+
+/**************************************************************/
+/*                   called by host                           */
+/**************************************************************/
+/**
+ * \brief Create the server enclave with the given create argument.
+ *
+ * \param create_args The create argument using in the creating server enclave phase.
+ */
+uintptr_t create_server_enclave(enclave_create_param_t create_args)
+{
+  struct enclave_t* enclave = NULL;
+  struct server_enclave_t* server_enclave = NULL;
+  uintptr_t ret = 0;
+  int need_free_secure_memory = 0;
+
+  acquire_enclave_metadata_lock();
+
+  if(!enable_enclave())
+  {
+    ret = ENCLAVE_ERROR;
+    goto failed;
+  }
+  if((create_args.paddr & (RISCV_PGSIZE-1)) || (create_args.size & (RISCV_PGSIZE-1)) || create_args.size < RISCV_PGSIZE)
+  {
+    ret = ENCLAVE_ERROR;
+    goto failed;
+  }
+
+  //check enclave memory layout
+  if(check_and_set_secure_memory(create_args.paddr, create_args.size) != 0)
+  {
+    ret = ENCLAVE_ERROR;
+    goto failed;
+  }
+  need_free_secure_memory = 1;
+
+  //check enclave memory layout
+  if(check_enclave_layout(create_args.paddr + RISCV_PGSIZE, 0, -1UL, create_args.paddr, create_args.paddr + create_args.size) != 0)
+  {
+    ret = ENCLAVE_ERROR;
+    goto failed;
+  }
+
+  server_enclave = __alloc_server_enclave(create_args.name);
+  if(server_enclave == (void*)(-1UL))
+  {
+    ret = ENCLAVE_ERROR;
+    goto failed;
+  }
+  if(!server_enclave)
+  {
+    ret = ENCLAVE_NO_MEM;
+    goto failed;
+  }
+
+  enclave = server_enclave->entity;
+  enclave->entry_point = create_args.entry_point;
+  enclave->ocall_func_id = create_args.ecall_arg0;
+  enclave->ocall_arg0 = create_args.ecall_arg1;
+  enclave->ocall_arg1 = create_args.ecall_arg2;
+  enclave->ocall_syscall_num = create_args.ecall_arg3;
+  enclave->host_ptbr = csr_read(CSR_SATP);
+  enclave->root_page_table = create_args.paddr + RISCV_PGSIZE;
+  enclave->thread_context.encl_ptbr = ((create_args.paddr+RISCV_PGSIZE) >> RISCV_PGSHIFT) | SATP_MODE_CHOICE;
+  enclave->type = SERVER_ENCLAVE;
+  //we directly set server_enclave's state as RUNNABLE as it won't be called by run_enclave call
+  enclave->state = RUNNABLE;
+  enclave->caller_eid = -1;
+  enclave->top_caller_eid = -1;
+  enclave->cur_callee_eid = -1;
+
+  //traverse vmas
+  struct pm_area_struct* pma = (struct pm_area_struct*)(create_args.paddr);
+  struct vm_area_struct* vma = (struct vm_area_struct*)(create_args.paddr + sizeof(struct pm_area_struct));
+  pma->paddr = create_args.paddr;
+  pma->size = create_args.size;
+  pma->free_mem = create_args.free_mem;
+  pma->pm_next = NULL;
+  enclave->pma_list = pma;
+  traverse_vmas(enclave->root_page_table, vma);
+
+  //FIXME: here we assume there are exactly text(include text/data/bss) vma and stack vma
+  while(vma)
+  {
+    if(vma->va_start == ENCLAVE_DEFAULT_TEXT_BASE)
+    {
+      enclave->text_vma = vma;
+    }
+    if(vma->va_end == ENCLAVE_DEFAULT_STACK_BASE)
+    {
+      enclave->stack_vma = vma;
+      enclave->_stack_top = enclave->stack_vma->va_start;
+    }
+    vma->pma = pma;
+    vma = vma->vm_next;
+  }
+  if(enclave->text_vma)
+    enclave->text_vma->vm_next = NULL;
+  if(enclave->stack_vma)
+    enclave->stack_vma->vm_next = NULL;
+  enclave->_heap_top = ENCLAVE_DEFAULT_HEAP_BASE;
+  enclave->heap_vma = NULL;
+  enclave->mmap_vma = NULL;
+
+  enclave->free_pages = NULL;
+  enclave->free_pages_num = 0;
+  uintptr_t free_mem = create_args.paddr + create_args.size - RISCV_PGSIZE;
+  while(free_mem >= create_args.free_mem)
+  {
+    struct page_t *page = (struct page_t*)free_mem;
+    page->paddr = free_mem;
+    page->next = enclave->free_pages;
+    enclave->free_pages = page;
+    enclave->free_pages_num += 1;
+    free_mem -= RISCV_PGSIZE;
+  }
+  // check shm
+    if(create_args.shm_paddr && create_args.shm_size &&
+      !(create_args.shm_paddr & (RISCV_PGSIZE-1)) && !(create_args.shm_size & (RISCV_PGSIZE-1)))
+  {
+    mmap((uintptr_t*)(enclave->root_page_table), &(enclave->free_pages), ENCLAVE_DEFAULT_SHM_BASE, create_args.shm_paddr, create_args.shm_size);
+    enclave->shm_paddr = create_args.shm_paddr;
+    enclave->shm_size = create_args.shm_size;
+  }
+  else
+  {
+    enclave->shm_paddr = 0;
+    enclave->shm_size = 0;
+  }
+  copy_word_to_host((unsigned int*)create_args.eid_ptr, enclave->eid);
+  release_enclave_metadata_lock();
+  return ret;
+
+failed:
+  if(need_free_secure_memory)
+  {
+    free_secure_memory(create_args.paddr, create_args.size);
+  }
+  release_enclave_metadata_lock();
+  return ret;
+}
+
+/**
+ * \brief Destroy the server enclave if server enclave is not runnable.
+ *
+ * \param regs The host regs.
+ * \param eid The server enclave id.
+ */
+uintptr_t destroy_server_enclave(uintptr_t* regs, unsigned int eid)
+{
+  uintptr_t retval = 0;
+  struct enclave_t *enclave = NULL;
+  struct server_enclave_t *server_enclave = NULL;
+  struct pm_area_struct* pma = NULL;
+  int need_free_enclave_memory = 0;
+
+  acquire_enclave_metadata_lock();
+
+  server_enclave = __get_server_enclave(eid);
+  if(!server_enclave)
+  {
+    sbi_bug("M mode: destroy_server_enclave: server%d is not found\r\n", eid);
+    retval = -1UL;
+    goto out;
+  }
+  enclave = server_enclave->entity;
+  if(!enclave || enclave->state < FRESH)
+  {
+    sbi_bug("M mode: destroy_server_enclave: server%d can not be accessed\r\n", eid);
+    retval = -1UL;
+    goto out;
+  }
+  sbi_memset((void*)server_enclave, 0, sizeof(struct server_enclave_t));
+
+  if(enclave->state != RUNNING)
+  {
+    pma = enclave->pma_list;
+    need_free_enclave_memory = 1;
+    __free_enclave(eid);
+  }
+  else
+  {
+    //TODO: use ipi to stop the server enclave
+    sbi_bug("M mode: destroy_server_enclave: server enclave is running, can not destroy\n");
+  }
+
+out:
+  release_enclave_metadata_lock();
+  if(need_free_enclave_memory)
+  {
+    free_enclave_memory(pma);
+  }
+
+  return retval;
+}
+/**************************************************************/
+/*                   called by enclave                        */
+/**************************************************************/
+/**
+ * \brief Acquire the server enclave and retrieve the corresponding server enclave handler.
+ *
+ * \param server_name_u The acquired server enclave name.
+ */
+uintptr_t acquire_server_enclave(uintptr_t *regs, char* server_name_u)
+{
+  uintptr_t ret = 0;
+  struct enclave_t *enclave = NULL;
+  struct server_enclave_t *server_enclave = NULL;
+  char *server_name = NULL;
+  int eid = 0;
+  if(check_in_enclave_world() < 0)
+  {
+    return -1UL;
+  }
+
+  acquire_enclave_metadata_lock();
+
+  eid = get_curr_enclave_id();
+  enclave = __get_enclave(eid);
+  if(!enclave)
+  {
+    ret = -1UL;
+    goto failed;
+  }
+
+  server_name = va_to_pa((uintptr_t*)(enclave->root_page_table), server_name_u);
+  if(!server_name)
+  {
+    ret = -1UL;
+    goto failed;
+  }
+  server_enclave = __get_server_enclave_by_name(server_name);
+  if(!server_enclave)
+  {
+    ret = -1UL;
+    goto failed;
+  }
+  ret = server_enclave->entity->eid;
+
+  release_enclave_metadata_lock();
+  return ret;
+
+failed:
+  release_enclave_metadata_lock();
+  sbi_bug("M MODE: acquire_server_enclave: acquire encalve failed\n");
+  return ret;
+}
+
+
+/* Retrive the eid of the caller enclave  */
+/**
+ * \brief Get the enclave id.
+ */
+uintptr_t get_caller_id(uintptr_t* regs)
+{
+  uintptr_t ret = 0;
+  struct enclave_t *enclave = NULL;
+  int eid = 0; 
+  if(check_in_enclave_world() < 0)
+  {
+    return -1UL;
+  }
+
+  acquire_enclave_metadata_lock();
+
+  eid = get_curr_enclave_id();
+  enclave = __get_enclave(eid);
+  if(!enclave)
+  {
+    ret = -1UL;
+    goto failed;
+  }
+
+  ret = enclave->caller_eid;;
+
+  release_enclave_metadata_lock();
+  return ret;
+
+failed:
+  release_enclave_metadata_lock();
+  sbi_bug("M MODE: get_caller_id: failed\n");
+  return ret;
+}
diff --git a/lib/sbi/sm/sm.ac b/lib/sbi/sm/sm.ac
new file mode 100644
index 0000000..2c62c70
--- /dev/null
+++ b/lib/sbi/sm/sm.ac
@@ -0,0 +1,9 @@
+AC_ARG_WITH([target_platform], AS_HELP_STRING([--with-target-platform], [Set a specific platform for the sm to build with]),
+  [AC_SUBST([TARGET_PLATFORM], $with_target_platform, [Set a specific platform for the sm to build with])],
+  [AC_SUBST([TARGET_PLATFORM], default, [Set a specific platform for the sm to build with])])
+AS_IF([test "$TARGET_PLATFORM" == "default"], [
+  AC_DEFINE([pt_area_enabled],,[Define if the Penglai uses PT Area])
+],[
+  AC_DEFINE([${TARGET_PLATFORM}_enabled],,[Define if the Penglai uses other platform])
+])
+AC_DEFINE([SOFTWARE_PT_AREA],,[Define as we always use the SW PT AREA by default])
diff --git a/lib/sbi/sm/sm.c b/lib/sbi/sm/sm.c
new file mode 100644
index 0000000..4572ab5
--- /dev/null
+++ b/lib/sbi/sm/sm.c
@@ -0,0 +1,1374 @@
+#include "sbi/riscv_atomic.h"
+#include "sbi/sbi_tvm.h"
+#include "sbi/sbi_console.h"
+#include "sm/sm.h"
+#include "sm/pmp.h"
+#include "sm/enclave.h"
+#include "sm/enclave_vm.h"
+#include "sm/enclave_mm.h"
+#include "sm/server_enclave.h"
+#include "sm/relay_page.h"
+#include "sm/platform/pt_area/platform.h"
+
+/**
+ * Init secure monitor by invoking platform_init
+ */
+void sm_init()
+{
+  platform_init();
+}
+
+//Init the monitor organized memory.
+uintptr_t sm_mm_init(uintptr_t paddr, uintptr_t size)
+{
+  uintptr_t retval = 0;
+
+  retval = mm_init(paddr, size);
+
+  return retval;
+}
+
+//Extand the monitor organized memory.
+uintptr_t sm_mm_extend(uintptr_t paddr, uintptr_t size)
+{
+  uintptr_t retval = 0;
+
+  retval = mm_init(paddr, size);
+
+  return retval;
+}
+
+uintptr_t pt_area_base = 0;
+uintptr_t pt_area_size = 0;
+uintptr_t mbitmap_base = 0;
+uintptr_t mbitmap_size = 0;
+uintptr_t pgd_order = 0;
+uintptr_t pmd_order = 0;
+spinlock_t mbitmap_lock = SPINLOCK_INIT;
+
+/**
+ * \brief This function validates whether the enclave environment is ready
+ * It will check the PT_AREA and MBitmap.
+ * If the two regions are properly configured, it means the host OS
+ * has invoked SM_INIT sbi call and everything to run enclave is ready.
+ *
+ */
+int enable_enclave()
+{
+  return pt_area_base && pt_area_size && mbitmap_base && mbitmap_size;
+}
+
+/**
+ * \brief Init the bitmap, set the bitmap memory as the secure memory.
+ *
+ * \param _mbitmap_base The start address of the bitmap.
+ * \param _mbitmap_size The bitmap memory size.
+ */
+int init_mbitmap(uintptr_t _mbitmap_base, uintptr_t _mbitmap_size)
+{
+  page_meta* meta = (page_meta*)_mbitmap_base;
+  uintptr_t cur = 0;
+  while(cur < _mbitmap_size)
+  {
+    *meta = MAKE_PUBLIC_PAGE(NORMAL_PAGE);
+    meta += 1;
+    cur += sizeof(page_meta);
+  }
+
+  return 0;
+}
+
+/**
+ * \brief Check whether the pfn range contains the secure memory.
+ *
+ * \param pfn The start page frame.
+ * \param pagenum The page number in the pfn range.
+ */
+int contain_private_range(uintptr_t pfn, uintptr_t pagenum)
+{
+  if(!enable_enclave())
+    return 0;
+
+  if(pfn < ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT)){
+    sbi_bug("M mode: contain_private_range: pfn is out of the DRAM range\r\n");
+    return -1;
+  }
+
+  pfn = pfn - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  page_meta* meta = (page_meta*)mbitmap_base + pfn;
+  if((uintptr_t)(meta + pagenum) > (mbitmap_base + mbitmap_size)){
+    sbi_bug("M mode: contain_private_range: meta is out of the mbitmap range\r\n");
+    return -1;
+  }
+
+  uintptr_t cur = 0;
+  while(cur < pagenum)
+  {
+    if(IS_PRIVATE_PAGE(*meta))
+      return 1;
+    meta += 1;
+    cur += 1;
+  }
+
+  return 0;
+}
+
+/**
+ * \brief The function checks whether a range of physical memory is untrusted memory (for 
+ *  Host OS/apps to use)
+ * Return value:
+ * 	-1: some pages are not public (untrusted)
+ * 	 0: all pages are public (untrusted).
+ *
+ * \param pfn The start page frame.
+ * \param pagenum The page number in the pfn range.
+ */
+int test_public_range(uintptr_t pfn, uintptr_t pagenum)
+{
+  if(!enable_enclave())
+    return 0;
+
+  if(pfn < ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT)){
+    sbi_bug("M mode: test_public_range: pfn is out of DRAM range\r\n");
+    return -1;
+  }
+
+  pfn = pfn - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  page_meta* meta = (page_meta*)mbitmap_base + pfn;
+  if((uintptr_t)(meta + pagenum) > (mbitmap_base + mbitmap_size)){
+    sbi_bug("M mode: test_public_range: meta is out of range\r\n");
+    return -1;
+  }
+
+  uintptr_t cur = 0;
+  while(cur < pagenum)
+  {
+    if(!IS_PUBLIC_PAGE(*meta)){
+      sbi_bug("M mode: test_public_range: IS_PUBLIC_PAGE is failed\r\n");
+      return -1;
+    }
+    meta += 1;
+    cur += 1;
+  }
+
+  return 0;
+}
+
+/**
+ * \brief This function will set a range of physical pages, [pfn, pfn+pagenum],
+ *  to secure pages (or private pages).
+ * This function only updates the metadata of physical pages, but not unmap
+ * them in the host PT pages.
+ * Also, the function will not check whether a page is already secure.
+ * The caller of the function should be careful to perform the above two tasks.
+ *
+ * \param pfn The start page frame.
+ * \param pagenum The page number in the pfn range.
+ */
+int set_private_range(uintptr_t pfn, uintptr_t pagenum)
+{
+  if(!enable_enclave())
+    return 0;
+
+  if(pfn < ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT))
+    return -1;
+
+  pfn = pfn - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  page_meta* meta = (page_meta*)mbitmap_base + pfn;
+  if((uintptr_t)(meta + pagenum) > (mbitmap_base + mbitmap_size))
+    return -1;
+
+  uintptr_t cur = 0;
+  while(cur < pagenum)
+  {
+    *meta = MAKE_PRIVATE_PAGE(*meta);
+    meta += 1;
+    cur += 1;
+  }
+
+  return 0;
+}
+
+/**
+ * \brief Similiar to set_private_pages, but set_public range turns a set of pages
+ *  into public (or untrusted).
+ *
+ * \param pfn The start page frame.
+ * \param pagenum The page number in the pfn range.
+ */
+int set_public_range(uintptr_t pfn, uintptr_t pagenum)
+{
+  if(!enable_enclave())
+    return 0;
+
+  if(pfn < ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT))
+    return -1;
+
+  pfn = pfn - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  page_meta* meta = (page_meta*)mbitmap_base + pfn;
+  if((uintptr_t)(meta + pagenum) > (mbitmap_base + mbitmap_size))
+    return -1;
+
+  uintptr_t cur = 0;
+  while(cur < pagenum)
+  {
+    *meta = MAKE_PUBLIC_PAGE(*meta);
+    meta += 1;
+    cur += 1;
+  }
+
+  return 0;
+}
+
+/**
+ * \brief Init the schrodinger page. Check whether can mark these pages as schrodinger page.
+ *
+ * \param paddr The start page frame.
+ * \param size The page number in the pfn range.
+ */
+uintptr_t sm_schrodinger_init(uintptr_t paddr, uintptr_t size)
+{
+  int ret = 0;
+  if(!enable_enclave())
+    return 0;
+
+  if(paddr & (RISCV_PGSIZE-1) || !(paddr >= (uintptr_t)DRAM_BASE
+        /*&& paddr + size <= (uintptr_t)DRAM_BASE + */))
+    return -1;
+
+  if(size < RISCV_PGSIZE || size & (RISCV_PGSIZE-1))
+    return -1;
+
+  spin_lock(&mbitmap_lock);
+
+  uintptr_t pagenum = size >> RISCV_PGSHIFT;
+  uintptr_t pfn = PADDR_TO_PFN(paddr);
+  if(test_public_range(pfn, pagenum) != 0)
+  {
+    ret = -1;
+    goto out;
+  }
+
+  //fast path
+  uintptr_t _pfn = pfn - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  page_meta* meta = (page_meta*)mbitmap_base + _pfn;
+  uintptr_t cur = 0;
+  while(cur < pagenum)
+  {
+    if(!IS_SCHRODINGER_PAGE(*meta))
+      break;
+    meta += 1;
+    cur += 1;
+    _pfn += 1;
+  }
+  if(cur >= pagenum)
+  {
+    ret = 0;
+    goto out;
+  }
+
+  //slow path
+  uintptr_t *pte = (uintptr_t*)pt_area_base;
+  uintptr_t pte_pos = 0;
+  uintptr_t *pte_end = (uintptr_t*)(pt_area_base + pt_area_size);
+  uintptr_t pfn_base = PADDR_TO_PFN((uintptr_t)DRAM_BASE) + _pfn;
+  uintptr_t pfn_end = PADDR_TO_PFN(paddr + size);
+  uintptr_t _pfn_base = pfn_base - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  uintptr_t _pfn_end = pfn_end - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  //check whether these page only has one mapping in the kernel
+  //pte @ pt entry address
+  //pfn @ the pfn in the current pte
+  //pte_pos @ the offset begin the pte and pt_area_base
+  while(pte < pte_end)
+  {
+    if(!IS_PGD(*pte) && PTE_VALID(*pte))
+    {
+      pfn = PTE_TO_PFN(*pte);
+      //huge page entry
+      if( ((unsigned long)pte >= pt_area_base + (1<<pgd_order)*RISCV_PGSIZE) && ((unsigned long)pte < pt_area_base + (1<<pgd_order)*RISCV_PGSIZE + (1<<pmd_order)*RISCV_PGSIZE)
+      &&IS_LEAF_PTE(*pte))
+      {
+        //the schrodinger page is large than huge page
+        if(pfn >= pfn_base && (pfn + RISCV_PTENUM) <= pfn_end)
+        {
+          _pfn = pfn - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+          //mark the page as schrodinger page, note: a huge page has 512 schrodinger pages
+          for(int i=0; i<RISCV_PTENUM; i++)
+          {
+            meta = ((page_meta*)mbitmap_base) + _pfn + i;
+            //check whether this physical page is already be a schrodinger page, but pt psoition is not current position
+            if(IS_SCHRODINGER_PAGE(*meta) && SCHRODINGER_PTE_POS(*meta) != pte_pos)
+            {
+              sbi_bug("M mode: schrodinger_init: page0x%lx is multi mapped\r\n", pfn);
+              ret = -1;
+              goto failed;
+            }
+            *meta = MAKE_SCHRODINGER_PAGE(0, pte_pos);
+          }
+        }
+        else if(pfn >= pfn_end || (pfn+RISCV_PTENUM )<= pfn_base)
+        {
+          //There is no  overlap between the  pmd region and schrodinger region
+        }
+        else
+        {
+          sbi_bug(" M mode: ERROR: schrodinger_init: non-split page\r\n");
+          return -1;
+          //map_pt_pte_page(pte);
+        }
+      }
+      else if( ((unsigned long)pte >= pt_area_base + (1<<pgd_order)*RISCV_PGSIZE + (1<<pmd_order)*RISCV_PGSIZE) && ((unsigned long)pte < pt_area_base + pt_area_size)) 
+      { //pte is located in the pte sub-area
+        if(pfn >= pfn_base && pfn < pfn_end)
+        {
+          sbi_printf("M mode: schrodinger_init: pfn %lx in pte\r\n", pfn);
+          _pfn = pfn - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+          meta = (page_meta*)mbitmap_base + _pfn;
+          //check whether this physical page is already be a schrodinger page, but pt psoition is not current position
+          if(IS_SCHRODINGER_PAGE(*meta) && SCHRODINGER_PTE_POS(*meta) != pte_pos)
+          {
+            sbi_bug("M mode: schrodinger_init: page0x%lx is multi mapped\r\n", pfn);
+            ret = -1;
+            goto failed;
+          }
+          *meta = MAKE_SCHRODINGER_PAGE(0, pte_pos);
+        }
+      }
+    }
+    pte_pos += 1;
+    pte += 1;
+  }
+  while(_pfn_base < _pfn_end)
+  {
+    meta = (page_meta*)mbitmap_base + _pfn_base;
+    if(!IS_SCHRODINGER_PAGE(*meta))
+      *meta = MAKE_ZERO_MAP_PAGE(*meta);
+    _pfn_base += 1;
+  }
+out:
+  spin_unlock(&mbitmap_lock);
+  return ret;
+
+failed:
+  _pfn_base = pfn_base - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  _pfn_end = pfn_end - ((uintptr_t)DRAM_BASE >> RISCV_PGSHIFT);
+  while(_pfn_base < _pfn_end)
+  {
+    meta = (page_meta*)mbitmap_base + _pfn_base;
+    *meta = MAKE_PUBLIC_PAGE(NORMAL_PAGE);
+    _pfn_base += 1;
+  }
+
+  spin_unlock(&mbitmap_lock);
+  return ret;
+}
+
+int sm_count = 0;
+/**
+ * \brief Auxiliary function for debug.
+ */
+uintptr_t sm_print(uintptr_t paddr, uintptr_t size)
+{
+  sm_count++;
+  return 0;
+  int zero_map_num = 0;
+  int single_map_num = 0;
+  int multi_map_num = 0;
+  uintptr_t pfn = PADDR_TO_PFN(paddr);
+  uintptr_t _pfn = pfn - PADDR_TO_PFN((uintptr_t)DRAM_BASE);
+  uintptr_t pagenum = size >> RISCV_PGSHIFT;
+  page_meta* meta = (page_meta*)mbitmap_base + _pfn;
+  uintptr_t i = 0;
+  while(i < pagenum)
+  {
+    if(IS_ZERO_MAP_PAGE(*meta))
+      zero_map_num+=1;
+    else if(IS_SCHRODINGER_PAGE(*meta))
+      single_map_num+=1;
+    else
+      multi_map_num+=1;
+    i += 1;
+    meta += 1;
+  }
+  sbi_printf("sm_print: paddr:0x%lx, zeromapnum:0x%x,singleapnum:0x%x,multimapnum:0x%x\r\n",
+      paddr, zero_map_num, single_map_num, multi_map_num);
+  return 0;
+}
+
+/**
+ * \brief split the pte (a huge page, 2M) into new_pte_addr (4K PT page)
+ * 
+ * \param pmd The given huge pmd entry.
+ * \param new_pte_addr The new pte page address.
+ */
+uintptr_t sm_map_pte(uintptr_t* pmd, uintptr_t* new_pte_addr)
+{
+  unsigned long pte_attribute = PAGE_ATTRIBUTION(*pmd);
+  unsigned long pfn = PTE_TO_PFN(*pmd);
+  *pmd = PA_TO_PTE((uintptr_t)new_pte_addr, PTE_V);
+  for(int i = 0; i <RISCV_PTENUM; i++)
+  {
+    new_pte_addr[i] = PFN_TO_PTE((pfn + i), pte_attribute);
+  }
+  return 0;
+}
+
+/**
+ * \brief It finds the target pte entry for a huge page
+ * FIXME: turn the functionality into host OS
+ * 
+ * \param paddr The start address of the checking memory range.
+ * \param size The size of the checking memory range.
+ * \param split_pmd Return the pmd of the huge page which needs to be splitted. 
+ */
+uintptr_t sm_split_huge_page(unsigned long paddr, unsigned long size, uintptr_t split_pmd)
+{
+  struct pt_entry_t split_pmd_local;
+  uintptr_t retval = 0;
+  retval = copy_from_host(&split_pmd_local,
+      (struct pt_entry_t*)split_pmd,
+      sizeof(struct pt_entry_t));
+  if(paddr < (uintptr_t)DRAM_BASE /*|| (paddr + size) > */)
+    return -1;
+  uintptr_t _pfn = PADDR_TO_PFN(paddr) - PADDR_TO_PFN((uintptr_t)DRAM_BASE);
+  uintptr_t pfn_base = PADDR_TO_PFN((uintptr_t)DRAM_BASE) + _pfn;
+  uintptr_t pfn_end = PADDR_TO_PFN(paddr + size);
+  uintptr_t *pte = (uintptr_t*)pt_area_base;
+  uintptr_t *pte_end = (uintptr_t*)(pt_area_base + pt_area_size);
+  while(pte < pte_end)
+  {
+    if(!IS_PGD(*pte) && PTE_VALID(*pte))
+    {
+      uintptr_t pfn = PTE_TO_PFN(*pte);
+      if( ((unsigned long)pte >= pt_area_base + (1<<pgd_order)*RISCV_PGSIZE) && ((unsigned long)pte < pt_area_base + (1<<pgd_order)*RISCV_PGSIZE + (1<<pmd_order)*RISCV_PGSIZE)
+      &&IS_LEAF_PTE(*pte))
+      {
+        if(pfn >= pfn_end || (pfn+RISCV_PTENUM )<= pfn_base)
+        {
+          //There is no  overlap between the  pmd region and remap region
+          pte += 1;
+          continue;
+        }
+        else if(pfn_base<=pfn && pfn_end>=(pfn+RISCV_PTENUM))
+        {
+          pte += 1;
+          continue;
+        }
+        else
+        {
+          split_pmd_local.pte_addr = (unsigned long)pte;
+          split_pmd_local.pte = *pte;
+	  break;
+        }
+      }
+    }
+    pte += 1;
+  }
+  retval = copy_to_host((struct pt_entry_t*)split_pmd,
+      &split_pmd_local,
+      sizeof(struct pt_entry_t));
+  return retval;
+}
+
+/**
+ * \brief Unmap a memory region, [paddr, paddr + size], from host's all PTEs
+ * We can achieve a fast path unmapping if the unmapped pages are SCHRODINGER PAGEs.
+ * 
+ * \param paddr The unmap memory address.
+ * \param size The unmap memory size.
+ */
+int unmap_mm_region(unsigned long paddr, unsigned long size)
+{
+  if(!enable_enclave())
+    return 0;
+
+  if(paddr < (uintptr_t)DRAM_BASE /*|| (paddr + size) > */){
+    sbi_bug("M mode: unmap_mm_region: paddr is less than DRAM_BASE\r\n");
+    return -1;
+  }
+
+  //fast path
+  uintptr_t _pfn = PADDR_TO_PFN(paddr) - PADDR_TO_PFN((uintptr_t)DRAM_BASE);
+  page_meta* meta = (page_meta*)mbitmap_base + _pfn;
+  uintptr_t pagenum = size >> RISCV_PGSHIFT;
+  uintptr_t cur = 0;
+  while(cur < pagenum)
+  {
+    if(!IS_SCHRODINGER_PAGE(*meta))
+      break;
+    if(!IS_ZERO_MAP_PAGE(*meta))
+    {
+      //Get pte addr in the pt_area region
+      uintptr_t *pte = (uintptr_t*)pt_area_base + SCHRODINGER_PTE_POS(*meta);
+      *pte = INVALIDATE_PTE(*pte);
+    }
+    cur += 1;
+    _pfn += 1;
+    meta += 1;
+  }
+  if(cur >= pagenum)
+    return 0;
+
+  //slow path
+  if(_pfn != (PADDR_TO_PFN(paddr) - PADDR_TO_PFN((uintptr_t)DRAM_BASE)))
+  {
+    sbi_printf("M mode: Error in unmap_mm_region, pfn is conflict, _pfn_old is %lx _pfn_new is %lx\r\n", 
+		    _pfn, (PADDR_TO_PFN(paddr) - PADDR_TO_PFN((uintptr_t)DRAM_BASE)));
+  }
+  uintptr_t pfn_base = PADDR_TO_PFN((uintptr_t)DRAM_BASE) + _pfn;
+  uintptr_t pfn_end = PADDR_TO_PFN(paddr + size);
+  uintptr_t *pte = (uintptr_t*)pt_area_base;
+  uintptr_t *pte_end = (uintptr_t*)(pt_area_base + pt_area_size);
+
+  while(pte < pte_end)
+  {
+    if(!IS_PGD(*pte) && PTE_VALID(*pte))
+    {
+      uintptr_t pfn = PTE_TO_PFN(*pte);
+      if( ((unsigned long)pte >= pt_area_base + (1<<pgd_order)*RISCV_PGSIZE) 
+		      && ((unsigned long)pte < pt_area_base + (1<<pgd_order)*RISCV_PGSIZE + (1<<pmd_order)*RISCV_PGSIZE)
+		      && IS_LEAF_PTE(*pte))
+      {
+        if(pfn >= pfn_end || (pfn+RISCV_PTENUM )<= pfn_base)
+        {
+          //There is no  overlap between the  pmd region and remap region
+          pte += 1;
+          continue;
+        }
+        else if(pfn_base<=pfn && pfn_end>=(pfn+RISCV_PTENUM))
+        {
+          //This huge page is covered by remap region
+          *pte = INVALIDATE_PTE(*pte);
+        }
+        else
+        {
+          sbi_bug("M mode: ERROR: unmap_mm_region: non-split page\r\n");
+          return -1;
+        }
+      }
+      else if( ((unsigned long)pte >= pt_area_base + (1<<pgd_order)*RISCV_PGSIZE + (1<<pmd_order)*RISCV_PGSIZE) 
+		      && ((unsigned long)pte < pt_area_base + pt_area_size)
+		      && IS_LEAF_PTE(*pte))
+      {
+        if(pfn >= pfn_base && pfn < pfn_end)
+        {
+          *pte = INVALIDATE_PTE(*pte);
+        }
+      }
+    }
+    pte += 1;
+  }
+
+  return 0;
+}
+
+/**
+ * \brief Remap a set of pages to host PTEs.
+ * It's usually used when we try to free a set of secure pages.
+ * 
+ * \param paddr The mmap memory address.
+ * \param size The mmap memory size.
+ */
+int remap_mm_region(unsigned long paddr, unsigned long size)
+{
+  if(!enable_enclave())
+    return 0;
+
+  if(paddr < (uintptr_t)DRAM_BASE /*|| (paddr + size) > */)
+    return -1;
+
+  //Fast path
+  uintptr_t _pfn = PADDR_TO_PFN(paddr) - PADDR_TO_PFN((uintptr_t)DRAM_BASE);
+  page_meta* meta = (page_meta*)mbitmap_base + _pfn;
+  uintptr_t cur = 0;
+  uintptr_t pagenum = size >> RISCV_PGSHIFT;
+  while(cur < pagenum)
+  {
+    if(!IS_SCHRODINGER_PAGE(*meta))
+      break;
+    if(!IS_ZERO_MAP_PAGE(*meta))
+    {
+      uintptr_t *pte = (uintptr_t*)pt_area_base + SCHRODINGER_PTE_POS(*meta);
+      *pte = VALIDATE_PTE(*pte);
+    }
+    cur += 1;
+    _pfn += 1;
+    meta += 1;
+  }
+  if(cur >= pagenum)
+    return 0;
+
+  //Slow path
+  uintptr_t pfn_base = PADDR_TO_PFN((uintptr_t)DRAM_BASE) + _pfn;
+  uintptr_t pfn_end = PADDR_TO_PFN(paddr + size);
+  uintptr_t *pte = (uintptr_t*)pt_area_base;
+  uintptr_t *pte_end = (uintptr_t*)(pt_area_base + pt_area_size);
+  while(pte < pte_end)
+  {
+    if(!IS_PGD(*pte))
+    {
+      uintptr_t pfn = PTE_TO_PFN(*pte);
+      if( ((unsigned long)pte >= pt_area_base + (1<<pgd_order)*RISCV_PGSIZE) && ((unsigned long)pte < pt_area_base + (1<<pgd_order)*RISCV_PGSIZE + (1<<pmd_order)*RISCV_PGSIZE))
+      {
+        if(pfn >= pfn_end || (pfn+RISCV_PTENUM )<= pfn_base)
+        {
+          //There is no  overlap between the  pmd region and remap region
+          pte += 1;
+          continue;
+        }
+        else if(pfn_base<=pfn && pfn_end>=(pfn+RISCV_PTENUM))
+        {
+          //This huge page is covered by remap region
+          *pte = VALIDATE_PTE(*pte);
+        }
+        else
+        {
+          sbi_bug("M mode: The partial of his huge page is belong to enclave and the rest is belong to untrusted OS\r\n");
+          return -1;
+        }
+      }
+      else if( ((unsigned long)pte >= pt_area_base + (1<<pgd_order)*RISCV_PGSIZE + (1<<pmd_order)*RISCV_PGSIZE) && ((unsigned long)pte < pt_area_base + pt_area_size))
+      {
+        if(pfn >= pfn_base && pfn < pfn_end)
+        {
+          *pte = VALIDATE_PTE(*pte);
+        }
+      }
+    }
+    pte += 1;
+  }
+
+  return 0;
+}
+
+/**
+ * \brief Set a single pte entry. It will be triggled by the untrusted OS when setting the new pte entry value.
+ * 
+ * \param pte_dest The location of pt entry in pt area
+ * \param pte_src The content of pt entry
+ */
+int set_single_pte(uintptr_t *pte_dest, uintptr_t pte_src)
+{
+  if(!enable_enclave())
+  {
+    *pte_dest = pte_src;
+    return 0;
+  }
+
+  uintptr_t pfn = 0;
+  uintptr_t _pfn = 0;
+  page_meta* meta = NULL;
+  int huge_page = 0;
+  //Check whether it is a huge page mapping
+  if( ((unsigned long)pte_dest >= pt_area_base + (1<<pgd_order)*RISCV_PGSIZE) && ((unsigned long)pte_dest < pt_area_base + (1<<pgd_order)*RISCV_PGSIZE + (1<<pmd_order)*RISCV_PGSIZE)
+      &&IS_LEAF_PTE(*pte_dest))
+    huge_page = 1;
+  else
+    huge_page = 0;
+  
+  if(huge_page == 0)
+  {
+    //Unmap the original page in the old pte
+    if(!IS_PGD(*pte_dest) && PTE_VALID(*pte_dest))
+    {
+      pfn = PTE_TO_PFN(*pte_dest);
+      _pfn = pfn - PADDR_TO_PFN((uintptr_t)DRAM_BASE);
+      meta = (page_meta*)mbitmap_base + _pfn;
+      if(IS_SCHRODINGER_PAGE(*meta))
+      {
+        *meta = MAKE_ZERO_MAP_PAGE(*meta);
+      }
+    }
+    //Map the new page according to the pte_src
+    if(!IS_PGD(pte_src) && PTE_VALID(pte_src))
+    {
+      uintptr_t pte_pos = pte_dest - (uintptr_t*)pt_area_base;
+      pfn = PTE_TO_PFN(pte_src);
+      _pfn = pfn - PADDR_TO_PFN((uintptr_t)DRAM_BASE);
+      meta = (page_meta*)mbitmap_base + _pfn;
+      if(IS_ZERO_MAP_PAGE(*meta))
+      {
+        *meta = MAKE_SCHRODINGER_PAGE(0, pte_pos);
+      }
+      else if(IS_SCHRODINGER_PAGE(*meta))
+      {
+        *meta = MAKE_PUBLIC_PAGE(NORMAL_PAGE);
+      }
+    }
+
+    *pte_dest = pte_src;
+  }
+  else
+  {
+    if(!IS_PGD(*pte_dest) && PTE_VALID(*pte_dest))
+    {
+      pfn = PTE_TO_PFN(*pte_dest);
+      _pfn = pfn - PADDR_TO_PFN((uintptr_t)DRAM_BASE);
+      for(int i = 0; i < RISCV_PTENUM; i++)
+      {
+        meta = (page_meta*)mbitmap_base + _pfn + i;
+        if(IS_SCHRODINGER_PAGE(*meta))
+        {
+          *meta = MAKE_ZERO_MAP_PAGE(*meta);
+        }
+      }
+    }
+
+    if(!IS_PGD(pte_src) && PTE_VALID(pte_src))
+    {
+      uintptr_t pte_pos = pte_dest - (uintptr_t*)pt_area_base;
+      pfn = PTE_TO_PFN(pte_src);
+      _pfn = pfn - PADDR_TO_PFN((uintptr_t)DRAM_BASE);
+      for(int i = 0; i < RISCV_PTENUM; i++)
+      {
+        meta = (page_meta*)mbitmap_base + _pfn +i;
+        if(IS_ZERO_MAP_PAGE(*meta))
+          *meta = MAKE_SCHRODINGER_PAGE(0, pte_pos);
+        else if(IS_SCHRODINGER_PAGE(*meta))
+        {
+          *meta = MAKE_PUBLIC_PAGE(NORMAL_PAGE);
+        }
+      }
+    }
+
+    *pte_dest = pte_src;
+  }
+  
+
+  return 0;
+}
+
+/**
+ * \brief Check whether the page table entry located in the legitimate location.
+ * 
+ * \param pte_addr The address of the pte entry.
+ * \param pte_src The value of the pte entry.
+ * \param pa The physical address contained in the pte entry.
+ */
+int check_pt_location(uintptr_t pte_addr, uintptr_t pa, uintptr_t pte_src)
+{
+  if((pt_area_base < pte_addr) && ((pt_area_base + (1<<pgd_order)*RISCV_PGSIZE) > pte_addr))
+  {
+    if (((pt_area_base + (1<<pgd_order)*RISCV_PGSIZE) > pa) || ((pt_area_base + ((1<<pgd_order) + (1<<pmd_order))*RISCV_PGSIZE) < pa) )
+    {
+      sbi_printf("pt_area_base %lx pte_addr %lx pa %lx", pt_area_base, pte_addr, pa);
+      sbi_bug("M mode: invalid pt location\r\n");
+      return -1;
+    }
+  }
+  if(((pt_area_base + (1<<pgd_order)*RISCV_PGSIZE) < pte_addr) && ((pt_area_base + ((1<<pgd_order) + (1<<pmd_order))*RISCV_PGSIZE) > pte_addr))
+  {
+    if((pte_src & PTE_V) && !(pte_src & PTE_R) && !(pte_src & PTE_W) && !(pte_src & PTE_X))
+    {
+      if (((pt_area_base + ((1<<pgd_order) + (1<<pmd_order))*RISCV_PGSIZE) > pa) || ((pt_area_base + pt_area_size) < pa) )
+      {
+        sbi_printf("pt_area_base %lx pt_area_pte_base %lx pt_area_pte_end %lx pte_addr %lx pa %lx\r\n", pt_area_base, (pt_area_base + ((1<<pgd_order) + (1<<pmd_order))*RISCV_PGSIZE), 
+        (pt_area_base + pt_area_size), pte_addr, pa);
+        sbi_bug("M mode: invalid pt location\r\n");
+        return -1;
+      }
+    }
+  }
+  return 0;
+}
+
+/**
+ * \brief Set a pte entry. It will be triggled by the untrusted OS when setting the pte entry (set, copy, clear).
+ * 
+ * \param pte_addr The location of pt entry in pt area.
+ * \param pte_src The content of pt entry.
+ * \param flag The page entry flag.
+ * \param size The total pte entries size.
+ */
+uintptr_t sm_set_pte(uintptr_t flag, uintptr_t* pte_addr, uintptr_t pte_src, uintptr_t size)
+{
+  unsigned long ret = 0;
+  if(test_public_range(PADDR_TO_PFN((uintptr_t)pte_addr),1) < 0){
+    sbi_bug("M mode: sm_set_pte: test_public_range is failed\r\n");
+    return -1;
+  }
+  spin_lock(&mbitmap_lock);
+  switch(flag)
+  {
+    case SBI_SET_PTE_ONE:
+      if((!IS_PGD(pte_src)) && PTE_VALID(pte_src))
+      {
+        uintptr_t pfn = PTE_TO_PFN(pte_src);
+        if (check_pt_location((uintptr_t)pte_addr, PTE_TO_PA(pte_src), pte_src) < 0)
+        {
+          ret = -1;
+          sbi_bug("M mode: sm_set_pte: SBI_SET_PTE_ONE: check_pt_location is failed \r\n");
+          break;
+        }
+        if(test_public_range(pfn, 1) < 0)
+        {
+          ret = -1;
+          sbi_bug("M mode: sm_set_pte: SBI_SET_PTE_ONE: test_public_range is failed \r\n");
+          goto free_mbitmap_lock;
+        }
+      }
+      set_single_pte(pte_addr, pte_src);
+      //*pte_addr = pte_src;
+      break;
+    case SBI_PTE_MEMSET:
+      if((!IS_PGD(pte_src)) && PTE_VALID(pte_src))
+      {
+        if(test_public_range(PTE_TO_PFN(pte_src),1) < 0)
+        {
+          ret = -1;
+          sbi_bug("M mode: sm_set_pte: SBI_PTE_MEMSET: test_public_range is failed \r\n");
+          goto free_mbitmap_lock;
+        }
+      }
+      //memset(pte_addr, pte_src, size);
+      uintptr_t i1 = 0;
+      for(i1 = 0; i1 < size/sizeof(uintptr_t); ++i1, ++pte_addr)
+      {
+        set_single_pte(pte_addr, pte_src);
+      }
+      break;
+    case SBI_PTE_MEMCPY:
+      if(size % 8)
+      {
+        ret = -1;
+        sbi_bug("M mode: sm_set_pte: SBI_PTE_MEMCPY: size align is failed \r\n");
+        goto free_mbitmap_lock;
+      }
+      unsigned long i=0, pagenum=size>>3;
+      for(i=0; i<pagenum; ++i)
+      {
+        uintptr_t pte = *((uintptr_t*)pte_src + i);
+        if(!IS_PGD(pte) && PTE_VALID(pte))
+        {
+          if(test_public_range(PTE_TO_PFN(pte),1) < 0)
+          {
+            ret =-1;
+            sbi_bug("M mode: sm_set_pte: SBI_PTE_MEMCPY: test_public_range is failed \r\n");
+            goto free_mbitmap_lock;
+          }
+        }
+      }
+      //sbi_memcpy(pte_addr, (char*)pte_src, size);
+      for(i = 0; i< pagenum; ++i, ++pte_addr)
+      {
+        uintptr_t pte = *((uintptr_t*)pte_src + i);
+        set_single_pte(pte_addr, pte);
+      }
+      break;
+    default:
+      ret = -1;
+      break;
+  }
+
+free_mbitmap_lock:
+  spin_unlock(&mbitmap_lock);
+  return ret;
+}
+
+/**
+ * \brief SM_INIT: This is an SBI call provided by monitor
+ *  The Host OS can invoke the call to init the enclave enviroment, with two regions: [pt_area_base, pt_area_base + area_size]
+ *  and [mbitmap_base + mbitmap_size].
+ *  The first region is PT AREA in penglai, which includes all possible PT pages used for address translation.
+ *  The second region is for monitor to maintain metadata for each physical page (e.g., whether a page is secure/non-secure/or 
+ *  Schrodinger.
+ *  The two regions will be protected by PMPs, and this function will synchronize the PMP configs to other HARTs (if have).
+ *
+ *  The function can only be invoked once (checked by monitor).
+ * 
+ * \param _pt_area_base The pt area start address.
+ * \param _pt_area_size The pt_area size.
+ * \param _mbitmap_base The bitmap start address.
+ * \param _mbitmap_size The bitmap size.
+ */
+uintptr_t sm_sm_init(uintptr_t _pt_area_base, uintptr_t _pt_area_size, uintptr_t _mbitmap_base, uintptr_t _mbitmap_size)
+{
+  if(pt_area_base && pt_area_size && mbitmap_base && mbitmap_size)
+  {
+    sbi_bug("M MODE: sm_sm_init: param is not existed\n");
+    return -1UL;
+  }
+  uintptr_t smregion_base = (uintptr_t)SM_BASE;
+  uintptr_t smregion_size = (uintptr_t)SM_SIZE;
+  if(region_overlap(_pt_area_base, _pt_area_size, smregion_base, smregion_size))
+  {
+    sbi_bug("M MODE: sm_sm_init: region_overlap1 check failed\n");
+    return -1UL;
+  }
+  if(region_overlap(_mbitmap_base, _mbitmap_size, smregion_base, smregion_size))
+  {
+    sbi_bug("M MODE: sm_sm_init: region_overlap2 check failed\n");
+    return -1UL;
+  }
+  if(region_overlap(_pt_area_base, _pt_area_size, _mbitmap_base, _mbitmap_size))
+  {
+    sbi_bug("M MODE: sm_sm_init: region_overlap3 check failed\n");
+    return -1UL;
+  }
+  if(illegal_pmp_addr(_pt_area_base, _pt_area_size) || illegal_pmp_addr(_mbitmap_base, _mbitmap_size))
+  {
+    sbi_bug("M MODE: sm_sm_init: region_overlap4 check failed\n");
+    return -1UL;
+  }
+
+  struct pmp_config_t pmp_config;
+  pmp_config.paddr = _mbitmap_base;
+  pmp_config.size = _mbitmap_size;
+  pmp_config.perm = PMP_NO_PERM;
+  pmp_config.mode = PMP_A_NAPOT;
+  //pmp 2 is used to protect mbitmap
+  set_pmp_and_sync(2, pmp_config);
+  //should protect mbitmap before initializing it
+  init_mbitmap(_mbitmap_base, _mbitmap_size);
+  //enable pt_area and mbitmap
+  //this step must be after initializing mbitmap
+  pt_area_size = _pt_area_size;
+  pt_area_base = _pt_area_base;
+  mbitmap_base = _mbitmap_base;
+  mbitmap_size = _mbitmap_size;
+
+  pmp_config.paddr = _pt_area_base;
+  pmp_config.size = _pt_area_size;
+  pmp_config.perm = PMP_R;
+  pmp_config.mode = PMP_A_NAPOT;
+  //pmp 3 is used to protect pt_area
+  //this step must be after enabling pt_area and mbitmap
+  set_pmp_and_sync(1, pmp_config);
+  return 0;
+}
+
+/**
+ * \brief This function sets the pgd and pmd orders in PT Area, and enable the TVM trap.
+ * 
+ * \param tmp_pgd_order The order of the pgd area page 
+ * \param tmp_pmd_order The prder of the pt_area page.
+ */
+uintptr_t sm_pt_area_separation(uintptr_t tmp_pgd_order, uintptr_t tmp_pmd_order)
+{
+  pgd_order = tmp_pgd_order;
+  pmd_order = tmp_pmd_order;
+  uintptr_t mstatus = csr_read(CSR_MSTATUS);
+  /* Enable TVM here */
+  mstatus = INSERT_FIELD(mstatus, MSTATUS_TVM, 1);
+  csr_write(CSR_MSTATUS, mstatus);
+  set_tvm_and_sync();
+  return 0;
+}
+
+/**
+ * \brief This transitional function for create the enclave.
+ * 
+ * \param enclave_sbi_param The enclave create arguments.
+ */
+uintptr_t sm_create_enclave(uintptr_t enclave_sbi_param)
+{
+  enclave_create_param_t enclave_sbi_param_local;
+  uintptr_t retval = 0;
+  if(test_public_range(PADDR_TO_PFN(enclave_sbi_param),1) < 0){
+    return ENCLAVE_ERROR;
+  }
+
+  retval = copy_from_host(&enclave_sbi_param_local,
+      (enclave_create_param_t*)enclave_sbi_param,
+      sizeof(enclave_create_param_t));
+  if(retval != 0)
+    return ENCLAVE_ERROR;
+
+  retval = create_enclave(enclave_sbi_param_local);
+
+  return retval;
+}
+
+/**
+ * \brief This transitional function for attest the enclave.
+ * 
+ * \param eid The enclave id.
+ * \param report The enclave measurement report.
+ * \param nouce The attestation nonce.
+ */
+uintptr_t sm_attest_enclave(uintptr_t eid, uintptr_t report, uintptr_t nonce)
+{
+  uintptr_t retval;
+
+  retval = attest_enclave(eid, report, nonce);
+
+  return retval;
+}
+
+/**
+ * \brief This transitional function for attest the shadow enclave.
+ * 
+ * \param eid The shadow enclave id.
+ * \param report The shadow enclave measurement report.
+ * \param nouce The attestation nonce.
+ */
+uintptr_t sm_run_enclave(uintptr_t* regs, uintptr_t eid, uintptr_t mm_arg_addr, uintptr_t mm_arg_size)
+{
+  uintptr_t retval = 0;
+
+  retval = run_enclave(regs, (unsigned int)eid, mm_arg_addr, mm_arg_size);
+
+  return retval;
+}
+
+/**
+ * \brief This transitional function for stop the enclave.
+ * 
+ * \param regs The host reg.
+ * \param eid The enclave id.
+ */
+uintptr_t sm_stop_enclave(uintptr_t* regs, uintptr_t eid)
+{
+  //TODO
+
+  return 0;
+}
+
+/**
+ * \brief This transitional function for resume the enclave.
+ * 
+ * \param regs The host reg.
+ * \param eid The enclave id.
+ */
+uintptr_t sm_resume_enclave(uintptr_t* regs, uintptr_t eid)
+{
+  uintptr_t retval = 0;
+  uintptr_t resume_func_id = regs[11];
+  switch(resume_func_id)
+  {
+    case RESUME_FROM_TIMER_IRQ:
+      retval = resume_enclave(regs, eid);
+      break;
+    case RESUME_FROM_STOP:
+      retval = wake_enclave(regs, eid);
+      break;
+    case RESUME_FROM_OCALL:
+      retval = resume_from_ocall(regs, eid);
+      break;
+    default:
+      break;
+  }
+
+  return retval;
+}
+
+/**
+ * \brief This transitional function for destroy the enclave.
+ * 
+ * \param regs The host reg.
+ * \param enclave_eid The enclave id.
+ */
+uintptr_t sm_destroy_enclave(uintptr_t *regs, uintptr_t enclave_id)
+{
+  //TODO
+  uintptr_t ret = 0;
+
+  ret = destroy_enclave(regs, enclave_id);
+
+  return ret;
+}
+
+/**************************************************************/
+/*                   Interfaces for shadow enclave           */
+/**************************************************************/
+/**
+ * \brief This transitional function for attest the shadow enclave.
+ * 
+ * \param eid The shadow enclave id.
+ * \param report The shadow enclave measurement report.
+ * \param nouce The attestation nonce.
+ */
+uintptr_t sm_attest_shadow_enclave(uintptr_t eid, uintptr_t report, uintptr_t nonce)
+{
+  uintptr_t retval;
+
+  retval = attest_shadow_enclave(eid, report, nonce);
+
+  return retval;
+}
+
+/**
+ * \brief This transitional function creates the shadow enclave.
+ * 
+ * \param enclave_sbi_param The arguments for creating the shadow enclave.
+ */
+uintptr_t sm_create_shadow_enclave(uintptr_t enclave_sbi_param)
+{
+  enclave_create_param_t enclave_sbi_param_local;
+  uintptr_t retval = 0;
+  if(test_public_range(PADDR_TO_PFN(enclave_sbi_param),1) < 0){
+    return ENCLAVE_ERROR;
+  }
+  retval = copy_from_host(&enclave_sbi_param_local,
+      (enclave_create_param_t*)enclave_sbi_param,
+      sizeof(enclave_create_param_t));
+  if(retval != 0)
+    return ENCLAVE_ERROR;
+
+  retval = create_shadow_enclave(enclave_sbi_param_local);
+
+  return retval;
+}
+
+/**
+ * \brief This transitional function for run the shadow enclave.
+ * 
+ * \param regs The host reg.
+ * \param eid The shadow enclave id.
+ * \param shadow_enclave_run_args The arguments for running the shadow enclave.
+ * \param mm_arg_addr The relay page address.
+ * \param mm_arg_size The relay page size.
+ */
+uintptr_t sm_run_shadow_enclave(uintptr_t* regs, uintptr_t eid, uintptr_t shadow_enclave_run_args, uintptr_t mm_arg_addr, uintptr_t mm_arg_size)
+{
+  shadow_enclave_run_param_t enclave_sbi_param_local;
+  uintptr_t retval = 0;
+  if(test_public_range(PADDR_TO_PFN(shadow_enclave_run_args), 1) < 0){
+    return ENCLAVE_ERROR;
+  }
+  retval = copy_from_host(&enclave_sbi_param_local,
+      (shadow_enclave_run_param_t*)shadow_enclave_run_args,
+      sizeof(shadow_enclave_run_param_t));
+  if(retval != 0)
+    return ENCLAVE_ERROR;
+
+  retval = run_shadow_enclave(regs, (unsigned int)eid, enclave_sbi_param_local, mm_arg_addr, mm_arg_size);
+  if (retval ==  ENCLAVE_ATTESTATION)
+  {
+    copy_to_host((shadow_enclave_run_param_t*)shadow_enclave_run_args,
+      &enclave_sbi_param_local,
+      sizeof(shadow_enclave_run_param_t));
+  }
+  return retval;
+}
+
+/**************************************************************/
+/*                   called by enclave                        */
+/**************************************************************/
+/**
+ * \brief This transitional function exits the enclave mode.
+ * 
+ * \param regs The enclave reg.
+ * \param retval The enclave return value.
+ */
+uintptr_t sm_exit_enclave(uintptr_t* regs, uintptr_t retval)
+{
+  uintptr_t ret = 0;
+
+  ret = exit_enclave(regs, retval);
+
+  return ret;
+}
+
+/**
+ * \brief This transitional function is for enclave ocall procedure.
+ * 
+ * \param regs The enclave reg.
+ * \param ocall_id The ocall function id.
+ * \param arg0 The ocall argument 0.
+ * \param arg1 The ocall argument 1.
+ */
+uintptr_t sm_enclave_ocall(uintptr_t* regs, uintptr_t ocall_id, uintptr_t arg0, uintptr_t arg1)
+{
+  uintptr_t ret = 0;
+  switch(ocall_id)
+  {
+    case OCALL_MMAP:
+      ret = enclave_mmap(regs, arg0, arg1);
+      break;
+    case OCALL_UNMAP:
+      ret = enclave_unmap(regs, arg0, arg1);
+      break;
+    case OCALL_SYS_WRITE:
+      ret = enclave_sys_write(regs);
+      break;
+    case OCALL_SBRK:
+      ret = enclave_sbrk(regs, arg0);
+      break;
+    case OCALL_READ_SECT:
+      ret = enclave_read_sec(regs,arg0);
+      break;
+    case OCALL_WRITE_SECT:
+      ret = enclave_write_sec(regs, arg0);
+      break;
+    case OCALL_RETURN_RELAY_PAGE:
+      ret = enclave_return_relay_page(regs);
+      break;   
+    default:
+      ret = -1UL;
+      break;
+  }
+
+  return ret;
+}
+
+/**
+ * \brief This transitional function is for handling the time irq triggered in the enclave.
+ * 
+ * \param regs The enclave reg.
+ * \param mcause CSR mcause value.
+ * \param mepc CSR mepc value.
+ */
+uintptr_t sm_do_timer_irq(uintptr_t *regs, uintptr_t mcause, uintptr_t mepc)
+{
+  uintptr_t ret = 0;
+
+  ret = do_timer_irq(regs, mcause, mepc);
+  if((ret >= 0) && (ret <= SBI_LEGAL_MAX))
+	{
+		regs[10] = 0;
+		regs[11] = ret;
+	}
+  return ret;
+}
+
+/**
+ * \brief This transitional function is for handling yield() triggered in the enclave.
+ * 
+ * \param regs The enclave reg.
+ * \param mcause CSR mcause value.
+ * \param mepc CSR mepc value.
+ */
+uintptr_t sm_handle_yield(uintptr_t *regs, uintptr_t mcause, uintptr_t mepc)
+{
+  uintptr_t ret = 0;
+
+  ret = do_yield(regs, mcause, mepc);
+
+  return ret;
+}
+
+/**************************************************************/
+/*                   Interfaces for server enclave           */
+/**************************************************************/
+/**
+ * \brief This transitional function creates the server enclave.
+ * 
+ * \param regs The enclave reg.
+ * \param mcause CSR mcause value.
+ * \param mepc CSR mepc value.
+ */
+uintptr_t sm_create_server_enclave(uintptr_t enclave_sbi_param)
+{
+  enclave_create_param_t enclave_sbi_param_local;
+  uintptr_t retval = 0;
+  if(test_public_range(PADDR_TO_PFN(enclave_sbi_param),1)<0){
+    return ENCLAVE_ERROR;
+  }
+  retval = copy_from_host(&enclave_sbi_param_local,
+      (enclave_create_param_t*)enclave_sbi_param,
+      sizeof(enclave_create_param_t));
+  if(retval != 0)
+    return ENCLAVE_ERROR;
+
+  retval = create_server_enclave(enclave_sbi_param_local);
+
+  return retval;
+}
+
+/**
+ * \brief This transitional function destroys the server enclave.
+ * 
+ * \param enclave_sbi_param The arguments for creating the shadow enclave.
+ */
+uintptr_t sm_destroy_server_enclave(uintptr_t *regs, uintptr_t enclave_id)
+{
+  //TODO
+  uintptr_t ret = 0;
+
+  ret = destroy_server_enclave(regs, enclave_id);
+
+  return ret;
+}
+
+/**
+ * \brief This transitional function acquires the server enclave handler.
+ * 
+ * \param regs The enclave regs.
+ * \param server_name The acquired server enclave name.
+ */
+uintptr_t sm_server_enclave_acquire(uintptr_t *regs, uintptr_t server_name)
+{
+  uintptr_t ret = 0;
+
+  ret = acquire_server_enclave(regs, (char*)server_name);
+
+  return ret;
+}
+
+/**
+ * \brief This transitional function gets the enclave id.
+ * 
+ * \param regs The enclave regs.
+ */
+uintptr_t sm_get_caller_id(uintptr_t *regs)
+{
+  uintptr_t ret = 0;
+
+  ret = get_caller_id(regs);
+
+  return ret;
+}
+
+/**
+ * \brief This transitional function call the server enclave.
+ * 
+ * \param regs The enclave regs.
+ * \param eid The callee enclave id.
+ * \param arg The calling arguments.
+ */
+uintptr_t sm_call_enclave(uintptr_t* regs, uintptr_t eid, uintptr_t arg)
+{
+  uintptr_t retval = 0;
+
+  retval = call_enclave(regs, (unsigned int)eid, arg);
+
+  return retval;
+}
+
+/**
+ * \brief This transitional function is for server enclave return .
+ * 
+ * \param regs The enclave regs.
+ * \param arg The return arguments.
+ */
+uintptr_t sm_enclave_return(uintptr_t* regs, uintptr_t arg)
+{
+  uintptr_t ret = 0;
+
+  ret = enclave_return(regs, arg);
+
+  return ret;
+}
+
+/**
+ * \brief This transitional function is for the asynchronous call to the server enclave.
+ * 
+ * \param regs The enclave regs.
+ * \param enclave_name The callee enclave name.
+ * \param arg The calling arguments.
+ */
+uintptr_t sm_asyn_enclave_call(uintptr_t *regs, uintptr_t enclave_name, uintptr_t arg)
+{
+  uintptr_t ret = 0;
+
+  ret = asyn_enclave_call(regs, enclave_name, arg);
+  return ret;
+}
+
+/**
+ * \brief This transitional function splits the enclave memory into two pieces.
+ * 
+ * \param regs The enclave regs.
+ * \param mem_addr The splitted memory address.
+ * \param mem_size The splitted memory size.
+ * \param split_addr The split point in the memory range.
+ */
+uintptr_t sm_split_mem_region(uintptr_t *regs, uintptr_t mem_addr, uintptr_t mem_size, uintptr_t split_addr)
+{
+  uintptr_t ret = 0;
+
+  ret = split_mem_region(regs, mem_addr, mem_size, split_addr);
+
+  return ret;
+}
\ No newline at end of file
diff --git a/lib/sbi/sm/thread.c b/lib/sbi/sm/thread.c
new file mode 100644
index 0000000..4c68478
--- /dev/null
+++ b/lib/sbi/sm/thread.c
@@ -0,0 +1,101 @@
+#include "sm/thread.h"
+#include "sbi/riscv_encoding.h"
+#include "sbi/riscv_asm.h"
+/**
+ * \brief swap general registers in thread->prev_state and regs
+ *
+ * \param thread is the thread abstraction in enclaves
+ * \param regs usually is the location to save regs of host/enclaves (before trap)
+ */
+void swap_prev_state(struct thread_state_t* thread, uintptr_t* regs)
+{
+  int i;
+
+  uintptr_t* prev = (uintptr_t*) &thread->prev_state;
+  for(i = 1; i < N_GENERAL_REGISTERS; ++i)
+  {
+    //swap general register
+    uintptr_t tmp = prev[i];
+    prev[i] = regs[i];
+    regs[i] = tmp;
+  }
+
+  return;
+}
+
+/**
+ * \brief it switch the mepc with an enclave, and updates the mepc csr
+ * 
+ * \param thread is the thread abstraction in enclaves
+ * \param current_mepc is the current mepc value
+ */
+void swap_prev_mepc(struct thread_state_t* thread, uintptr_t current_mepc)
+{
+  uintptr_t tmp = thread->prev_mepc;
+  thread->prev_mepc = current_mepc;
+  csr_write(CSR_MEPC, tmp);
+}
+
+/**
+ * \brief it switch the stvec with an enclave, and updates the stvec csr
+ * 
+ * \param thread is the thread abstraction in enclaves
+ * \param current_stvec is the current stvec value
+ */
+void swap_prev_stvec(struct thread_state_t* thread, uintptr_t current_stvec)
+{
+  uintptr_t tmp = thread->prev_stvec;
+  thread->prev_stvec = current_stvec;
+  csr_write(CSR_STVEC, tmp);
+}
+
+/**
+ * \brief it switches the enclave cache binding status
+ * 
+ * \param thread is the thread abstraction in enclaves
+ * \param current_cache_binding is the current cache binding status
+ */
+void swap_prev_cache_binding(struct thread_state_t* thread, uintptr_t current_cache_binding)
+{
+  thread->prev_cache_binding = current_cache_binding;
+  //TODO
+}
+
+/**
+ * \brief it switches the enclave mie status
+ * 
+ * \param thread is the thread abstraction in enclaves
+ * \param current_cache_binding is the current mie status
+ */
+void swap_prev_mie(struct thread_state_t* thread, uintptr_t current_mie)
+{
+  uintptr_t tmp = thread->prev_mie;
+  thread->prev_mie = current_mie;
+  csr_write(CSR_MIE, tmp);
+}
+
+/**
+ * \brief it switches the enclave mideleg status
+ * 
+ * \param thread is the thread abstraction in enclaves
+ * \param current_cache_binding is the current mideleg status
+ */
+void swap_prev_mideleg(struct thread_state_t* thread, uintptr_t current_mideleg)
+{
+  uintptr_t tmp = thread->prev_mideleg;
+  thread->prev_mideleg = current_mideleg;
+  csr_write(CSR_MIDELEG, tmp);
+}
+
+/**
+ * \brief it switches the enclave medeleg status
+ * 
+ * \param thread is the thread abstraction in enclaves
+ * \param current_cache_binding is the current medeleg status
+ */
+void swap_prev_medeleg(struct thread_state_t* thread, uintptr_t current_medeleg)
+{
+  uintptr_t tmp = thread->prev_medeleg;
+  thread->prev_medeleg = current_medeleg;
+  csr_write(CSR_MEDELEG, tmp);
+}
-- 
2.17.1

